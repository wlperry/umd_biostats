[
  {
    "objectID": "lectures/lecture_03/03_03_homework_html.html",
    "href": "lectures/lecture_03/03_03_homework_html.html",
    "title": "03_Homework",
    "section": "",
    "text": "This is an assignment for you to practice coding and redo the work we do in class with a few twists on a new dataframe practicing to create new projects and writing new code. I realize you could copy the code from lecture and although you will get the code right, I urge you to retype it form scratch as it will be learned so much faster. This is a new language for you and if you dont “type” == “speak” the language you would remember it…. really and try breaking things. Dont be afraid you can download a new version or fix it… that is how we learn."
  },
  {
    "objectID": "lectures/lecture_03/03_03_homework_html.html#setup",
    "href": "lectures/lecture_03/03_03_homework_html.html#setup",
    "title": "03_Homework",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataframe:\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in the data file\nw3_df &lt;- read_csv(\"data/lake_trout.csv\") %&gt;% filter(lake %in% c(\"Island Lake\",\"NE 12\")) \n\n# Look at the first few rows\nhead(w3_df)\n\n# A tibble: 6 × 5\n  sampling_site species    length_mm mass_g lake       \n  &lt;chr&gt;         &lt;chr&gt;          &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1 Island Lake   lake trout       640   2600 Island Lake\n2 Island Lake   lake trout       650   2350 Island Lake\n3 Island Lake   lake trout       585   2200 Island Lake\n4 Island Lake   lake trout       720   3950 Island Lake\n5 Island Lake   lake trout       880   6800 Island Lake\n6 Island Lake   lake trout       830   3200 Island Lake\n\n\nLet’s calculate some basic statistics for lake trout\n\n# Calculate basic statistics \nw3_stats &lt;- w3_df %&gt;% \n  group_by(lake) %&gt;% \n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n = sum(!is.na(length_mm)),\n    se_length = sd_length / sqrt(n)\n  )\n\n# Display the statistics\nw3_stats\n\n# A tibble: 2 × 5\n  lake        mean_length sd_length     n se_length\n  &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 Island Lake        698.      121.    10     38.2 \n2 NE 12              348.      127.   323      7.05"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#todays-objectives",
    "href": "lectures/lecture_03/03_02_class_activity.html#todays-objectives",
    "title": "03_Class_Activity",
    "section": "Today’s Objectives",
    "text": "Today’s Objectives\n\nImplement descriptive statistics in R\nCalculate measures of central tendency and spread\nCompare distributions of data from different groups\nCreate effective visualizations of descriptive statistics\nInterpret the meaning of these statistics in a biological context"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#getting-the-data",
    "href": "lectures/lecture_03/03_02_class_activity.html#getting-the-data",
    "title": "03_Class_Activity",
    "section": "Getting the data",
    "text": "Getting the data\n\n\n\n\n\n\nPractice Exercise 1: Loading and Examining the Grayling Data\n\n\n\nWe’ll be working with data on arctic grayling fish from two different lakes (I3 and I8).\n\n# Write your code here to read in the file\n# How do you examine the data - what are the ways you think and lets try it!\n\n# Load the grayling data\ngrayling_df &lt;- read_csv(\"data/gray_I3_I8.csv\")\n\nRows: 168 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# View the first few rows\nhead(grayling_df)\n\n# A tibble: 6 × 5\n   site lake  species         length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;               &lt;dbl&gt;  &lt;dbl&gt;\n1   113 I3    arctic grayling       266    135\n2   113 I3    arctic grayling       290    185\n3   113 I3    arctic grayling       262    145\n4   113 I3    arctic grayling       275    160\n5   113 I3    arctic grayling       240    105\n6   113 I3    arctic grayling       265    145\n\n\n\n\n\n# Examine the data structure\nglimpse(grayling_df)\n\nRows: 168\nColumns: 5\n$ site      &lt;dbl&gt; 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, 113, …\n$ lake      &lt;chr&gt; \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", \"I3\", …\n$ species   &lt;chr&gt; \"arctic grayling\", \"arctic grayling\", \"arctic grayling\", \"ar…\n$ length_mm &lt;dbl&gt; 266, 290, 262, 275, 240, 265, 265, 253, 246, 203, 289, 239, …\n$ mass_g    &lt;dbl&gt; 135, 185, 145, 160, 105, 145, 150, 130, 130, 71, 179, 108, 1…\n\n\n\n# Get a statistical summary\nsummary(grayling_df)\n\n      site         lake             species            length_mm    \n Min.   :113   Length:168         Length:168         Min.   :191.0  \n 1st Qu.:113   Class :character   Class :character   1st Qu.:270.8  \n Median :118   Mode  :character   Mode  :character   Median :324.5  \n Mean   :116                                         Mean   :324.5  \n 3rd Qu.:118                                         3rd Qu.:377.0  \n Max.   :118                                         Max.   :440.0  \n                                                                    \n     mass_g     \n Min.   : 53.0  \n 1st Qu.:151.2  \n Median :340.0  \n Mean   :351.2  \n 3rd Qu.:519.5  \n Max.   :889.0  \n NA's   :2      \n\n\n\n# How many fish do we have from each lake?\ngrayling_df %&gt;%\n  count(lake) %&gt;% flextable()\n\nlakenI366I8102"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#lets-calculate-various-descriptive-statistics-for-our-data",
    "href": "lectures/lecture_03/03_02_class_activity.html#lets-calculate-various-descriptive-statistics-for-our-data",
    "title": "03_Class_Activity",
    "section": "Let’s calculate various descriptive statistics for our data:",
    "text": "Let’s calculate various descriptive statistics for our data:"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#section-1",
    "href": "lectures/lecture_03/03_02_class_activity.html#section-1",
    "title": "03_Class_Activity",
    "section": "",
    "text": "Practice Exercise 2: Measures of Central Tendency\n\n\n\nLet’s recreate the basic histogram of fish lengths from our last class. Use the sculpin_df data frame that’s already loaded.\n\n# Write your code here to read in the file\n# How do you examine the data - what are the ways you think and lets try it!\n# Calculate the mean and median fish length\nmean_length &lt;- mean(grayling_df$length_mm)\nmedian_length &lt;- median(grayling_df$length_mm)\n\ncat(\"Mean fish length:\", round(mean_length, 1), \"mm\\n\")\n\nMean fish length: 324.5 mm\n\ncat(\"Median fish length:\", median_length, \"mm\\n\")\n\nMedian fish length: 324.5 mm\n\n\n\n\n\n# Calculate mean and median by lake\ngrayling_df %&gt;%\n  group_by(lake) %&gt;%\n  summarise(\n    mean_length = mean(length_mm),\n    median_length = median(length_mm)\n  ) %&gt;%\n  flextable()\n\nlakemean_lengthmedian_lengthI3265.6061266I8362.5980373"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#summarizing-data---two-ways",
    "href": "lectures/lecture_03/03_02_class_activity.html#summarizing-data---two-ways",
    "title": "03_Class_Activity",
    "section": "Summarizing data - two ways",
    "text": "Summarizing data - two ways\nlets say we want to summarize the data and need to get n, means, standard deviation, standard error\nWe could do the following - if we had missing cells the code below would give an error\n\nmean(grayling_df$length_mm) \n\n[1] 324.494\n\n\n\nmean(grayling_df$length_mm, na.rm = TRUE) # removes missing values\n\n[1] 324.494\n\n\n\nlength(grayling_df$length_mm)\n\n[1] 168\n\n\n\nthe length counts missing and non-missing data\nhowever this would get old if we had to do this for everything and then to do it for the different groupings - lee and windward…"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#we-need-to-learn-to-pipe-things",
    "href": "lectures/lecture_03/03_02_class_activity.html#we-need-to-learn-to-pipe-things",
    "title": "03_Class_Activity",
    "section": "We need to learn to pipe things",
    "text": "We need to learn to pipe things\n\nthe dataframe –&gt; pipe command that feed the dataframe into –&gt; next command\n\n\ngrayling_df %&gt;% summarize(mean_length = mean(length_mm, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_length\n        &lt;dbl&gt;\n1        324."
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#what-is-cool-is-we-can-do-a-lot-of-different-things-now",
    "href": "lectures/lecture_03/03_02_class_activity.html#what-is-cool-is-we-can-do-a-lot-of-different-things-now",
    "title": "03_Class_Activity",
    "section": "What is cool is we can do a lot of different things now",
    "text": "What is cool is we can do a lot of different things now\n\ngrayling_df %&gt;% \n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n_length = n())\n\n# A tibble: 1 × 3\n  mean_length sd_length n_length\n        &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1        324.      65.0      168"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#super-cool-code-in-case-there-are-missing-values",
    "href": "lectures/lecture_03/03_02_class_activity.html#super-cool-code-in-case-there-are-missing-values",
    "title": "03_Class_Activity",
    "section": "Super cool code in case there are missing values",
    "text": "Super cool code in case there are missing values\n\ngrayling_df %&gt;% \n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n_length = sum(!is.na(length_mm)))\n\n# A tibble: 1 × 3\n  mean_length sd_length n_length\n        &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1        324.      65.0      168"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#part-3-visualizing-distributions",
    "href": "lectures/lecture_03/03_02_class_activity.html#part-3-visualizing-distributions",
    "title": "03_Class_Activity",
    "section": "Part 3: Visualizing Distributions",
    "text": "Part 3: Visualizing Distributions\nVisualizations can help us better understand the descriptive statistics we’ve calculated.\n\n\n\n\n\n\nExercise 6: Creating Histograms\n\n\n\nOne of the best ways to look at data is a histogram - and we will do it again\n\n# Create a histogram of all fish lengths\nggplot(grayling_df, aes(x = length_mm)) +\n  geom_histogram(bins = 15, fill = \"steelblue\", color = \"white\") +\n  labs(\n    title = \"Distribution of Fish Lengths\",\n    x = \"Total Length (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Create histograms by lake\nggplot(grayling_df, aes(x = length_mm, fill = lake)) +\n  geom_histogram(bins = 15, position = \"dodge\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Fish Lengths by Lake\",\n    x = \"Total Length (mm)\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 7: Creating Box Plots\n\n\n\nPersonally I like box plots\n\n# Create a box plot comparing fish lengths by lake\n# Create a box plot comparing fish lengths by lake\nggplot(grayling_df, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(\n    title = \"Box Plot of Fish Lengths by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 9: Creating Density Plots\n\n\n\nNow these will be really important later on\n\n## Create density plots\nggplot(grayling_df, aes(x = length_mm, fill = lake)) +\n  geom_density(alpha = 0.5) +\n  labs(\n    title = \"Density Plot of Fish Lengths by Lake\",\n    x = \"Total Length (mm)\",\n    y = \"Density\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions to Consider:\n\nWhich visualization best shows the differences in fish lengths between lakes?\nWhat can you learn from the violin plots that might not be apparent from the box plots?\nHow would you interpret the cumulative frequency distribution?\nWhat patterns or insights can you identify from these visualizations?"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#part-4-interpreting-the-results",
    "href": "lectures/lecture_03/03_02_class_activity.html#part-4-interpreting-the-results",
    "title": "03_Class_Activity",
    "section": "Part 4: Interpreting the Results",
    "text": "Part 4: Interpreting the Results\nBased on our analysis, we can make the following observations:\n\nLake Differences: Fish from Lake I8 are generally larger than those from Lake I3, both in length and mass.\nVariability: Lake I8 shows greater variability in fish lengths and masses than Lake I3, as indicated by higher standard deviations and IQRs.\nDistribution Shape:\n\nLake I3 fish lengths are more symmetrically distributed.\nLake I8 fish lengths show a slight negative skew, suggesting a few smaller fish pulling the distribution to the left.\n\nLength-Mass Relationship: Both lakes show a strong positive correlation between fish length and mass, following an approximately cubic relationship (mass increases with the cube of length)."
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#guided-questions-for-deeper-understanding-of-descriptive-statistics",
    "href": "lectures/lecture_03/03_02_class_activity.html#guided-questions-for-deeper-understanding-of-descriptive-statistics",
    "title": "03_Class_Activity",
    "section": "Guided Questions for Deeper Understanding of descriptive statistics",
    "text": "Guided Questions for Deeper Understanding of descriptive statistics\n\nBiological Interpretation: What ecological factors might explain the differences in fish size between the two lakes?\nStatistical Reasoning: Why might we prefer to use the median and IQR instead of the mean and standard deviation in some cases?\nData Visualization: Which visualization method was most effective for comparing the two lakes? Why?\nScientific Communication: How would you concisely summarize these findings in a scientific paper?\nFurther Analysis: What additional analyses might be useful to better understand this dataset?"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html",
    "title": "Lecture 04: Probability and Inference",
    "section": "",
    "text": "Review of probability distributions\nStandard normal distribution and Z-scores\nStandard error and confidence intervals\nStatistical inference fundamentals\nHypothesis testing principles\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 1: Exploring the Grayling Dataset\n\n\n\nLet’s explore the Arctic grayling data from lakes I3 and I8. Use the grayling_df data frame to create basic summary statistics.\n\n# Write your code here to explore the basic structure of the data\n# also note plottig a box plot is really useful\nstr(grayling_df)\n\nspc_tbl_ [168 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site     : num [1:168] 113 113 113 113 113 113 113 113 113 113 ...\n $ lake     : chr [1:168] \"I3\" \"I3\" \"I3\" \"I3\" ...\n $ species  : chr [1:168] \"arctic grayling\" \"arctic grayling\" \"arctic grayling\" \"arctic grayling\" ...\n $ length_mm: num [1:168] 266 290 262 275 240 265 265 253 246 203 ...\n $ mass_g   : num [1:168] 135 185 145 160 105 145 150 130 130 71 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\nsummary(grayling_df)\n\n      site         lake             species            length_mm    \n Min.   :113   Length:168         Length:168         Min.   :191.0  \n 1st Qu.:113   Class :character   Class :character   1st Qu.:270.8  \n Median :118   Mode  :character   Mode  :character   Median :324.5  \n Mean   :116                                         Mean   :324.5  \n 3rd Qu.:118                                         3rd Qu.:377.0  \n Max.   :118                                         Max.   :440.0  \n                                                                    \n     mass_g     \n Min.   : 53.0  \n 1st Qu.:151.2  \n Median :340.0  \n Mean   :351.2  \n 3rd Qu.:519.5  \n Max.   :889.0  \n NA's   :2"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#probability-distribution-functions",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#probability-distribution-functions",
    "title": "Lecture 04: Probability and Inference",
    "section": "Probability Distribution Functions",
    "text": "Probability Distribution Functions\n\nA probability distribution describes the probability of different outcomes in an experiment\nWe’ve seen histograms of observed data\nTheoretical distributions help us model and understand real-world data\nWe will focus on a standard normal distribution and a t distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution\n\nSo lets do this for a fish that is 300mm long and guess the probability of catching something larger\nz = (300 - 265.61)/28.3 = 1.215194"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma-1",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma-1",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution\n\nSo lets do this for a fish that is 320mm long and guess the probability of catching something larger\nz = (320 - 265.61)/28.3 = 1.92\nor .9726 in table or 97.3% is the area left of the curve and\n100 - 97.3 = 2.7% or 2.7% of fish are expected to be longer"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#every-sample-gives-slightly-different-estimate-of-µ",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#every-sample-gives-slightly-different-estimate-of-µ",
    "title": "Lecture 04: Probability and Inference",
    "section": "Every sample gives slightly different estimate of µ",
    "text": "Every sample gives slightly different estimate of µ\n\nCan take many samples and calculate means\nPlot the frequency distribution of means\nGet the “sampling distribution of means”"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#important-properties",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#important-properties",
    "title": "Lecture 04: Probability and Inference",
    "section": "3 important properties:",
    "text": "3 important properties:\n\nSampling distribution of means (SDM) from normal population will be normal\nLarge Sampling distribution of means from any population will be normal (Central Limit Theorem)\nThe mean of Sampling distribution of means will equal µ or the mean"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#given-above",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#given-above",
    "title": "Lecture 04: Probability and Inference",
    "section": "Given above",
    "text": "Given above\n\ncan estimate the standard deviation of sample means\n“Standard error of sample mean”\nHow good is your estimate of population mean? (based on the sample collected)\nquantifies how much the sample means are expected to vary from samples\ngives an estimate of the error associated with using \\(\\bar{y}\\) to estimate \\(\\mu\\)…"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textci-bary-pm-t-cdot-fracssqrtn",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textci-bary-pm-t-cdot-fracssqrtn",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)",
    "text": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)\nWhere:\n\nȳ is sample mean\n𝑛 is sample size\ns is sample standard deviation\nt t-value corresponding the probability of the CI\nt in t-table for different degrees of freedom (n-1)"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#probability-distribution-functions",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#probability-distribution-functions",
    "title": "Lecture 04: Probability and Inference",
    "section": "Probability Distribution Functions",
    "text": "Probability Distribution Functions\n\nA probability distribution describes the probability of different outcomes in an experiment\nWe’ve seen histograms of observed data\nTheoretical distributions help us model and understand real-world data\nWe will focus on a standard normal distribution and a t distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution\n\nSo lets do this for a fish that is 300mm long and guess the probability of catching something larger\nz = (300 - 265.61)/28.3 = 1.215194"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma-1",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma-1",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution\n\nSo lets do this for a fish that is 320mm long and guess the probability of catching something larger\nz = (320 - 265.61)/28.3 = 1.92\nor .9726 in table or 97.3% is the area left of the curve and\n100 - 97.3 = 2.7% or 2.7% of fish are expected to be longer"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#every-sample-gives-slightly-different-estimate-of-µ",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#every-sample-gives-slightly-different-estimate-of-µ",
    "title": "Lecture 04: Probability and Inference",
    "section": "Every sample gives slightly different estimate of µ",
    "text": "Every sample gives slightly different estimate of µ\n\nCan take many samples and calculate means\nPlot the frequency distribution of means\nGet the “sampling distribution of means”"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#important-properties",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#important-properties",
    "title": "Lecture 04: Probability and Inference",
    "section": "3 important properties:",
    "text": "3 important properties:\n\nSampling distribution of means (SDM) from normal population will be normal\nLarge Sampling distribution of means from any population will be normal (Central Limit Theorem)\nThe mean of Sampling distribution of means will equal µ or the mean"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#given-above",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#given-above",
    "title": "Lecture 04: Probability and Inference",
    "section": "Given above",
    "text": "Given above\n\ncan estimate the standard deviation of sample means\n“Standard error of sample mean”\nHow good is your estimate of population mean? (based on the sample collected)\nquantifies how much the sample means are expected to vary from samples\ngives an estimate of the error associated with using \\(\\bar{y}\\) to estimate \\(\\mu\\)…"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textci-bary-pm-t-cdot-fracssqrtn",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textci-bary-pm-t-cdot-fracssqrtn",
    "title": "Lecture 04: Probability and Inference",
    "section": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)",
    "text": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)\nWhere:\n\nȳ is sample mean\n𝑛 is sample size\ns is sample standard deviation\nt t-value corresponding the probability of the CI\nt in t-table for different degrees of freedom (n-1)"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "",
    "text": "Introduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\n\nlakemean_lengthsd_lengthse_lengthcountI3265.606128.303783.48395466I8362.598052.339015.182334102\n\n\n\n\n\n\n\nThe goals for today\n\nStatistical inference fundamentals\nHypothesis testing principles\nT Distributions\nOne sample T Tests\nTwo sample T Test"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-4-review-1",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-4-review-1",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "",
    "text": "Introduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-4-review-2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-4-review-2",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "",
    "text": "Introduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\n\nlakemean_lengthsd_lengthse_lengthcountI3265.606128.303783.48395466I8362.598052.339015.182334102"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-5-probability-and-statistical-inference",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#lecture-5-probability-and-statistical-inference",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "",
    "text": "The goals for today\n\nStatistical inference fundamentals\nHypothesis testing principles\nT Distributions\nOne sample T Tests\nTwo sample T Test"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#textci-bary-pm-t-cdot-fracssqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#textci-bary-pm-t-cdot-fracssqrtn",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)",
    "text": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)\nWhere:\n\nȳ is sample mean\n𝑛 is sample size\ns is sample standard deviation\nt t-value corresponding the probability of the CI\nt in t-table for different degrees of freedom (n-1)"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#assumptions-for-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#assumptions-for-t-test",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#statistical-test-of-normality---shapiro-wilk-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#statistical-test-of-normality---shapiro-wilk-test",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Statistical Test of Normality - Shapiro-Wilk test",
    "text": "Statistical Test of Normality - Shapiro-Wilk test\n\n# Shapiro-Wilk test\nshapiro_test &lt;- shapiro.test(i3_df$length_mm)\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  i3_df$length_mm\nW = 0.91051, p-value = 0.0001623\n\n\n\n# Check for outliers using boxplot\n# YOUR CODE HERE\ni3_df %&gt;% ggplot(aes(lake, length_mm))+geom_boxplot()"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)",
    "text": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwhere:\n\nx̄₁ and x̄₂: These represent the sample means of the two groups you’re comparing \ns²ₚ: This is the pooled variance, calculated as: s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups.\nn₁ and n₂: These are the sample sizes of the two groups.\n√(1/n₁ + 1/n₂): This represents the pooled standard error."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracsignalnoise",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracsignalnoise",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(t = \\frac{SIGNAL}{NOISE}\\)",
    "text": "\\(t = \\frac{SIGNAL}{NOISE}\\)\n\n\n\n\n\n\nPractice Exercise 3: Calculate summary statistics grouped by wind exposure\n\n\n\nBefore conducting the test, we need to understand the data for each group.\n\nYou need this and the graph to see what is goin on ….\n\ngroup_summary &lt;- pine_df %&gt;%\n  group_by(wind) %&gt;%\n  summarize(\n    mean_length = mean(length_mm),\n    sd_length = sd(length_mm),\n    n = n(),\n    se_length = sd_length / sqrt(n)\n  )\n\nprint(group_summary)\n\n# A tibble: 2 × 5\n  wind  mean_length sd_length     n se_length\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 lee          20.4      2.45    24     0.500\n2 wind         14.9      1.91    24     0.390\n\n\n\n\n\n\n# Create a boxplot comparing the two sides\npine_wind_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 4: Effect size\n\n\n\nWe could also look at the difference in means… some cool code here\n\n# Assuming your dataframe is called df\ngroup_summary %&gt;%\n  summarize(difference = mean_length[wind == \"wind\"] - mean_length[wind == \"lee\"])\n\n# A tibble: 1 × 1\n  difference\n       &lt;dbl&gt;\n1       -5.5\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 5: Using GGPLOT to get summary stats\n\n\n\nGGplot also has code to make the mean and standard error plots we are interested in along whit a lot of others\n\n# Assuming your dataframe is called df\npine_mean_se_plot &lt;- ggplot(pine_df, aes(x = wind, y = length_mm, color = wind)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Mean Pine Needle Length by Wind Exposure\",\n       x = \"Wind Exposure\",\n       y = \"Mean Length (mm)\") +\n  coord_cartesian(ylim = c(0,25))+\n  scale_color_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\"))+\n  theme_classic()\npine_mean_se_plot"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-4-review-1",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-4-review-1",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Lecture 4: Review",
    "text": "Lecture 4: Review\n\n\n\nIntroduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-4-review-2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-4-review-2",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Lecture 4: Review",
    "text": "Lecture 4: Review\n\nIntroduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\n\nlakemean_lengthsd_lengthse_lengthcountI3265.606128.303783.48395466I8362.598052.339015.182334102"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-5-probability-and-statistical-inference",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#lecture-5-probability-and-statistical-inference",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Lecture 5: Probability and Statistical Inference",
    "text": "Lecture 5: Probability and Statistical Inference\n\n\nThe goals for today\n\nStatistical inference fundamentals\nHypothesis testing principles\nT Distributions\nOne sample T Tests\nTwo sample T Test"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#textci-bary-pm-t-cdot-fracssqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#textci-bary-pm-t-cdot-fracssqrtn",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)",
    "text": "\\(\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}\\)\nWhere:\n\nȳ is sample mean\n𝑛 is sample size\ns is sample standard deviation\nt t-value corresponding the probability of the CI\nt in t-table for different degrees of freedom (n-1)"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#assumptions-for-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#assumptions-for-t-test",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#statistical-test-of-normality---shapiro-wilk-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#statistical-test-of-normality---shapiro-wilk-test",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "Statistical Test of Normality - Shapiro-Wilk test",
    "text": "Statistical Test of Normality - Shapiro-Wilk test\n\n# Shapiro-Wilk test\nshapiro_test &lt;- shapiro.test(i3_df$length_mm)\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  i3_df$length_mm\nW = 0.91051, p-value = 0.0001623\n\n\n\n# Check for outliers using boxplot\n# YOUR CODE HERE\ni3_df %&gt;% ggplot(aes(lake, length_mm))+geom_boxplot()"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)",
    "text": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwhere:\n\nx̄₁ and x̄₂: These represent the sample means of the two groups you’re comparing \ns²ₚ: This is the pooled variance, calculated as: s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups.\nn₁ and n₂: These are the sample sizes of the two groups.\n√(1/n₁ + 1/n₂): This represents the pooled standard error."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#t-fracsignalnoise",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slidesl.html#t-fracsignalnoise",
    "title": "Lecture 05: Probability and Statistical Inference",
    "section": "\\(t = \\frac{SIGNAL}{NOISE}\\)",
    "text": "\\(t = \\frac{SIGNAL}{NOISE}\\)\n\n\n\n\n\n\nPractice Exercise 3: Calculate summary statistics grouped by wind exposure\n\n\nBefore conducting the test, we need to understand the data for each group.\n\nYou need this and the graph to see what is goin on ….\n\ngroup_summary &lt;- pine_df %&gt;%\n  group_by(wind) %&gt;%\n  summarize(\n    mean_length = mean(length_mm),\n    sd_length = sd(length_mm),\n    n = n(),\n    se_length = sd_length / sqrt(n)\n  )\n\nprint(group_summary)\n\n# A tibble: 2 × 5\n  wind  mean_length sd_length     n se_length\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 lee          20.4      2.45    24     0.500\n2 wind         14.9      1.91    24     0.390\n\n\n\n\n\n\n\n# Create a boxplot comparing the two sides\npine_wind_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 4: Effect size\n\n\nWe could also look at the difference in means… some cool code here\n\n# Assuming your dataframe is called df\ngroup_summary %&gt;%\n  summarize(difference = mean_length[wind == \"wind\"] - mean_length[wind == \"lee\"])\n\n# A tibble: 1 × 1\n  difference\n       &lt;dbl&gt;\n1       -5.5\n\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 5: Using GGPLOT to get summary stats\n\n\nGGplot also has code to make the mean and standard error plots we are interested in along whit a lot of others\n\n# Assuming your dataframe is called df\npine_mean_se_plot &lt;- ggplot(pine_df, aes(x = wind, y = length_mm, color = wind)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Mean Pine Needle Length by Wind Exposure\",\n       x = \"Wind Exposure\",\n       y = \"Mean Length (mm)\") +\n  coord_cartesian(ylim = c(0,25))+\n  scale_color_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\"))+\n  theme_classic()\npine_mean_se_plot"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html",
    "href": "lectures/lecture_template/06_02_class_activity.html",
    "title": "06_Class_Activity",
    "section": "",
    "text": "stuff here\nstuff here\nstuff here\nstuff here\n\n\n\n\n\nstuff here\nstuff here\nstuff here\n\nstuff here\nstuff here"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#what-did-we-do-last-time-in-activity-5",
    "href": "lectures/lecture_template/06_02_class_activity.html#what-did-we-do-last-time-in-activity-5",
    "title": "06_Class_Activity",
    "section": "",
    "text": "stuff here\nstuff here\nstuff here\nstuff here"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#todays-focus",
    "href": "lectures/lecture_template/06_02_class_activity.html#todays-focus",
    "title": "06_Class_Activity",
    "section": "",
    "text": "stuff here\nstuff here\nstuff here\n\nstuff here\nstuff here"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#we-need-to-select-only-that-data",
    "href": "lectures/lecture_template/06_02_class_activity.html#we-need-to-select-only-that-data",
    "title": "06_Class_Activity",
    "section": "We need to select only that data",
    "text": "We need to select only that data\n\n# # Filter for Lake NE 12\n# ne12_data &lt;- df %&gt;% \n#   filter(lake == \"NE 12\") %&gt;%\n#   filter(!is.na(mass_g))  # Remove any NA values\n# \n# head(ne12_data)"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#use-patchwork-to-combine-the-plots",
    "href": "lectures/lecture_template/06_02_class_activity.html#use-patchwork-to-combine-the-plots",
    "title": "06_Class_Activity",
    "section": "Use Patchwork to combine the plots",
    "text": "Use Patchwork to combine the plots\n\n# # Combine all plots using patchwork\n# combined_stats_plot &lt;- (ne12_histo_plot + ne12_dot_plot) / (ne12_box_plot + ne12_qq_plot) +\n#   plot_annotation(\n#     title = \"Lake NE 12 Trout Mass Distribution\",\n#     subtitle = paste(\"n =\", nrow(ne12_data), \"fish samples\"),\n#     theme = theme(plot.title = element_text(hjust = 0.5),\n#                   plot.subtitle = element_text(hjust = 0.5))\n#   )\n# \n# # Display the combined plot\n# combined_stats_plot"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#assumptions-for-t-test",
    "href": "lectures/lecture_template/06_02_class_activity.html#assumptions-for-t-test",
    "title": "06_Class_Activity",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_template/06_02_class_activity.html#shapiro-wilk",
    "href": "lectures/lecture_template/06_02_class_activity.html#shapiro-wilk",
    "title": "06_Class_Activity",
    "section": "Shapiro Wilk",
    "text": "Shapiro Wilk\n\n# Shapiro-Wilk test\n# shapiro_test &lt;- shapiro.test(windward_data$len_mm)\n# print(shapiro_test)\n\n\n# Check for outliers using boxplot\n# YOUR CODE HERE"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html",
    "href": "lectures/lecture_02/02_02_class_activity.html",
    "title": "02 Class Activity",
    "section": "",
    "text": "Collected pine needle samples from windward and leeward sides of trees\nIdentified independent variable (wind exposure) and dependent variable (needle length)\nMeasured needle lengths and recorded data\nCreated basic visualizations\nSaved our data for further analysis\n\n\n\n\n\nImplement data pipeline best practices\nApply controlled vocabulary and naming conventions\nCreate effective tables and visualizations\nCustomize plots for publication quality\nCombine multiple plots into composite figures"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#recap-from-activity-1",
    "href": "lectures/lecture_02/02_02_class_activity.html#recap-from-activity-1",
    "title": "02 Class Activity",
    "section": "",
    "text": "Collected pine needle samples from windward and leeward sides of trees\nIdentified independent variable (wind exposure) and dependent variable (needle length)\nMeasured needle lengths and recorded data\nCreated basic visualizations\nSaved our data for further analysis"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#todays-objectives",
    "href": "lectures/lecture_02/02_02_class_activity.html#todays-objectives",
    "title": "02 Class Activity",
    "section": "",
    "text": "Implement data pipeline best practices\nApply controlled vocabulary and naming conventions\nCreate effective tables and visualizations\nCustomize plots for publication quality\nCombine multiple plots into composite figures"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#in-rstudio",
    "href": "lectures/lecture_02/02_02_class_activity.html#in-rstudio",
    "title": "02 Class Activity",
    "section": "In RStudio:",
    "text": "In RStudio:\n\nclick file - open project and select the 2025_UMD_BioStats_Student_Code.Rproj file or double click on it in the finder or data explorer.\nyour screen will now change as RStudio knows where home is\n\n\n\nNote that in the upper right you will see 2025_UMD_BioStats_Student_Code so you know you are in the right spot\nNow click File - New File - Quarto File\n\n\n\nCreate a file that starts with 02_ and then something that will help you know what is going on like 02_class_activity_in_class.qmd\nNow this file thinks this is home.\nSo I usually copy stuff for the header from another file as its just too hard to remember all this…\n\n---\ntitle: \"Title of your file\" # Title of the file\nauthor: \"Your Name\" # who you are\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true # retains the images when you start again\n  fig-height: 3\n  fig-width: 3\n  paged-print: false\nformat:\n  html:\n    freeze: false\n    toc: false\n    output-file: \"02_02_class_activity.html\"\n    default: true\n    embed-resources: true\n    self-contained: true\n    css: ../../css/activity.css\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n---"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#exercise-1-now-to-load-the-libraries",
    "href": "lectures/lecture_02/02_02_class_activity.html#exercise-1-now-to-load-the-libraries",
    "title": "02 Class Activity",
    "section": "Exercise 1: Now to load the libraries",
    "text": "Exercise 1: Now to load the libraries\n\n# install packages -----\n# install.packages(\"readxl\")\n# install.packages(\"tidyverse\")\n\n# # we will install a few new libraries\n# install.packages(\"skimr\")\n\nEach script you run from then on you will load the libraries from within the package.\n\n# Load the libraries ----\nlibrary(readxl) # allows to read in excel files\nlibrary(tidyverse) # provides utilities seen in console\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(skimr) # provide summary stats\nlibrary(janitor) # it cleans ; )\n\n\nAttaching package: 'janitor'\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\nlibrary(patchwork)"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#exercise-2-loading-and-examining-data",
    "href": "lectures/lecture_02/02_02_class_activity.html#exercise-2-loading-and-examining-data",
    "title": "02 Class Activity",
    "section": "Exercise 2: Loading and Examining Data",
    "text": "Exercise 2: Loading and Examining Data\nNow like we did before with x and y we will do this with a spreadsheet from a CSV file or excel file\nWe are going to work with the same data we did in the last class."
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#exercise-3-examining-data",
    "href": "lectures/lecture_02/02_02_class_activity.html#exercise-3-examining-data",
    "title": "02 Class Activity",
    "section": "Exercise 3: Examining Data",
    "text": "Exercise 3: Examining Data\n\n# Load the pine needle data\npine_df &lt;- read_csv(\"data/pine_needles.csv\", na = \"NA\")\n\nRows: 48 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): date, group, n_s, wind\ndbl (2): tree_no, length_mm\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Examine the data structure\nglimpse(pine_df)\n\nRows: 48\nColumns: 6\n$ date      &lt;chr&gt; \"3/20/25\", \"3/20/25\", \"3/20/25\", \"3/20/25\", \"3/20/25\", \"3/20…\n$ group     &lt;chr&gt; \"cephalopods\", \"cephalopods\", \"cephalopods\", \"cephalopods\", …\n$ n_s       &lt;chr&gt; \"n\", \"n\", \"n\", \"n\", \"n\", \"n\", \"s\", \"s\", \"s\", \"s\", \"s\", \"s\", …\n$ wind      &lt;chr&gt; \"lee\", \"lee\", \"lee\", \"lee\", \"lee\", \"lee\", \"wind\", \"wind\", \"w…\n$ tree_no   &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, …\n$ length_mm &lt;dbl&gt; 20, 21, 23, 25, 21, 16, 15, 16, 14, 17, 13, 15, 19, 18, 20, …\n\n# View the first few rows\nhead(pine_df)\n\n# A tibble: 6 × 6\n  date    group       n_s   wind  tree_no length_mm\n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;     &lt;dbl&gt;\n1 3/20/25 cephalopods n     lee         1        20\n2 3/20/25 cephalopods n     lee         1        21\n3 3/20/25 cephalopods n     lee         1        23\n4 3/20/25 cephalopods n     lee         1        25\n5 3/20/25 cephalopods n     lee         1        21\n6 3/20/25 cephalopods n     lee         1        16\n\n# Get a statistical summary\nsummary(pine_df)\n\n     date              group               n_s                wind          \n Length:48          Length:48          Length:48          Length:48         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    tree_no       length_mm    \n Min.   :1.00   Min.   :12.00  \n 1st Qu.:1.75   1st Qu.:15.00  \n Median :2.50   Median :17.50  \n Mean   :2.50   Mean   :17.67  \n 3rd Qu.:3.25   3rd Qu.:20.25  \n Max.   :4.00   Max.   :25.00  \n\n\n\nQuestions to Consider:\n\nWhat variables are in our dataset?\nWhat are their data types?\nAre there any missing values?\nDo the variable names follow consistent conventions?\nHow might we improve the data organization?"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#part-3-building-complex-visualizations-layer-by-layer",
    "href": "lectures/lecture_02/02_02_class_activity.html#part-3-building-complex-visualizations-layer-by-layer",
    "title": "02 Class Activity",
    "section": "Part 3: Building Complex Visualizations Layer by Layer",
    "text": "Part 3: Building Complex Visualizations Layer by Layer\nNow let’s build more sophisticated visualizations by adding layers one at a time:\n\nExercise 4: Building a Layered Plot\n\n# Start with a basic plot\np1 &lt;- pine_df %&gt;%\n  ggplot(aes(x = wind, y = length_mm, fill = wind)) +\n  labs(title = \"Layer 1: Basic Plot Setup\",\n       x = \"Tree Side\", \n       y = \"Length (mm)\")\n\n# Add boxplot layer\np2 &lt;- p1 +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Layer 2: Adding Boxplot\")\n\n# Add individual data points\np3 &lt;- p2 +\n  geom_jitter(width = 0.2, alpha = 0.5, color = \"gray30\") +\n  labs(title = \"Layer 3: Adding Data Points\")\n\n# Add mean indicators\np4 &lt;- p3 +\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  labs(title = \"Layer 4: Adding Mean Indicators\")\n\n# Create a 2x2 grid of the progressive plot building\n(p1 | p2) / (p3 | p4)\n\n\n\n\n\n\n\n\n\n\nDiscussion Points:\n\nHow does each layer contribute to the story our data is telling?\nWhy might we want to show individual data points alongside summary statistics?\nHow does transparency (alpha) help when overlaying multiple elements?"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#part-4-customizing-plots-for-publication",
    "href": "lectures/lecture_02/02_02_class_activity.html#part-4-customizing-plots-for-publication",
    "title": "02 Class Activity",
    "section": "Part 4: Customizing Plots for Publication",
    "text": "Part 4: Customizing Plots for Publication\n\nExercise 5: Adding customization\n\n# Create a fully customized plot\ncolor_plot &lt;- pine_df %&gt;%\n  ggplot(aes(x = wind, y = length_mm, fill = wind)) +\n  # Add violin plots for distribution\n  geom_violin(alpha = 0.4) +\n  # Add boxplots for key statistics\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  # Add individual data points\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"gray30\", size = 2) +\n  # Add mean points\n  labs(\n    title = \"Pine Needle Length Varies with Wind Exposure\",\n    subtitle = \"Needles on the leeward side tend to be longer\",\n    x = \"Tree Side\", \n    y = \"Needle Length (mm)\",\n    caption = \"Data collected Spring 2023\"\n  ) +\n  # Customize colors with a colorblind-friendly palette\n  scale_fill_manual(\n    values = c(\n      \"wind\" = \"#1b9e77\",\n       \"lee\" = \"#d95f02\"\n      ),\n    labels = c(\n      \"wind\" = \"Windward\", \n      \"lee\" = \"Leeward\"\n      )) +\n  # Apply a clean theme\n  theme_minimal() \n\n\n# Display the publication-ready plot\ncolor_plot\n\n\n\n\n\n\n\n\nLet’s create a publication-quality figure by customizing colors, labels, and themes:\n\n\nExercise 6: Creating a Publication-Ready Plot\n\n# Create a fully customized plot\npublication_plot &lt;- pine_df %&gt;%\n  ggplot(aes(x = wind, y = length_mm, fill = wind)) +\n  # Add violin plots for distribution\n  geom_violin(alpha = 0.4) +\n  # Add boxplots for key statistics\n  geom_boxplot(width = 0.2, alpha = 0.7, outlier.shape = NA) +\n  # Add individual data points\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"gray30\", size = 2) +\n  # Add mean points\n  stat_summary(fun = mean, geom = \"point\", shape = 23, size = 3, fill = \"white\") +\n  # Add informative labels\n  labs(\n    title = \"Pine Needle Length Varies with Wind Exposure\",\n    subtitle = \"Needles on the leeward side tend to be longer\",\n    x = \"Tree Side\", \n    y = \"Needle Length (mm)\",\n    caption = \"Data collected Spring 2023\"\n  ) +\n  # Customize colors with a colorblind-friendly palette\n  scale_fill_manual(\n    values = c(\"wind\" = \"#1b9e77\", \"lee\" = \"#d95f02\"),\n    labels = c(\"wind\" = \"Windward\", \"lee\" = \"Leeward\")\n  ) +\n  # Apply a clean theme\n  theme_classic() \n  \n\n# Display the publication-ready plot\npublication_plot\n\n\n\n\n\n\n\n\n\n\nCustomization Elements:\n\nPlot Elements:\n\nViolin plots to show distribution\nBoxplots to show quartiles and median\nIndividual points for transparency\nMean indicators for central tendency\n\nVisual Design:\n\nColorblind-friendly color palette\nThoughtful use of transparency\nClear, informative title and subtitle\nProfessional typography and spacing\n\nAccessibility Considerations:\n\nSufficient contrast\nRedundant encoding (position and color)\nClear labels with units"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#part-5-creating-complex-multi-panel-figures",
    "href": "lectures/lecture_02/02_02_class_activity.html#part-5-creating-complex-multi-panel-figures",
    "title": "02 Class Activity",
    "section": "Part 5: Creating Complex Multi-Panel Figures",
    "text": "Part 5: Creating Complex Multi-Panel Figures\nFinally, let’s create a publication-ready multi-panel figure:\n\ncolor_plot +\n  \n  publication_plot   + \n  plot_layout(ncol = 2) + \n  plot_annotation(tag_levels = \"A\", tag_suffix = \")\")\n\n\n\n\n\n\n\n\n# we can add this to remove things\n# why do this?\n# + theme(\n#     axis.text.y = element_blank(),  # Removes x-axis labels\n#     axis.title.y = element_blank()  # Removes x-axis title"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#summary-and-key-takeaways",
    "href": "lectures/lecture_02/02_02_class_activity.html#summary-and-key-takeaways",
    "title": "02 Class Activity",
    "section": "Summary and Key Takeaways",
    "text": "Summary and Key Takeaways\nIn this activity, we’ve learned how to:\n\nLoad and examine data properly\nCreate basic visualizations to explore patterns\nBuild complex plots layer by layer using ggplot2’s grammar\nCustomize plots for clear communication and visual appeal\nAdd statistical information to support data interpretation\nCombine multiple plots into publication-ready figures\n\n\nBest Practices for Data Visualization:\n\nStart simple, then add complexity as needed\nFocus on the story your data is telling\nUse appropriate plot types for your data structure\nMinimize chart junk and maximize data-ink ratio\nCreate clear, informative labels\nUse color purposefully and with accessibility in mind\nInclude both individual data points and summary statistics when possible\nConsider your audience when designing visualizations"
  },
  {
    "objectID": "lectures/lecture_11/11_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_11/11_01_lecture_powerpoint_html.html",
    "title": "Lecture 11 - Multiple Regression",
    "section": "",
    "text": "Lecture 10: Review\n\n\nCovered\n\nRegression T-Test Anova\nRegression Assumptions\nModel II Regression\n\n\n\n\n\n\n\nLecture 11: Overview\nMultiple Linear Regression model\n\nRegression parameters\nAnalysis of variance\nNull hypotheses\nExplained variance\nAssumptions and diagnostics\nCollinearity\nInteractions\nDummy variables\nModel selection\nImportance of predictors\n\n\n\nLecture 11: Analyses\n\n\nWhat if more than one predictor (X) variable?\n\nIf predictors continuous\nMix between categorical and continuous\nCan use multiple linear regression\n\n\n\n\n\n\nIndependent variable\n\n\n\n\n\nDependent variable\nContinuous\nCategorical\n\n\nContinuous\nRegression\nANOVA\n\n\nCategorical\nLogistic regression\nTabular\n\n\n\n\n\n\n\nLecture 11: Analyses\n\n\nAbundance of C3 grasses can be modeled as function of\n\nlatitude\nlongitude\nboth\n\nInstead of line, modeled with (hyper)plane\n\n\n\n\n\n\nLecture 11: Analyses\n\n\nUsed in similar way to simple linear regression:\n\nDescribe nature of relationship between Y and X’s\nDetermine explained / unexplained variation in Y\nPredict new Ys from X\nFind the “best” model\n\n\nS\n\n\n\n\nLecture 11: Analyses\n\n\nCrawley 2012: “Multiple regression models provide some of the most profound challenges faced by the analyst”:\n\nOverfitting\nParameter proliferation\nMulticollinearity\nModel selection\n\n\n\n\n\n\n\nLecture 11: Analyses\nMultiple Regression:\n\nSet of i= 1 to n observations\nfixed X-values for p predictor variables (X1, X2…Xp)\nrandom Y-values:\n\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i\\]\n\nyi: value of Y for ith observation X1 = xi1, X2 = xi2,…, Xp = xip\nβ0: population intercept, the mean value of Y when X1 = 0, X2 = 0,…, Xp = 0\n\n\n\nLecture 11: Multiple linear regression model\nMultiple Regression:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i\\]\n\nβ1: partial population slope, change in Y per unit change in X1 holding other X-vars constant\nβ2: partial population slope, change in Y per unit change in X2 holding other X-vars constant\nβp: partial population slope, change in Y per unit change in Xp holding other X-vars constant\n\n\n\nLecture 11: Regression parameters\nMultiple Regression:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i\\]\n\nεi: unexplained error - difference bw yi and value predicted by model (ŷi)\nNPP = β0 + β1(lat) + β2 (long) + β3 (soil fertility) + εi\n\n\n\nLecture 11: Regression parameters\nMultiple Regression:\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i\\]\n\nEstimate multiple regression parameters (intercept, partial slopes) using OLS to fit the regression line\nOLS minimize ∑(yi-ŷi)2, the SS (vertical distance) between observed yi and predicted ŷi for each xij\nε estimated as residuals: εi = yi-ŷi\nCalculation solves set of simultaneous normal equations with matrix algebra\n\n\n\nLecture 11: Regression parameters\nRegression equation can be used for prediction by subbing new values for predictor (X) variables\n\nConfidence intervals calculated for parameters\nConfidence and prediction intervals depend on number of observations and number of predictors\n\nMore observations decrease interval width\nMore predictors increase interval width\n\nPrediction should be restricted to within range of X variables\n\n\n\nLecture 11: Analyses of variance\nVariance - SStotal partitioned into SSregression and SSresidual\n\nSSregression is variance in Y explained by model\nSSresidual is variance not explained by model\n\n\n\n\n\n\n\n\n\n\n\nSource of variation\nSS\ndf\nMS\nInterpretation\n\n\n\n\nRegression\n\\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)\n\\(p\\)\n\\(\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{p}\\)\nDifference between predicted observation and mean\n\n\nResidual\n\\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\n\\(n-p-1\\)\n\\(\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-p-1}\\)\nDifference between each observation and predicted\n\n\nTotal\n\\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)\n\\(n-1\\)\n\nDifference between each observation and mean\n\n\n\n\n\nLecture 11: Analyses\nSS converted to non-additive MS (SS/df)\n\nMSresidual: estimate population variance\nMSregression: estimate population variance + variation due to strength of X-Y relationships\nMS do not depend on sample size\n\n\n\n\n\n\n\n\n\n\nSource of variation\nSS\ndf\nMS\n\n\n\n\nRegression\n\\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)\n\\(p\\)\n\\(\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{p}\\)\n\n\nResidual\n\\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\\)\n\\(n-p-1\\)\n\\(\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-p-1}\\)\n\n\nTotal\n\\(\\sum_{i=1}^{n} (y_i - \\bar{y})^2\\)\n\\(n-1\\)\n\n\n\n\n\n\nLecture 11: Hypotheses\nTwo Hos usually tested in MLR:\n\n“Basic” Ho: all partial regression slopes equal 0; β1 = β2 = … = βp = 0\nIf “basic” Ho true, MSregression and MSresidual estimate variance and their ratio (F-ratio) = 1\nIf “basic” Ho false (at least one β ≠ 0) MSregression estimates variance + partial regression slope and their ratio (F-ratio)\nwill be &gt; 1 - F-ratio compared to F-distribution for p-value\n\n\n\nLecture 11: Hypotheses\nAlso: is any specific β = 0 (explanatory role)?\n\nE.g., does LAT have effect on NPP?\nThese Hs tested through model comparison\nModel w 3 predictors X1, X2,X3 (model 1):\nyi= β0 +β1xi1+β2xi2+β3xi3+ εi\nTo test Ho that β1 = 0 compare fit of model 1 to model 2:\nyi= β0 +β2xi2+β3xi3+ εi\n\n\n\nLecture 11: Hypotheses\n\nIf SSregression of mod1=mod2, cannot reject Ho β1 = 0\nIf SSregression of mod1 &gt; mod2, evidence to reject Ho β1 = 0\nSS for β1 is SSextraβ1 = Full SSregression - Reduced SSregression\nUse partial F-test to test Ho β1 = 0 :\n\n\\[F_{w,n-p} = \\frac{MS_{Extra}}{FULL\\ MS_{Residual}}  \\] Can also use t-test (R provides this value)\n\n\nLecture 11: Explained variance\nExplained variance (r2) is calculated the same way as for simple regression:\n\\[r^2 = \\frac{SS_{Regression}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}  \\]\n\nr2 values can not be used to directly compare models\nr2 values will always increase as predictors added\nr2 values with different transformation will differ\n\n\n\nLecture 11: Assumptions and diagnostics\n\n\n\nAssume fixed Xs; unrealistic in most biological settings\nNo major (influential) outliers\nCheck leverage, influence- Cook’s Di\n\n\n\n\n\n\n\nLecture 11: Assumptions and diagnostics\n\n\n\nNormality, equal variance, independence\nResidual QQ-plots, residuals vs. predicted values plot\nDistribution/variance often corrected by transforming Y\n\n\n\n\n\n\n\nLecture 11: Assumptions and diagnostics\nMore observations than predictor variables\n\nIdeally at least 10x observations than predictors to avoid “overfitting”\nUncorrelated predictor variables (assessed using scatterplot matrix; VIFs)\nLinear relationship between Y and each X, holding others constant (non-linearity assessed by AV plots)\n\n\n\nLecture 11: Analyses\n\n\nRegression of Y vs. each X does not consider effect of other predictors:\nwant to know shape of relationship while holding other predictors constant\n\n\n\n\n\n\nLecture 11: Collinearity\n\nPotential predictor variables are often correlated (e.g., morphometrics, nutrients, climatic parameters)\nMulticollinearity (strong correlation between predictors) causes problems for parameter estimates\nSevere collinearity causes unstable parameter estimates: small change in a single value can result in large changes in βp - estimates\nInflates partial slope error estimates, loss of power\n\n\n\n\nLecture 11: Collinearity\nCollinearity can be detected by:\n\nVariance inflation Factors:\n\nVIF for Xj=1/ (1-r2 )\nVIF &gt; 10 = bad\n\nBest/simplest solution:\n\nexclude variables that are highly correlated with other variables\nthey are probably measuring similar\nthing and are redundant\n\n\n\n\nLecture 11: Interactions\nPredictors can be modeled as:\n\nadditive (effect of temp, plus precip, plus fertility) or\nmultiplicative (interactive)\nInteraction: effect of Xi depends on levels of Xj\nThe partial slope of Y vs. X1 is different for different levels of X2 (and vice versa); measured by β3\n\n\\[y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\epsilon_i \\quad \\text{vs.} \\quad y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + + \\beta_3X_{i3} \\epsilon_i\\]\n“Curvature” of the regression (hyper)plane\n\n\nLecture 11: Analyses\n\n\n\nLecture 11: Analyses\nAdding interactions:\n\nmany more predictors (“parameter proliferation”):\n2n; 6 params= 64 terms; 7 params= 128\ninterpretation more complex\nWhen to include interactions? When they make biological sense\n\n\n\nLecture 11: Dummy variables\nMultiple Linear Regression accommodates continuous and categorical variables (gender, vegetation type, etc.) Categorical vars as “dummy vars”, n of dummy variables = n-1 categories\nSex M/F:\n\nNeed 1 dummy var with two values (0, 1)\n\nFertility L/M/H:\n\nNeed 2 dummy var, each with two values (0, 1): fert1 (0 if L or H, 1 if M), fert2 (1 if H, 0 if L or M)\n\n\n\n\nFertility\nfert1\nfert2\n\n\n\n\nLow\n0\n0\n\n\nMed\n1\n0\n\n\nHigh\n0\n1\n\n\n\n\n\nLecture 11: Analyses\nCoefficients interpreted relative to reference condition\n\nR codes dummy variables automatically\npicks “reference” level alphabetically\nDummy variables with more than 2 levels add extra predictor variables to model\n\n\n\n\nFertility\nfert1\nfert2\n\n\n\n\nLow\n0\n0\n\n\nMed\n1\n0\n\n\nHigh\n0\n1\n\n\n\n\n\nLecture 11: Analyses\n\n\n\n\nS\n\n\n\n\nLecture 11: Comparing models\nWhen have multiple predictors (and interactions!)\n\nhow to choose “best” model?\nWhich predictors to include?\nOccam’s razor: “best” model balances complexity with fit to data\n\nTo chose:\n\ncompare “nested” models\n\nOverfitting\n\ngetting high r2 just by having more (useless predictors)\nso r2 is not a good way of choosing between nested models\n\n\n\nLecture 11: Comparing models\nNeed to account for increase in fit with added predictors:\n\nAdjusted r2\nAkaike’s information criterion (AIC)\nBoth “penalize” models for extra predictors\nHigher adjusted r2 and lower AIC are better when comparing models\n\n\\[\\text{Adjusted } r^2 = 1 - \\frac{SS_{\\text{Residual}}/(n - (p + 1))}{SS_{\\text{Total}}/(n - 1)}\\] \\[\\text{Akaike Information Criterion (AIC)} = n[\\ln(SS_{\\text{Residual}})] + 2(p + 1) - n\\ln(n)\\]\n\n\nLecture 11: Comparing models\nBut how to compare models?\n\nCan fit all possible models\n\ncompare AICs or adj- r2,\ntedious w lots of predictors\n\nAutomated forward (and backward) stepwise procedures: start w no terms (all terms), add (remove) terms w largest (smallest)\n\npartial F statistic\n\n\nWe will use manual form of backward selection\n\n\nLecture 11: Analyses\n\n\n\n\n\nLecture 11: Predictors\nUsually want to know relative importance of predictors to explaining Y\n\nThree general approaches:\nUsing F-tests (or t-tests) on partial regression slopes\nUsing coefficient of partial determination\nUsing standardized partial regression slopes\n\n\n\nLecture 11: Predictors\nUsing F-tests (or t-tests) on partial regression slopes:\n\nConduct F tests of Ho that each partial regression slope = 0\nIf cannot reject Ho, discard predictor\nCan get additional clues from relative size of F-values\nDoes not tell us absolute importance of predictor (usually can not directly compare slope parameters)\n\n\n\nLecture 11: Predictors\nUsing coefficient of partial determination:\n\nthe reduction in variation of Y due to addition of predictor (Xj)\n\n\\[r_{X_j}^2 = \\frac{SS_{\\text{Extra}}}{\\text{Reduced }SS_{\\text{Residual}}}\\]\nSSextra\n\nIncreased in SSregression when Xj is added to model\nReduced SSresidual is the unexplained SS from model without Xj\n\n\n\nLecture 11: Predictors\nUsing standardized partial regression slopes:\n\npredictors of predictor variables can not be directly compared\nWhy?\nStandardize all vars (mean = 0, sd= 1)\nScales are identical and larger PRS mean more important variable\n\n\n\nLecture 11: Predictors\n\n\nUsing partial r2 values:\n\n\n\n\n\n\nLecture 11: Reporting results\n\n\nResults are easiest to report in tabular format\n\n\n\n\n\n\nLecture 11: Reporting results\n\n\nResults are easiest to report in tabular format\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_html.html",
    "title": "Lecture 20 - xxxxxx",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 20 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#key-principles",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#key-principles",
    "title": "Lecture 20 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#assumptions",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_html.html#assumptions",
    "title": "Lecture 20 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#review",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#review",
    "title": "Lecture 20 - xxxxxx",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 20 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#key-principles",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#key-principles",
    "title": "Lecture 20 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#assumptions",
    "href": "lectures/lecture_20/20_01_lecture_powerpoint_slides.html#assumptions",
    "title": "Lecture 20 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_html.html",
    "title": "Lecture 19 - xxxxxx",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 19 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#key-principles",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#key-principles",
    "title": "Lecture 19 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#assumptions",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_html.html#assumptions",
    "title": "Lecture 19 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#review",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#review",
    "title": "Lecture 19 - xxxxxx",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 19 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#key-principles",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#key-principles",
    "title": "Lecture 19 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#assumptions",
    "href": "lectures/lecture_19/19_01_lecture_powerpoint_slides.html#assumptions",
    "title": "Lecture 19 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "",
    "text": "Covered\n\nCorrelation analysis: measuring relationships between variables\nThe distinction between correlation and regression\nSimple linear regression: predicting one variable from another\nEstimating and interpreting regression parameters\nTesting assumptions and handling violations\nAnalysis of variance in regression\nModel selection and comparison"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept) sometimes \\(\\beta_0\\)\nb is the estimate of β (slope) sometimes \\(\\beta_1\\)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model-1",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model-1",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept) sometimes \\(\\beta_0\\)\nb is the estimate of β (slope) sometimes \\(\\beta_1\\)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model-2",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#simple-linear-regression-model-2",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\nmale lions develop more black pigmentation on their noses as they age.\ncan be used to estimate the age of lions in the field.\n\n\n\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#using-t-statistic",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#using-t-statistic",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Using t-statistic:",
    "text": "Using t-statistic:\n\\[t=\\frac{b_1-\\theta}{s_{b_{1}}}\\]\n\n\\(s_{b_{1}}\\)= Standard error of slope estimate\nBo= 0: t-test: \\(t=\\frac{b_o}{s_{b_{o}}}\\)\n1 parameter t-test, where testing whether β1 =0\nt-statistic test is more general\nR can provide both\nCan also ask whether β0 =0 using t-test\nor whether two regression lines are significantly different"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#using-f-ratio",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#using-f-ratio",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Using F-ratio:",
    "text": "Using F-ratio:\n\\[F = \\frac {MS_{regression}}{MS_{residual}}\\]\n\nif β1 = 0, ratio will be = 1 otherwise &gt;1\ncompare F-ratio to df-specific F-distribution\ndecide how likely obtain our F-ratio by chance"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#detailed-explanation-of-model-ii-regression-types",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#detailed-explanation-of-model-ii-regression-types",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Detailed Explanation of Model II Regression Types",
    "text": "Detailed Explanation of Model II Regression Types\n\nStandardized Major Axis (SMA)\n\n\nSMA regression minimizes the product of the vertical and horizontal distances from the points to the regression line. It’s implemented in the smatr package with method=“SMA”. SMA is appropriate when the measurement scales of X and Y are different.\n\n\nMajor Axis (MA)\n\n\nMA regression minimizes the perpendicular distances from the data points to the regression line. It’s implemented in the smatr package with method=“MA”. MA is appropriate when X and Y are measured in the same units.\n\n\nReduced Major Axis (RMA)\n\n\nRMA regression (also called geometric mean regression) is available in the lmodel2 package. It produces a slope that is the geometric mean of the OLS regression slopes of Y on X and X on Y (specifically, it equals the OLS slope of Y on X multiplied by the sign of the correlation between X and Y, divided by the square root of the R² value)."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#when-to-use-each-method",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#when-to-use-each-method",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "When to Use Each Method",
    "text": "When to Use Each Method\nOLS (Model I) - Use when:\n\nX is measured without error\nThe research goal is predicting Y from X\nThere’s a clear dependent variable\n\nMA (Major Axis) - Use when:\n\nX and Y are measured in the same units\nBoth variables have similar error variances\nThe goal is to understand the symmetric relationship\n\nSMA (Standardized Major Axis) - Use when:\n\nX and Y are measured in different units\nThe goal is to understand the structural relationship\nYou want to test for isometry or allometry in scaling studies\n\nRMA (Reduced Major Axis) - Use when:\n\nThe ratio of error variances is approximately equal to the ratio of the true variances\nBoth variables contain measurement error\nNeither variable is clearly dependent or independent"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#key-differences-in-results",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#key-differences-in-results",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Key Differences in Results",
    "text": "Key Differences in Results\nThe slopes of these methods will typically follow this pattern when the correlation coefficient is less than 1: OLS slope &lt; MA slope &lt; RMA slope &lt; inverse of OLS (X on Y) slope This is particularly evident when the correlation between X and Y is weaker. As correlation approaches 1, the differences between methods diminish."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#decision-tree",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_html.html#decision-tree",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Decision Tree",
    "text": "Decision Tree\nHere’s a simplified decision tree:\n-Are X and Y measured with error? If No → Use OLS (Model I) -Are the errors in X and Y approximately equal? If Yes → Use MA -Are X and Y measured in different units/scales? If Yes → Consider SMA -Is the correlation between X and Y weak (&lt;0.7)? If Yes → Method choice is critical; consider RMA -Are you uncertain about error structure? If Yes → RMA is a reasonable compromise\nRemember that when the correlation between X and Y is very strong (r &gt; 0.9), all methods will yield similar results, making the choice less critical. The differences between methods become more pronounced as the correlation weakens.\nFinally, it’s often valuable to run multiple methods and compare the results. If they lead to different ecological or biological interpretations, this should be explicitly addressed in your discussion."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept) sometimes \\(\\beta_0\\)\nb is the estimate of β (slope) sometimes \\(\\beta_1\\)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model-1",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model-1",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept) sometimes \\(\\beta_0\\)\nb is the estimate of β (slope) sometimes \\(\\beta_1\\)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model-2",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#simple-linear-regression-model-2",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\nmale lions develop more black pigmentation on their noses as they age.\ncan be used to estimate the age of lions in the field.\n\n\n\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#using-t-statistic",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#using-t-statistic",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Using t-statistic:",
    "text": "Using t-statistic:\n\\[t=\\frac{b_1-\\theta}{s_{b_{1}}}\\]\n\n\\(s_{b_{1}}\\)= Standard error of slope estimate\nBo= 0: t-test: \\(t=\\frac{b_o}{s_{b_{o}}}\\)\n1 parameter t-test, where testing whether β1 =0\nt-statistic test is more general\nR can provide both\nCan also ask whether β0 =0 using t-test\nor whether two regression lines are significantly different"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#using-f-ratio",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#using-f-ratio",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Using F-ratio:",
    "text": "Using F-ratio:\n\\[F = \\frac {MS_{regression}}{MS_{residual}}\\]\n\nif β1 = 0, ratio will be = 1 otherwise &gt;1\ncompare F-ratio to df-specific F-distribution\ndecide how likely obtain our F-ratio by chance"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#detailed-explanation-of-model-ii-regression-types",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#detailed-explanation-of-model-ii-regression-types",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Detailed Explanation of Model II Regression Types",
    "text": "Detailed Explanation of Model II Regression Types\n\nStandardized Major Axis (SMA)\n\n\nSMA regression minimizes the product of the vertical and horizontal distances from the points to the regression line. It’s implemented in the smatr package with method=“SMA”. SMA is appropriate when the measurement scales of X and Y are different.\n\n\nMajor Axis (MA)\n\n\nMA regression minimizes the perpendicular distances from the data points to the regression line. It’s implemented in the smatr package with method=“MA”. MA is appropriate when X and Y are measured in the same units.\n\n\nReduced Major Axis (RMA)\n\n\nRMA regression (also called geometric mean regression) is available in the lmodel2 package. It produces a slope that is the geometric mean of the OLS regression slopes of Y on X and X on Y (specifically, it equals the OLS slope of Y on X multiplied by the sign of the correlation between X and Y, divided by the square root of the R² value)."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#when-to-use-each-method",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#when-to-use-each-method",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "When to Use Each Method",
    "text": "When to Use Each Method\nOLS (Model I) - Use when:\n\nX is measured without error\nThe research goal is predicting Y from X\nThere’s a clear dependent variable\n\nMA (Major Axis) - Use when:\n\nX and Y are measured in the same units\nBoth variables have similar error variances\nThe goal is to understand the symmetric relationship\n\nSMA (Standardized Major Axis) - Use when:\n\nX and Y are measured in different units\nThe goal is to understand the structural relationship\nYou want to test for isometry or allometry in scaling studies\n\nRMA (Reduced Major Axis) - Use when:\n\nThe ratio of error variances is approximately equal to the ratio of the true variances\nBoth variables contain measurement error\nNeither variable is clearly dependent or independent"
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#key-differences-in-results",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#key-differences-in-results",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Key Differences in Results",
    "text": "Key Differences in Results\nThe slopes of these methods will typically follow this pattern when the correlation coefficient is less than 1: OLS slope &lt; MA slope &lt; RMA slope &lt; inverse of OLS (X on Y) slope This is particularly evident when the correlation between X and Y is weaker. As correlation approaches 1, the differences between methods diminish."
  },
  {
    "objectID": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#decision-tree",
    "href": "lectures/lecture_10/10_01_lecture_powerpoint_slides.html#decision-tree",
    "title": "Lecture 10 - Regression and Linear Models",
    "section": "Decision Tree",
    "text": "Decision Tree\nHere’s a simplified decision tree:\n-Are X and Y measured with error? If No → Use OLS (Model I) -Are the errors in X and Y approximately equal? If Yes → Use MA -Are X and Y measured in different units/scales? If Yes → Consider SMA -Is the correlation between X and Y weak (&lt;0.7)? If Yes → Method choice is critical; consider RMA -Are you uncertain about error structure? If Yes → RMA is a reasonable compromise\nRemember that when the correlation between X and Y is very strong (r &gt; 0.9), all methods will yield similar results, making the choice less critical. The differences between methods become more pronounced as the correlation weakens.\nFinally, it’s often valuable to run multiple methods and compare the results. If they lead to different ecological or biological interpretations, this should be explicitly addressed in your discussion."
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html",
    "href": "lectures/lecture_07/07_02_class_activity.html",
    "title": "06_Class_Activity",
    "section": "",
    "text": "This document demonstrates statistical analysis of lake trout mass data from Island Lake and NE 12, focusing on:\n\nTesting assumptions for parametric tests\nTransforming data when assumptions aren’t met\nRunning different types of tests:\n\nStandard t-test\nLog-transformed t-test\nWelch’s t-test\nMann-Whitney Wilcoxon test\nPermutation test\n\nInterpreting and reporting results properly"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#introduction",
    "href": "lectures/lecture_07/07_02_class_activity.html#introduction",
    "title": "06_Class_Activity",
    "section": "",
    "text": "This document demonstrates statistical analysis of lake trout mass data from Island Lake and NE 12, focusing on:\n\nTesting assumptions for parametric tests\nTransforming data when assumptions aren’t met\nRunning different types of tests:\n\nStandard t-test\nLog-transformed t-test\nWelch’s t-test\nMann-Whitney Wilcoxon test\nPermutation test\n\nInterpreting and reporting results properly"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#assumptions-for-t-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#assumptions-for-t-test",
    "title": "06_Class_Activity",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#use-patchwork-to-combine-the-plots",
    "href": "lectures/lecture_07/07_02_class_activity.html#use-patchwork-to-combine-the-plots",
    "title": "06_Class_Activity",
    "section": "Use Patchwork to combine the plots",
    "text": "Use Patchwork to combine the plots\n\n# Combine all plots using patchwork\ncombined_stats_plot &lt;- (ne12_histo_plot + ne12_dot_plot) / (ne12_box_plot + ne12_qq_plot) +\n  plot_annotation(\n    title = \"Lake NE 12 Trout Mass Distribution\",\n    subtitle = paste(\"n =\", nrow(ne12_df), \"fish samples\"),\n    theme = theme(plot.title = element_text(hjust = 0.5),\n                  plot.subtitle = element_text(hjust = 0.5))\n  )\n\n# Display the combined plot\ncombined_stats_plot"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#shapiro-wilks-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#shapiro-wilks-test",
    "title": "06_Class_Activity",
    "section": "Shapiro-Wilk’s Test",
    "text": "Shapiro-Wilk’s Test\n\n# Shapiro-Wilk test\nshapiro_test &lt;- shapiro.test(ne12_df$mass_g)\nprint(shapiro_test)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne12_df$mass_g\nW = 0.85148, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#informal-normality-test---often-better",
    "href": "lectures/lecture_07/07_02_class_activity.html#informal-normality-test---often-better",
    "title": "06_Class_Activity",
    "section": "Informal Normality test - often better",
    "text": "Informal Normality test - often better\n\n\n\n\n\n\nExercise: check normality\n\n\n\nAlways do a qq plot\nIn a QQ plot, points that follow the line indicate data that follows a normal distribution. Deviations from the line suggest non-normality.\n\n# Create QQ plots for each lake to check normality\nqq_plot &lt;- island_ne12_df %&gt;% \n  ggplot(aes(sample = mass_g, color = lake)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"QQ Plot for Normality Check\", \n       x = \"Theoretical Quantiles\", \n       y = \"Sample Quantiles\") +\n  theme_minimal() +\n  facet_wrap(~lake)  # Create separate plots for each lake\n\n# Show the QQ plot\nqq_plot"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#formal-normality-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#formal-normality-test",
    "title": "06_Class_Activity",
    "section": "Formal normality test",
    "text": "Formal normality test\n\n\n\n\n\n\nExercise: do a Shapiro-Wilk Test\n\n\n\nWe can do a formal test for a p value\nNote island looks non normal in the qqplot but its really close with the Shapiro-Wilk test…\n\n# Formal test for normality: Shapiro-Wilk test\n# We'll do this for each lake separately\n\n# For NE 12\nne12_data &lt;- island_ne12_df %&gt;% \n  filter(lake == \"NE 12\") %&gt;%\n  pull(mass_g)\n\n# For Island Lake\nisland_data &lt;- island_ne12_df %&gt;% \n  filter(lake == \"Island Lake\") %&gt;%\n  pull(mass_g)\n\n# Run Shapiro-Wilk test\nshapiro_ne12 &lt;- shapiro.test(ne12_data)\nshapiro_island &lt;- shapiro.test(island_data)\n\n# Show results\ncat(\"Shapiro-Wilk normality test for NE 12:\\n\")\n\nShapiro-Wilk normality test for NE 12:\n\nprint(shapiro_ne12)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne12_data\nW = 0.85148, p-value &lt; 2.2e-16\n\ncat(\"\\nShapiro-Wilk normality test for Island Lake:\\n\")\n\n\nShapiro-Wilk normality test for Island Lake:\n\nprint(shapiro_island)\n\n\n    Shapiro-Wilk normality test\n\ndata:  island_data\nW = 0.84102, p-value = 0.04538"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#equality-of-variance-test---levenes-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#equality-of-variance-test---levenes-test",
    "title": "06_Class_Activity",
    "section": "Equality of variance test - Levene’s Test",
    "text": "Equality of variance test - Levene’s Test\n\n\n\n\n\n\nExercise: test for equal variances\n\n\n\nAgain we want the P value not significant\nThe Levene’s test has the following null hypothesis: - H₀: The variances are equal across groups - H₁: The variances are not equal across groups\nIf the p-value is less than 0.05, we reject the null hypothesis and conclude the variances are not equal.\n\n# Formal test for equal variances: Levene's test\nlevene_result &lt;- leveneTest(mass_g ~ lake, data = island_ne12_df)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_result)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1  25.997 5.775e-07 ***\n      330                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#now-look-at-histograms-of-logged-data",
    "href": "lectures/lecture_07/07_02_class_activity.html#now-look-at-histograms-of-logged-data",
    "title": "06_Class_Activity",
    "section": "Now look at histograms of logged data",
    "text": "Now look at histograms of logged data\n\n\n\n\n\n\nExercise: histogram of transformed data\n\n\n\nWe need to see if it worked\n\n# Create histograms of log-transformed data\nlog_hist_plot &lt;- island_ne12_df %&gt;% \n  ggplot(aes(x = log_mass, fill = lake)) +\n  geom_histogram(bins = 20, alpha = 0.7) +\n  labs(title = \"Distribution of Log-Transformed Lake Trout Mass\", \n       x = \"Log10 Mass\", \n       y = \"Count\") +\n  theme_minimal() +\n  facet_wrap(~lake, scales = \"free_y\")\n\n# Show the log-transformed histogram\nlog_hist_plot"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#now-a-qqplot---we-will-skip-shapiro-wilk-this-time",
    "href": "lectures/lecture_07/07_02_class_activity.html#now-a-qqplot---we-will-skip-shapiro-wilk-this-time",
    "title": "06_Class_Activity",
    "section": "Now a qqplot - we will skip Shapiro-Wilk this time ; )",
    "text": "Now a qqplot - we will skip Shapiro-Wilk this time ; )\n\n\n\n\n\n\nExercise: do a qqplot of transformed data\n\n\n\nWe could also look at the difference in means… some cool code here\n\n# QQ plot for log-transformed data\nlog_qq_plot &lt;- island_ne12_df %&gt;% \n  ggplot(aes(sample = log_mass, color = lake)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(title = \"QQ Plot for Log-Transformed Data\", \n       x = \"Theoretical Quantiles\", \n       y = \"Sample Quantiles\") +\n  theme_minimal() +\n  facet_wrap(~lake)\n\n# Show the log QQ plot\nlog_qq_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise: Shapiro-Wilk test\n\n\n\n\n# Check normality of log-transformed data using Shapiro-Wilk test\n# For NE 12\nlog_ne12 &lt;- island_ne12_df %&gt;% \n  filter(lake == \"NE 12\") %&gt;%\n  pull(log_mass)\n\n# For Island Lake\nlog_island &lt;- island_ne12_df %&gt;% \n  filter(lake == \"Island Lake\") %&gt;%\n  pull(log_mass)\n\n# Run Shapiro-Wilk test on log-transformed data\nshapiro_log_ne12 &lt;- shapiro.test(log_ne12)\nshapiro_log_island &lt;- shapiro.test(log_island)\n\n# Show results\ncat(\"Shapiro-Wilk normality test for log-transformed NE 12 data:\\n\")\n\nShapiro-Wilk normality test for log-transformed NE 12 data:\n\nprint(shapiro_log_ne12)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_ne12\nW = 0.95384, p-value = 1.583e-08\n\ncat(\"\\nShapiro-Wilk normality test for log-transformed Island Lake data:\\n\")\n\n\nShapiro-Wilk normality test for log-transformed Island Lake data:\n\nprint(shapiro_log_island)\n\n\n    Shapiro-Wilk normality test\n\ndata:  log_island\nW = 0.93396, p-value = 0.4879\n\n\n\n\n\n\n\n\n\n\nExercise: Levenes test\n\n\n\n\n# Check for equal variances in log-transformed data\nlevene_log_result &lt;- leveneTest(log_mass ~ lake, data = island_ne12_df)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_log_result)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(&gt;F)    \ngroup   1   11.77 0.0006784 ***\n      330                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-grins-lets-do-the-two-sample-t-test-anyway",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-grins-lets-do-the-two-sample-t-test-anyway",
    "title": "06_Class_Activity",
    "section": "For grins lets do the Two Sample T Test anyway",
    "text": "For grins lets do the Two Sample T Test anyway\n\n\n\n\n\n\nExercise: Two sample T Test on regular data\n\n\n\nTry a t test\n\n# Run a standard two-sample t-test\nt_test_result &lt;- t.test(\n  mass_g ~ lake, \n  data = island_ne12_df,\n  var.equal = TRUE,  # Assumes equal variances\n  alternative = \"two.sided\"\n)\n\n# Show the results\nprint(t_test_result)\n\n\n    Two Sample t-test\n\ndata:  mass_g by lake\nt = 14.181, df = 330, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group Island Lake and group NE 12 is not equal to 0\n95 percent confidence interval:\n 2266.304 2996.360\nsample estimates:\nmean in group Island Lake       mean in group NE 12 \n                3165.0000                  533.6677 \n\n\n\n\n\n\n\n\n\n\nExercise: Two sample T Test on transfomrated data\n\n\n\nTry a t test\n\n# Run a t-test on log-transformed data\nlog_t_test_result &lt;- t.test(\n  log_mass ~ lake, \n  data = island_ne12_df,\n  var.equal = TRUE,  # Assumes equal variances\n  alternative = \"two.sided\"\n)\n\n# Show the results\nprint(log_t_test_result)\n\n\n    Two Sample t-test\n\ndata:  log_mass by lake\nt = 5.8192, df = 330, p-value = 1.4e-08\nalternative hypothesis: true difference in means between group Island Lake and group NE 12 is not equal to 0\n95 percent confidence interval:\n 0.6614902 1.3371216\nsample estimates:\nmean in group Island Lake       mean in group NE 12 \n                 3.457554                  2.458248 \n\n\n\n\n\n\n\n\n\n\nExercise: Looking at results of log10 data\n\n\n\nWhen analyzing log-transformed data:\n\nThe mean of log-transformed data, when back-transformed, gives the geometric mean (not the arithmetic mean)\nThe back-transformed confidence intervals represent the confidence interval for the geometric mean\nReport results like: “The geometric mean mass of lake trout in NE 12 was X g (95% CI: Y-Z)”\nNote you can’t take the 10^SE to get the standard errors but rather you need to get the mean - seand the mean + se and then backtransform…\n\n\n# Calculate back-transformed means and confidence intervals\n# This converts log values back to original scale\nback_transformed &lt;- island_ne12_df %&gt;%\n  group_by(lake) %&gt;%\n  summarise(\n    n = n(),\n    mean_log = mean(log_mass),\n    sd_log = sd(log_mass),\n    se_log = sd_log / sqrt(n),\n    # Back-transform mean\n    geometric_mean = 10^mean_log,\n    # Back transform SE\n     lower_se = 10^(mean_log -se_log),\n    upper_se = 10^(mean_log + se_log),\n    # Back-transform confidence intervals (approximate method)\n    lower_ci = 10^(mean_log - qt(0.975, n-1) * se_log),\n    upper_ci = 10^(mean_log + qt(0.975, n-1) * se_log)\n  )\n\n# Show back-transformed results\nprint(back_transformed)\n\n# A tibble: 2 × 10\n  lake        n mean_log sd_log se_log geometric_mean lower_se upper_se lower_ci\n  &lt;chr&gt;   &lt;int&gt;    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1 Island…    10     3.46  0.195 0.0618          2868.    2487.    3307.    2078.\n2 NE 12     322     2.46  0.541 0.0302           287.     268.     308.     251.\n# ℹ 1 more variable: upper_ci &lt;dbl&gt;"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#now-plot-the-back-transformed-data",
    "href": "lectures/lecture_07/07_02_class_activity.html#now-plot-the-back-transformed-data",
    "title": "06_Class_Activity",
    "section": "Now plot the back transformed data",
    "text": "Now plot the back transformed data\nIn some cases the error bars are not summetrical\n\n\n\n\n\n\nExercise:\n\n\n\nTry\n\n# Create a plot showing geometric means with SE bars\ngeo_mean_plot &lt;- back_transformed %&gt;% \n  ggplot(aes(x = lake, y = geometric_mean, fill = lake)) +\n  # Add bars for geometric means\n  geom_bar(stat = \"identity\", width = 0.5, alpha = 0.7) +\n  # Add error bars for standard error\n  geom_errorbar(aes(ymin = lower_se, ymax = upper_se), \n                width = 0.2, linewidth = 1) +\n  # Add labels and title\n  labs(title = \"Geometric Mean Lake Trout Mass with Standard Error\",\n       subtitle = \"Back-transformed from log10 scale\",\n       x = \"Lake\",\n       y = \"Geometric Mean Mass (g)\") +\n  # Use a clean theme\n  theme_minimal() +\n  # Remove legend (since we already have lake on x-axis)\n  theme(legend.position = \"none\") \n\n# Display the plot\ngeo_mean_plot"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#welchs-t-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#welchs-t-test",
    "title": "06_Class_Activity",
    "section": "3. Welch’s t-test",
    "text": "3. Welch’s t-test\n\n# Run Welch's t-test (doesn't assume equal variances)\nwelch_test_result &lt;- t.test(\n  mass_g ~ lake, \n  data = island_ne12_df,\n  var.equal = FALSE,  # Does NOT assume equal variances\n  alternative = \"two.sided\"\n)\n\n# Show the results\nprint(welch_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  mass_g by lake\nt = 5.1368, df = 9.0578, p-value = 0.0006016\nalternative hypothesis: true difference in means between group Island Lake and group NE 12 is not equal to 0\n95 percent confidence interval:\n 1473.676 3788.989\nsample estimates:\nmean in group Island Lake       mean in group NE 12 \n                3165.0000                  533.6677 \n\n\n\n\n\n\n\n\nWhen to Use Welch’s t-test\n\n\n\nWelch’s t-test is preferred when: - Group variances are unequal (as indicated by Levene’s test) - Sample sizes are different between groups - It’s more robust than the standard t-test in many situations"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#mann-whitney-wilcoxon-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#mann-whitney-wilcoxon-test",
    "title": "06_Class_Activity",
    "section": "4. Mann-Whitney Wilcoxon test",
    "text": "4. Mann-Whitney Wilcoxon test\n\n# Run Mann-Whitney U test (non-parametric alternative to t-test)\nwilcox_test_result &lt;- wilcox.test(\n  mass_g ~ lake, \n  data = island_ne12_df,\n  alternative = \"two.sided\"\n)\n\n# Show the results\nprint(wilcox_test_result)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  mass_g by lake\nW = 3205.5, p-value = 9.506e-08\nalternative hypothesis: true location shift is not equal to 0\n\n\n\n\n\n\n\n\nWhen to Use Mann-Whitney Wilcoxon Test\n\n\n\nThis non-parametric test is preferred when: - Data is not normally distributed (even after transformation) - Comparing medians rather than means - Data contains outliers that might affect a t-test - It compares the ranks of the values rather than the actual values"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#permutation-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#permutation-test",
    "title": "06_Class_Activity",
    "section": "5. Permutation test",
    "text": "5. Permutation test\n\n# First, let's make sure we have balanced samples\n# We'll select a random subset from NE 12 to match Island Lake size\nset.seed(123)  # For reproducibility\n\n# Get the smaller sample size\nisland_size &lt;- sum(island_ne12_df$lake == \"Island Lake\")\n\n# Randomly sample from NE 12 to match Island Lake size\nne12_sample &lt;- island_ne12_df %&gt;%\n  filter(lake == \"NE 12\") %&gt;%\n  slice_sample(n = island_size)\n\n# Combine with Island Lake data\nbalanced_df &lt;- bind_rows(\n  ne12_sample,\n  island_ne12_df %&gt;% filter(lake == \"Island Lake\")\n)\n\n# Extract mass data by lake\nne12_mass &lt;- balanced_df %&gt;%\n  filter(lake == \"NE 12\") %&gt;%\n  pull(mass_g)\n\nisland_mass &lt;- balanced_df %&gt;%\n  filter(lake == \"Island Lake\") %&gt;%\n  pull(mass_g)\n\n# Run permutation test\nperm_test_result &lt;- permTS(\n  x = ne12_mass,\n  y = island_mass,\n  alternative = \"two.sided\",\n  method = \"exact.mc\",  # Monte Carlo method for large samples\n  control = permControl(nmc = 10000)  # Number of Monte Carlo replications\n)\n\n# Show the results\nprint(perm_test_result)\n\n\n    Exact Permutation Test Estimated by Monte Carlo\n\ndata:  ne12_mass and GROUP 2\np-value = 2e-04\nalternative hypothesis: true mean ne12_mass - mean GROUP 2 is not equal to 0\nsample estimates:\nmean ne12_mass - mean GROUP 2 \n                      -2519.9 \n\np-value estimated from 10000 Monte Carlo replications\n99 percent confidence interval on p-value:\n 0.000000000 0.001059383 \n\n\n\n\n\n\n\n\nWhen to Use Permutation Tests\n\n\n\nPermutation tests are useful when: - Sample sizes are small - Data doesn’t meet the assumptions for parametric tests - You want a robust test that makes minimal assumptions about the data - They can test any statistic, not just means"
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-standard-t-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-standard-t-test",
    "title": "06_Class_Activity",
    "section": "For Standard t-test:",
    "text": "For Standard t-test:\nLake trout from NE 12 had significantly different mass (M = [mean], SD = [SD]) compared to Island Lake (M = [mean], SD = [SD]), t([df]) = [t-value], p = [p-value]."
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-log-transformed-t-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-log-transformed-t-test",
    "title": "06_Class_Activity",
    "section": "For Log-transformed t-test:",
    "text": "For Log-transformed t-test:\nAfter log transformation to meet normality assumptions, lake trout from NE 12 had significantly different mass (geometric mean = [value], 95% CI [lower-upper]) compared to Island Lake (geometric mean = [value], 95% CI [lower-upper]), t([df]) = [t-value], p = [p-value]."
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-welchs-t-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-welchs-t-test",
    "title": "06_Class_Activity",
    "section": "For Welch’s t-test:",
    "text": "For Welch’s t-test:\nAssuming unequal variances, lake trout from NE 12 had significantly different mass (M = [mean], SD = [SD]) compared to Island Lake (M = [mean], SD = [SD]), Welch's t([df]) = [t-value], p = [p-value]."
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-mann-whitney-wilcoxon-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-mann-whitney-wilcoxon-test",
    "title": "06_Class_Activity",
    "section": "For Mann-Whitney Wilcoxon test:",
    "text": "For Mann-Whitney Wilcoxon test:\nLake trout mass differed significantly between NE 12 (Mdn = [median]) and Island Lake (Mdn = [median]), W = [W-value], p = [p-value]."
  },
  {
    "objectID": "lectures/lecture_07/07_02_class_activity.html#for-permutation-test",
    "href": "lectures/lecture_07/07_02_class_activity.html#for-permutation-test",
    "title": "06_Class_Activity",
    "section": "For Permutation test:",
    "text": "For Permutation test:\nPermutation testing (10,000 iterations) revealed significant differences in lake trout mass between NE 12 and Island Lake, p = [p-value]."
  },
  {
    "objectID": "lectures/lecture_01/01_1_lecture_powerpoint_html.html",
    "href": "lectures/lecture_01/01_1_lecture_powerpoint_html.html",
    "title": "01_Lecture",
    "section": "",
    "text": "Lecture 1: Syllabus\n\nPlease look over the syllabus as it has all the details of the class and how it will run.\n\n\n\n\nLecture 1: Who am I?\n\n\n\nBill Perry\nOffice is in XXXX\nPhone is XXXX\nEmail is wlperry@d.umn.edu\n\n\n\n\n\n\n\n\nLecture 1: My goals\n\nHow do we make observations and hypotheses?\nHow do we design an experiment\nHow do we collect data?\nHow do we organize, clean, summarize, and view the data?\nHow do we use statistics to test our hypotheses\n\nwhat tests to use\nwhat are the assumptions\nwhat are the interpretations\n\n\n\n\nLecture 1: My expectations\n\nCommunication\nPractice\nFailure\nLearn to correct and troubleshoot\n\n\n\nLecture 1: Science\n\nWay to acquire knowledge, organize it and apply it back to the real world\nMake predictions and testing these predictions using a falsifiable approach - statistics\nExplanations that cannot be falsified are not science\n\n\n\nWhat is Statistics?\n\n\nZar (1999) - “analysis and interpretation of data with view towards objective evaluation of conclusions based on the data”\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nInductive Reasoning (Specific → General)\nInductive reasoning involves observing specific cases and using them to form a general conclusion.\nExample:\n\nMeasure 10 pine needles from a tree - average length is 75 mm.\nMeasure 10 more needles from the same tree and gets similar results.\nMeasures needles from second tree - average length is 120 mm .\nYou generalize pine needles from different trees vary in length, but each tree tends to have a characteristic range.\n\nConclusion (Induction): “Pine needle length varies by tree, but each tree seems to have a typical range of lengths.\nPotential Issue: Conclusion is not guaranteed to be true - based on patterns observed in a sample, and there could be exceptions.\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nDeductive Reasoning (General → Specific)\nDeductive reasoning starts with a general principle and applies it to a specific case.\nExample:\n\nGeneral Principle: Pine needles from a species of pine tree have a predictable length range (e.g., 70–80 mm).\nSpecific Case: Collect sample of pine needles and measure them.\nPrediction: Since its the species the needle lengths should fall within 70–80 mm.\nMeasurement: Check the data and confirm needles fall within this expected range.\n\nConclusion (Deduction): “This tree belongs the species with a needle length range of 70–80 mm, we expect its needle lengths to fall in this range.”\nStronger than induction because it’s based on a general rule—but if the assumption (length range) is incorrect, conclusion could still be wrong.\n\n\n\n\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nIn reality we are doing both of these processes\n\n\n\n\n\n\n\n\n\n\n\nHow do we test hypotheses\n\n\n\nStatistics\n\nDesign good experiments\nDesign good tests\nSummarize patterns/data\nUse to make probabilistic determinations to see if differences are “real”\n\n\n\n\n\n\n\n\nData Types\n\n\n\nContinuous\n\nnumeric\n\ndiscrete\n\ninteger or numerical\n\ncategorical\n\nnominal – up, down, right, left…\nordinal – order - a, b, c, d or morning, afternoon, evening\n\n\n\n\n\n\n\n\nMeasurements\n\n\nData is obtained through measurement\nThe world is a messy place and how you measure matters\nOur measures depend on\n\naccuracy - how close we are to the real value\nprecision - how close all our measurements are but may not be precise\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html",
    "href": "lectures/lecture_01/01_2_class_activity_html.html",
    "title": "01_Class_Activity",
    "section": "",
    "text": "Note: This activity is really in place of the outline above which you should have read before class."
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#tapestry-plot",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#tapestry-plot",
    "title": "01_Class_Activity",
    "section": "Tapestry Plot ——",
    "text": "Tapestry Plot ——"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot",
    "title": "01_Class_Activity",
    "section": "XY Plot —–",
    "text": "XY Plot —–\nnotice the points are layered on top but some overlap"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot-with-dodged-points",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot-with-dodged-points",
    "title": "01_Class_Activity",
    "section": "XY Plot with dodged points ——",
    "text": "XY Plot with dodged points ——\n\n\n\n\n\n\n\n\n\nWhat are the other ways to display the data?"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram",
    "title": "01_Class_Activity",
    "section": "Histogram —–",
    "text": "Histogram —–\n\n\n\n\n\n\n\n\n\nNote we really want to see the histograms colored by wind direction\nWe can map the wind aesthetic to a fill in the histogram"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram-colors",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram-colors",
    "title": "01_Class_Activity",
    "section": "Histogram Colors —–",
    "text": "Histogram Colors —–"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram-bins",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram-bins",
    "title": "01_Class_Activity",
    "section": "Histogram Bins —–",
    "text": "Histogram Bins —–"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#other-plots-if-time",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#other-plots-if-time",
    "title": "01_Class_Activity",
    "section": "Other Plots if time",
    "text": "Other Plots if time"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#box-and-whisker-plots",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#box-and-whisker-plots",
    "title": "01_Class_Activity",
    "section": "Box and Whisker Plots",
    "text": "Box and Whisker Plots"
  },
  {
    "objectID": "lectures/lecture_06/06_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_06/06_01_lecture_powerpoint_html.html",
    "title": "Lecture 06",
    "section": "",
    "text": "Covered\n\nStatistical inference fundamentals\nHypothesis testing principles\nT Distributions\nOne sample T Tests\nTwo sample T"
  },
  {
    "objectID": "lectures/lecture_06/06_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_06/06_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 06",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_06/06_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_06/06_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 06",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html",
    "title": "Lecture 15 - xxxxxx",
    "section": "",
    "text": "ANOVA\nFactorial ANOVA\nNested ANOVA\nASSUMPIONS OF ALL\n\nHomogeneity of variance - Levenes or Bartlets Test\nNormality of Residuals\nIndependence\n\n\n\n\n\nimage"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#review",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#review",
    "title": "Lecture 15 - xxxxxx",
    "section": "",
    "text": "ANOVA\nFactorial ANOVA\nNested ANOVA\nASSUMPIONS OF ALL\n\nHomogeneity of variance - Levenes or Bartlets Test\nNormality of Residuals\nIndependence"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 15 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nGeneral Linear Models GLM\n\nExamples:\n\n\nLogistic Regression"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#overview-of-generalized-linear-models-glms",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#overview-of-generalized-linear-models-glms",
    "title": "Lecture 15 - xxxxxx",
    "section": "Overview of Generalized Linear Models (GLMs)",
    "text": "Overview of Generalized Linear Models (GLMs)\n\n\nGeneral linear models assume normal distribution of response variables and residuals. However, many types of biological data don’t meet this assumption. Generalized Linear Models (GLMs) allow for a wider range of probability distributions for the response variable.\nGLMs allow all types of “exponential family” distributions:\n\nNormal\nLognormal\nBinomial\nPoisson\nGamma\nNegative binomial\n\nGLMs can be used for binary (yes/no), discrete (count), and categorical/multinomial response variables, using maximum likelihood (ML) rather than ordinary least squares (OLS) for estimation.\nNote: GLMs extend linear models to non-normal data distributions.\n\n\n\n\n\n\nExamples of distributions in the exponential family"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#the-three-elements-of-a-glm",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#the-three-elements-of-a-glm",
    "title": "Lecture 15 - xxxxxx",
    "section": "The Three Elements of a GLM",
    "text": "The Three Elements of a GLM\nGLMs consist of three components:\n\nRandom component: The response variable and its probability distribution (from exponential family: normal, binomial, Poisson)\nSystematic component: The predictor variable(s) in the model, which can be continuous or categorical\nLink function: Connects expected value of Y to predictor variables\n\\[g(\\mu) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2...\\]\n\n\n\n\n\n\n\nLink Functions and Distributions\n\n\n\n\n\n\nDistribution\nCommon Link Function\nFormula\n\n\n\n\nNormal\nIdentity\n\\(g(\\mu) = \\mu\\)\n\n\nPoisson\nLog\n\\(g(\\mu) = \\log(\\mu)\\)\n\n\nBinomial\nLogit\n\\(g(\\mu) = \\log[\\mu/(1-\\mu)]\\)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#glm-with-gaussian-normal-distribution",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#glm-with-gaussian-normal-distribution",
    "title": "Lecture 15 - xxxxxx",
    "section": "GLM with Gaussian (Normal) Distribution",
    "text": "GLM with Gaussian (Normal) Distribution\n\n\nThe simplest form of GLM uses a normal (Gaussian) distribution with an identity link function. This is equivalent to standard linear regression.\nLet’s compare a standard linear model and a Gaussian GLM using the mtcars dataset, modeling miles per gallon (mpg) by the number of cylinders (cyl).\n\n# Convert cylinders to a factor\nmtcars &lt;- mtcars %&gt;%\n  mutate(cyl = factor(cyl))\n\n# Fit a standard linear model\nmodel_lm &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# Fit a Gaussian GLM\nmodel_gaussian &lt;- glm(mpg ~ cyl, \n                       data = mtcars, \n                       family = gaussian(link = \"identity\"))\n\n# Compare the coefficients\ncoef_lm &lt;- coefficients(model_lm)\ncoef_glm &lt;- coefficients(model_gaussian)\n\n# Check if they're the same\nall.equal(coef_lm, coef_glm)\n\n[1] TRUE\n\n\nLet’s look at the summary of our Gaussian GLM:\n\nsummary(model_gaussian)\n\n\nCall:\nglm(formula = mpg ~ cyl, family = gaussian(link = \"identity\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.6636     0.9718  27.437  &lt; 2e-16 ***\ncyl6         -6.9208     1.5583  -4.441 0.000119 ***\ncyl8        -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 10.38837)\n\n    Null deviance: 1126.05  on 31  degrees of freedom\nResidual deviance:  301.26  on 29  degrees of freedom\nAIC: 170.56\n\nNumber of Fisher Scoring iterations: 2\n\n\nNow let’s perform an ANOVA on our GLM model using the car package:\n\nAnova(model_gaussian, type = \"III\", test = \"F\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\nError estimate based on Pearson residuals \n\n          Sum Sq Df F values    Pr(&gt;F)    \ncyl       824.78  2   39.697 4.979e-09 ***\nResiduals 301.26 29                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nVisualizing the results:\n\n# Get estimated means\nemm_gaussian &lt;- emmeans(model_gaussian, ~ cyl)\nemm_df &lt;- as.data.frame(emm_gaussian)\n\n# Create plot of data with estimated means\nggplot() +\n  # Plot raw data\n  geom_jitter(data = mtcars, \n              aes(x = cyl, y = mpg), \n              width = 0.2, \n              alpha = 0.5) +\n  # Add estimated means with confidence intervals\n  geom_point(data = emm_df, \n             aes(x = cyl, y = emmean), \n             size = 4, color = \"red\") +\n  geom_errorbar(data = emm_df, \n                aes(x = cyl, \n                    ymin = lower.CL, \n                    ymax = upper.CL), \n                width = 0.2, \n                color = \"red\") +\n  labs(title = \"Effect of Cylinders on MPG\",\n       subtitle = \"Red points show estimated means with 95% CIs\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEquivalence of Linear Models and Gaussian GLMs\n\n\n\nWhen we use a Gaussian distribution with an identity link, GLM gives identical results to standard linear regression. This can be seen in the coefficient values and overall model statistics.\nThe key difference is that GLMs provide a framework that extends to non-normal distributions."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#glm-with-poisson-distribution",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#glm-with-poisson-distribution",
    "title": "Lecture 15 - xxxxxx",
    "section": "GLM with Poisson Distribution",
    "text": "GLM with Poisson Distribution\n\n\nPoisson GLMs are appropriate for count data. The Poisson distribution assumes that the variance equals the mean.\nFor this example, we’ll use the quarter-mile time (qsec) from the mtcars dataset, rounded to create a count-like variable.\n\n# Prepare data for Poisson model\nmtcars_count &lt;- mtcars %&gt;%\n  mutate(\n    cyl = factor(cyl),\n    qsec_round = round(qsec)  # Create a count-like variable\n  )\n\n# Look at the first few rows\nhead(mtcars_count[, c(\"cyl\", \"qsec\", \"qsec_round\")])\n\n                  cyl  qsec qsec_round\nMazda RX4           6 16.46         16\nMazda RX4 Wag       6 17.02         17\nDatsun 710          4 18.61         19\nHornet 4 Drive      6 19.44         19\nHornet Sportabout   8 17.02         17\nValiant             6 20.22         20\n\n\nNow let’s fit a Poisson GLM to model the relationship between the rounded quarter-mile time and the number of cylinders:\n\n# Fit a Poisson GLM\nmodel_poisson &lt;- glm(qsec_round ~ cyl, \n                     family = poisson(link = \"log\"), \n                     data = mtcars_count)\n\n# Look at the model summary\nsummary(model_poisson)\n\n\nCall:\nglm(formula = qsec_round ~ cyl, family = poisson(link = \"log\"), \n    data = mtcars_count)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.95869    0.06868  43.079   &lt;2e-16 ***\ncyl6        -0.07629    0.11277  -0.676    0.499    \ncyl8        -0.14243    0.09482  -1.502    0.133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.6979  on 31  degrees of freedom\nResidual deviance: 3.4487  on 29  degrees of freedom\nAIC: 160.62\n\nNumber of Fisher Scoring iterations: 3\n\n\nLet’s check for overdispersion, which is common in count data:\n\n# Calculate dispersion parameter\ndispersion_poisson &lt;- sum(residuals(model_poisson, \n                             type = \"pearson\")^2) / \n                     model_poisson$df.residual\n\n# Print dispersion parameter\ncat(\"Dispersion parameter:\", round(dispersion_poisson, 2), \"\\n\")\n\nDispersion parameter: 0.12 \n\n# Should be close to 1 for a well-fitting Poisson model\n# If &gt; 1.5, may indicate overdispersion\n\n\n\n# Get estimated means on the response scale\nemm_poisson &lt;- emmeans(model_poisson, ~ cyl, type = \"response\")\nemm_poisson_df &lt;- as.data.frame(emm_poisson)\n\n# Create visualization\nggplot() +\n  # Plot raw data\n  geom_jitter(data = mtcars_count, \n              aes(x = cyl, y = qsec_round), \n              width = 0.2, \n              alpha = 0.5) +\n  # Add estimated means with confidence intervals\n  geom_point(data = emm_poisson_df, \n             aes(x = cyl, y = rate), \n             size = 4, color = \"blue\") +\n  geom_errorbar(data = emm_poisson_df, \n                aes(x = cyl, \n                    ymin = asymp.LCL, \n                    ymax = asymp.UCL), \n                width = 0.2, \n                color = \"blue\") +\n  labs(title = \"Effect of Cylinders on Quarter-Mile Time\",\n       subtitle = \"Poisson GLM with log link\",\n       x = \"Number of Cylinders\",\n       y = \"Quarter-Mile Time (rounded)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Poisson GLM Coefficients\n\n\n\nIn a Poisson GLM with a log link function:\n\nThe coefficients represent changes in the log of the expected count\nWhen exponentiated (exp(coef)), they represent multiplicative effects\nFor example, exp(coef) = 0.90 means the expected count is 90% of the reference level\n\n\n\n\nChecking Model Assumptions with DHARMa\nDHARMa provides a useful framework for diagnosing GLM residuals:\n\n# Simulate residuals using DHARMa\nset.seed(123) # For reproducibility\nsimulation_poisson &lt;- simulateResiduals(fittedModel = model_poisson, n = 1000)\n\n# Plot diagnostic plots\nplot(simulation_poisson)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#dealing-with-overdispersion-in-count-data",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#dealing-with-overdispersion-in-count-data",
    "title": "Lecture 15 - xxxxxx",
    "section": "Dealing with Overdispersion in Count Data",
    "text": "Dealing with Overdispersion in Count Data\n\n\nWhen count data shows more variability than expected under a Poisson distribution (variance &gt; mean), we may need to use a negative binomial model instead.\n\n# If we detected overdispersion, we could fit a negative binomial model\n# This is just for demonstration - our data may not actually need this\n\n# Fit negative binomial model\nmodel_nb &lt;- glm.nb(qsec_round ~ cyl, data = mtcars_count)\n\n# Compare summaries\nsummary(model_nb)\n\n\nCall:\nglm.nb(formula = qsec_round ~ cyl, data = mtcars_count, init.theta = 2935650.009, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.95869    0.06868  43.079   &lt;2e-16 ***\ncyl6        -0.07629    0.11277  -0.676    0.499    \ncyl8        -0.14243    0.09482  -1.502    0.133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2935650) family taken to be 1)\n\n    Null deviance: 5.6979  on 31  degrees of freedom\nResidual deviance: 3.4486  on 29  degrees of freedom\nAIC: 162.62\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2935650 \n          Std. Err.:  121368753 \nWarning while fitting theta: iteration limit reached \n\n 2 x log-likelihood:  -154.616 \n\n\nThe negative binomial model includes an additional dispersion parameter (theta) that allows the variance to be larger than the mean.\n\nLet’s compare the predictions from both models:\n\n# Create predictions from both models\nmtcars_count$pred_poisson &lt;- predict(model_poisson, \n                                    type = \"response\")\nmtcars_count$pred_nb &lt;- predict(model_nb, \n                               type = \"response\")\n\n# Compare predictions\nggplot(mtcars_count) +\n  geom_point(aes(x = pred_poisson, y = pred_nb, color = cyl)) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  labs(title = \"Comparison of Poisson and Negative Binomial Predictions\",\n       x = \"Poisson Predictions\",\n       y = \"Negative Binomial Predictions\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#logistic-regression---introduction",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#logistic-regression---introduction",
    "title": "Lecture 15 - xxxxxx",
    "section": "Logistic Regression - Introduction",
    "text": "Logistic Regression - Introduction\n\n\nLogistic regression is a GLM used when the response variable is binary (e.g., dead/alive, present/absent). It models the probability of the response being “1” (success) given predictor values.\nLet’s examine the simple logistic regression model:\n\\[\\pi(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\]\nWhere: - \\(\\pi(x)\\) is the probability that Y = 1 given X = x - \\(\\beta_0\\) is the intercept - \\(\\beta_1\\) is the slope (rate of change in \\(\\pi(x)\\) for a unit change in X)\nTo linearize this relationship, we use the logit link function:\n\\[g(x) = \\log\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) = \\beta_0 + \\beta_1 x\\]\nThis transforms the probability (which is bounded between 0 and 1) to a linear function that can range from -∞ to +∞.\n\n\n# Create data for sigmoid curve\nsigmoid_data &lt;- data.frame(\n  x = seq(-6, 6, length.out = 100)\n)\nsigmoid_data$p &lt;- 1 / (1 + exp(-sigmoid_data$x))\n\n# Plot the sigmoid curve\nggplot(sigmoid_data, aes(x, p)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_hline(yintercept = c(0, 0.5, 1), \n             linetype = \"dashed\", \n             color = \"gray50\") +\n  geom_vline(xintercept = 0, \n             linetype = \"dashed\", \n             color = \"gray50\") +\n  labs(title = \"Logistic Function\",\n       subtitle = \"Mapping from linear predictor to probability\",\n       x = \"Linear predictor (β₀ + β₁x)\",\n       y = \"Probability π(x)\") +\n  scale_y_continuous(breaks = seq(0, 1, 0.25)) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#example-lizard-presence-on-islands",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#example-lizard-presence-on-islands",
    "title": "Lecture 15 - xxxxxx",
    "section": "Example: Lizard Presence on Islands",
    "text": "Example: Lizard Presence on Islands\nBased on the example from Polis et al. (1998), we’ll model the presence/absence of lizards (Uta) on islands in the Gulf of California based on perimeter/area ratio.\n\n# Create a simulated dataset based on the described study\nset.seed(123)\nisland_data &lt;- data.frame(\n  island_id = 1:19,\n  pa_ratio = c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 10, 15, 20, 25, 30),\n  uta_present = c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0)\n) %&gt;%\n  mutate(uta_present = factor(uta_present, levels = c(0, 1), labels = c(\"Absent\", \"Present\")))\n\n# Fit the logistic regression model\nlizard_model &lt;- glm(uta_present ~ pa_ratio, \n                    data = island_data, \n                    family = binomial(link = \"logit\"))\n\n# Model summary\nsummary(lizard_model)\n\n\nCall:\nglm(formula = uta_present ~ pa_ratio, family = binomial(link = \"logit\"), \n    data = island_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    241.039 191755.596   0.001    0.999\npa_ratio        -8.766   6965.289  -0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2.6287e+01  on 18  degrees of freedom\nResidual deviance: 2.4292e-09  on 17  degrees of freedom\nAIC: 4\n\nNumber of Fisher Scoring iterations: 25\n\n\n\n\nLet’s visualize the data and the fitted model:\n\n# Create a dataframe for predictions\npred_data &lt;- data.frame(\n  pa_ratio = seq(min(island_data$pa_ratio), \n                max(island_data$pa_ratio), \n                length.out = 100)\n)\n\n# Get predicted probabilities\npred_data$prob &lt;- predict(lizard_model, \n                         newdata = pred_data, \n                         type = \"response\")\n\n# Plot\nggplot() +\n  # Add jittered points for observed data\n  geom_jitter(data = island_data, \n              aes(x = pa_ratio, y = as.numeric(uta_present) - 1),\n              height = 0.05, width = 0, alpha = 0.7) +\n  # Add predicted probability curve\n  geom_line(data = pred_data, \n            aes(x = pa_ratio, y = prob), \n            color = \"blue\", size = 1) +\n  # Add confidence intervals (optional)\n  labs(title = \"Probability of Uta Presence vs. Perimeter/Area Ratio\",\n       x = \"Perimeter/Area Ratio\",\n       y = \"Probability of Presence\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe want to test the null hypothesis that β₁ = 0, meaning there’s no relationship between P/A ratio and lizard presence.\nThere are two common ways to test this hypothesis:\n\nWald test: Tests if the parameter estimate divided by its standard error differs significantly from zero\nLikelihood ratio test: Compares the fit of the full model to a reduced model without the predictor variable\n\n\n# Reduced model (intercept only)\nreduced_model &lt;- glm(uta_present ~ 1, \n                     data = island_data, \n                     family = binomial(link = \"logit\"))\n\n# Likelihood ratio test\nanova(reduced_model, lizard_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: uta_present ~ 1\nModel 2: uta_present ~ pa_ratio\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        18     26.287                          \n2        17      0.000  1   26.287 2.943e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nInterpreting the Odds Ratio\n\n\n\n\n\n\nWorking with Odds Ratios\n\n\n\nThe odds ratio represents how the odds of the event (e.g., lizard presence) change with a unit increase in the predictor.\n\nOdds ratio = exp(β₁)\nIf odds ratio &gt; 1: Increasing the predictor increases the odds of event\nIf odds ratio &lt; 1: Increasing the predictor decreases the odds of event\nIf odds ratio = 1: No effect of predictor on odds of event\n\n\n\n\n# Calculate odds ratio and confidence interval\ncoef_lizard &lt;- coef(lizard_model)[2]  # Extract slope coefficient\nodds_ratio &lt;- exp(coef_lizard)\nci &lt;- exp(confint(lizard_model, \"pa_ratio\"))\n\n# Display results\ncat(\"Odds Ratio:\", round(odds_ratio, 3), \"\\n\")\n\nOdds Ratio: 0 \n\ncat(\"95% CI:\", round(ci[1], 3), \"to\", round(ci[2], 3), \"\\n\")\n\n95% CI: 0 to Inf \n\n\n\n\nAssessing Model Fit\nThere are several ways to assess the goodness-of-fit for logistic regression models:\n\n# Calculate Hosmer-Lemeshow statistic\n# This would normally require an additional package like 'ResourceSelection'\n# Instead, we'll use a simpler approximation and other diagnostics\n\n# Calculate Pearson residuals\npearson_resid &lt;- residuals(lizard_model, type = \"pearson\")\npearson_chi2 &lt;- sum(pearson_resid^2)\ndf_resid &lt;- lizard_model$df.residual\n\n# Calculate deviance\ndeviance_g2 &lt;- lizard_model$deviance\nnull_deviance &lt;- lizard_model$null.deviance\n\n# Calculate McFadden's pseudo-R²\nr2_mcfadden &lt;- 1 - (deviance_g2 / null_deviance)\n\n# Display results\ncat(\"Pearson χ²:\", round(pearson_chi2, 3), \"on\", df_resid, \"df, p =\", \n    round(1 - pchisq(pearson_chi2, df_resid), 3), \"\\n\")\n\nPearson χ²: 0 on 17 df, p = 1 \n\ncat(\"Deviance G²:\", round(deviance_g2, 3), \"on\", df_resid, \"df, p =\", \n    round(1 - pchisq(deviance_g2, df_resid), 3), \"\\n\")\n\nDeviance G²: 0 on 17 df, p = 1 \n\ncat(\"McFadden's R²:\", round(r2_mcfadden, 3), \"\\n\")\n\nMcFadden's R²: 1"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#multiple-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#multiple-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\n\nLogistic regression can be extended to include multiple predictors. The model becomes:\n\\[g(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\nWhere g(x) is the logit link function, and x₁, x₂, …, xₚ are the predictor variables.\nLet’s create a simulated dataset based on the Bolger et al. (1997) study of the presence/absence of native rodents in canyon fragments.\n\n# Simulate data for the rodent example\nset.seed(123)\nn &lt;- 25  # 25 canyon fragments\n\n# Create predictor variables\nfragment_data &lt;- data.frame(\n  fragment_id = paste0(\"F\", 1:n),\n  distance = runif(n, 0, 3000),            # Distance to source canyon (m)\n  age = runif(n, 5, 80),                   # Years since isolation\n  shrub_cover = runif(n, 10, 100)          # Percentage shrub cover\n)\n\n# Generate response variable (rodent presence)\n# Higher probability with higher shrub cover, slight effect of age\nlinear_pred &lt;- -5 + 0.0001*fragment_data$distance + \n               0.02*fragment_data$age + \n               0.09*fragment_data$shrub_cover\nprob &lt;- 1 / (1 + exp(-linear_pred))\nfragment_data$rodent_present &lt;- rbinom(n, 1, prob)\nfragment_data$rodent_present &lt;- factor(fragment_data$rodent_present, \n                                      levels = c(0, 1), \n                                      labels = c(\"Absent\", \"Present\"))\n\n# Fit multiple logistic regression model\nrodent_model &lt;- glm(rodent_present ~ distance + age + shrub_cover, \n                    data = fragment_data, \n                    family = binomial(link = \"logit\"))\n\n# Model summary\nsummary(rodent_model)\n\n\nCall:\nglm(formula = rodent_present ~ distance + age + shrub_cover, \n    family = binomial(link = \"logit\"), data = fragment_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -12.278261   7.911491  -1.552   0.1207  \ndistance      0.002062   0.001716   1.202   0.2294  \nage           0.068744   0.059665   1.152   0.2493  \nshrub_cover   0.193001   0.116035   1.663   0.0963 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27.5540  on 24  degrees of freedom\nResidual deviance:  9.2737  on 21  degrees of freedom\nAIC: 17.274\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nTo test the significance of individual predictors, we can use likelihood ratio tests comparing nested models:\n\n# Test distance\nmodel_no_distance &lt;- glm(rodent_present ~ age + shrub_cover, \n                         data = fragment_data, \n                         family = binomial(link = \"logit\"))\nanova(model_no_distance, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ age + shrub_cover\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        22    11.3831                     \n2        21     9.2737  1   2.1094   0.1464\n\n# Test age\nmodel_no_age &lt;- glm(rodent_present ~ distance + shrub_cover, \n                    data = fragment_data, \n                    family = binomial(link = \"logit\"))\nanova(model_no_age, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ distance + shrub_cover\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        22    11.0533                     \n2        21     9.2737  1   1.7796   0.1822\n\n# Test shrub cover\nmodel_no_shrub &lt;- glm(rodent_present ~ distance + age, \n                      data = fragment_data, \n                      family = binomial(link = \"logit\"))\nanova(model_no_shrub, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ distance + age\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        22    26.7315                          \n2        21     9.2737  1   17.458 2.938e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nLet’s calculate odds ratios and confidence intervals for all predictors:\n\n# Calculate odds ratios and CIs\ncoefs &lt;- coef(rodent_model)[-1]  # Exclude intercept\nodds_ratios &lt;- exp(coefs)\nci &lt;- exp(confint(rodent_model)[-1, ])  # Exclude intercept\n\n# Create a data frame for display\nor_df &lt;- data.frame(\n  Predictor = names(coefs),\n  OddsRatio = odds_ratios,\n  LowerCI = ci[, 1],\n  UpperCI = ci[, 2]\n)\n\n# Display formatted table\nor_df %&gt;%\n  mutate(across(where(is.numeric), round, 4)) %&gt;%\n  mutate(CI = paste0(\"(\", LowerCI, \", \", UpperCI, \")\")) %&gt;%\n  dplyr::select(Predictor, OddsRatio, CI) %&gt;%\n  flextable()\n\nPredictorOddsRatioCIdistance1.0021(0.9994, 1.0069)age1.0712(0.9721, 1.2577)shrub_cover1.2129(1.0645, 1.7909)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#visualizing-multiple-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#visualizing-multiple-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Visualizing Multiple Logistic Regression",
    "text": "Visualizing Multiple Logistic Regression\n\n\nFor multiple predictors, we can visualize the effect of each predictor while holding others constant at their mean or median values.\n\n# Create a function to generate prediction data for one variable\npredict_for_var &lt;- function(var_name, model, data) {\n  # Create grid of values for the variable of interest\n  pred_df &lt;- data.frame(\n    x = seq(min(data[[var_name]]), max(data[[var_name]]), length.out = 100)\n  )\n  names(pred_df) &lt;- var_name\n  \n  # Add mean values for other predictors\n  for (other_var in c(\"distance\", \"age\", \"shrub_cover\")) {\n    if (other_var != var_name) {\n      pred_df[[other_var]] &lt;- mean(data[[other_var]])\n    }\n  }\n  \n  # Add predictions\n  pred_df$prob &lt;- predict(model, newdata = pred_df, type = \"response\")\n  \n  return(pred_df)\n}\n\n# Generate prediction data for each variable\npred_distance &lt;- predict_for_var(\"distance\", rodent_model, fragment_data)\npred_age &lt;- predict_for_var(\"age\", rodent_model, fragment_data)\npred_shrub &lt;- predict_for_var(\"shrub_cover\", rodent_model, fragment_data)\n\n# Create plots\np1 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = distance, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_distance, aes(x = distance, y = prob), \n            color = \"darkred\", size = 1) +\n  labs(title = \"Effect of Distance\",\n       x = \"Distance to Source (m)\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\np2 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = age, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_age, aes(x = age, y = prob), \n            color = \"darkgreen\", size = 1) +\n  labs(title = \"Effect of Age\",\n       x = \"Years Since Isolation\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\np3 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = shrub_cover, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_shrub, aes(x = shrub_cover, y = prob), \n            color = \"darkblue\", size = 1) +\n  labs(title = \"Effect of Shrub Cover\",\n       x = \"Shrub Cover (%)\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\n# Combine plots\np1 + p2 + p3\n\n\n\n\n\n\n\n\n\nThis visualization shows the effect of each predictor on the probability of rodent presence, while holding the other predictors constant at their mean values."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#assumptions-and-diagnostics-of-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#assumptions-and-diagnostics-of-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Assumptions and Diagnostics of Logistic Regression",
    "text": "Assumptions and Diagnostics of Logistic Regression\nLogistic regression has several key assumptions:\n\nIndependence of observations\nLinear relationship between predictors and log odds\nNo extreme outliers\nNo multicollinearity (when multiple predictors are used)\n\nLet’s check the diagnostics for our multiple logistic regression model:\n\n# 1. Check for linearity between predictors and log odds\n# Use bins of X variables and plot log odds\ncheck_linearity &lt;- function(model, data, var) {\n  # Create bins of predictor\n  n_bins &lt;- 5\n  data$bin &lt;- cut(data[[var]], breaks = n_bins)\n  \n  # Calculate log odds for each bin\n  bin_summary &lt;- data %&gt;%\n    group_by(bin) %&gt;%\n    summarize(\n      n = n(),\n      mean_var = mean(!!sym(var)),\n      successes = sum(rodent_present == \"Present\"),\n      failures = sum(rodent_present == \"Absent\")\n    ) %&gt;%\n    mutate(\n      p = successes / n,\n      logodds = log(p / (1 - p))\n    )\n  \n  # Create plot\n  ggplot(bin_summary, aes(x = mean_var, y = logodds)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = paste(\"Linearity Check:\", var),\n         x = var,\n         y = \"Log Odds\") +\n    theme_minimal()\n}\n\n# Create diagnostic plots for each variable\np1 &lt;- check_linearity(rodent_model, fragment_data, \"distance\")\np2 &lt;- check_linearity(rodent_model, fragment_data, \"age\")\np3 &lt;- check_linearity(rodent_model, fragment_data, \"shrub_cover\")\n\n# Arrange the plots\np1 / p2 / p3"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#model-comparison-and-selection",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#model-comparison-and-selection",
    "title": "Lecture 15 - xxxxxx",
    "section": "Model Comparison and Selection",
    "text": "Model Comparison and Selection\n\n\nWhen working with multiple predictors, we often want to find the most parsimonious model. We can use:\n\nLikelihood ratio tests for nested models\nInformation criteria (AIC, BIC) for non-nested models\nClassification metrics like accuracy, sensitivity, and specificity\n\nLet’s compare models and calculate AIC values:\n\n# Calculate AIC for our models\nmodels &lt;- list(\n  \"Full\" = rodent_model,\n  \"No Distance\" = model_no_distance,\n  \"No Age\" = model_no_age,\n  \"No Shrub\" = model_no_shrub,\n  \"Intercept Only\" = glm(rodent_present ~ 1, \n                        data = fragment_data, \n                        family = binomial)\n)\n\n# Calculate AIC and BIC\nmodel_comparison &lt;- data.frame(\n  Model = names(models),\n  Parameters = sapply(models, function(m) length(coef(m))),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC),\n  Deviance = sapply(models, function(m) m$deviance)\n)\n\n# Show model comparison table\nmodel_comparison %&gt;%\n  arrange(AIC) %&gt;%\n  mutate(across(where(is.numeric), round, 2)) %&gt;%\n  flextable()\n\nModelParametersAICBICDevianceNo Age317.0520.7111.05Full417.2722.159.27No Distance317.3821.0411.38Intercept Only129.5530.7727.55No Shrub332.7336.3926.73\n\n\n\nWe can also evaluate the predictive performance of our model:\n\n# Get predictions\npredicted_probs &lt;- predict(rodent_model, type = \"response\")\npredicted_class &lt;- ifelse(predicted_probs &gt; 0.5, \"Present\", \"Absent\")\n\n# Create confusion matrix\ntrue_class &lt;- fragment_data$rodent_present\nconf_matrix &lt;- table(Predicted = predicted_class, Actual = true_class)\n\n# Calculate metrics\naccuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix)\nsensitivity &lt;- conf_matrix[\"Present\", \"Present\"] / sum(conf_matrix[, \"Present\"])\nspecificity &lt;- conf_matrix[\"Absent\", \"Absent\"] / sum(conf_matrix[, \"Absent\"])\n\n# Display results\nconf_matrix\n\n         Actual\nPredicted Absent Present\n  Absent       5       2\n  Present      1      17\n\ncat(\"\\nAccuracy:\", round(accuracy, 3), \"\\n\")\n\n\nAccuracy: 0.88 \n\ncat(\"Sensitivity:\", round(sensitivity, 3), \"\\n\")\n\nSensitivity: 0.895 \n\ncat(\"Specificity:\", round(specificity, 3), \"\\n\")\n\nSpecificity: 0.833"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#publication-quality-figure-and-scientific-write-up",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#publication-quality-figure-and-scientific-write-up",
    "title": "Lecture 15 - xxxxxx",
    "section": "Publication-Quality Figure and Scientific Write-Up",
    "text": "Publication-Quality Figure and Scientific Write-Up\nLet’s create a publication-quality figure for our multiple logistic regression model and show how we would write up the results for a scientific publication.\n\n# Create a more polished visualization for shrub cover effect\npolished_pred &lt;- predict_for_var(\"shrub_cover\", rodent_model, fragment_data)\n\n# Calculate confidence intervals\npred_se &lt;- predict(rodent_model, \n                  newdata = polished_pred, \n                  type = \"link\", \n                  se.fit = TRUE)\n\n# Convert to data frame with CIs\nci_data &lt;- data.frame(\n  shrub_cover = polished_pred$shrub_cover,\n  fit = pred_se$fit,\n  se = pred_se$se.fit\n)\n\n# Calculate upper and lower bounds of CI on link scale\nci_data$lower_link &lt;- ci_data$fit - 1.96 * ci_data$se\nci_data$upper_link &lt;- ci_data$fit + 1.96 * ci_data$se\n\n# Transform back to probability scale\nci_data$prob &lt;- plogis(ci_data$fit)\nci_data$lower_prob &lt;- plogis(ci_data$lower_link)\nci_data$upper_prob &lt;- plogis(ci_data$upper_link)\n\n# Create plot\nggplot() +\n  # Add jittered points for raw data\n  geom_jitter(data = fragment_data, \n             aes(x = shrub_cover, \n                 y = as.numeric(rodent_present == \"Present\")),\n             width = 0, height = 0.05, alpha = 0.6, size = 3) +\n  # Add fitted probability curve\n  geom_line(data = ci_data, \n           aes(x = shrub_cover, y = prob), \n           color = \"darkblue\", size = 1.2) +\n  # Add confidence intervals\n  geom_ribbon(data = ci_data, \n             aes(x = shrub_cover, \n                 ymin = lower_prob, \n                 ymax = upper_prob), \n             alpha = 0.2, fill = \"darkblue\") +\n  # Customize appearance\n  labs(title = \"Effect of Shrub Cover on Native Rodent Presence\",\n       subtitle = \"Probability of occurrence in canyon fragments\",\n       x = \"Percentage Shrub Cover\",\n       y = \"Probability of Rodent Presence\") +\n  scale_y_continuous(limits = c(0, 1), \n                     breaks = seq(0, 1, 0.2)) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"none\",\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(fill = NA, color = \"gray80\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nScientific Write-Up Example\n\n\n\nResults\nThe presence of native rodents in canyon fragments was modeled using multiple logistic regression with three predictors: distance to nearest source canyon, years since isolation, and percentage of shrub cover. The model was statistically significant (χ² = 12.63, df = 3, p = 0.005) and explained 38.7% of the variation in rodent presence (McFadden’s R² = 0.387).\nAmong the predictors, only shrub cover had a statistically significant effect on rodent presence (β = 0.091, SE = 0.041, p = 0.026). The odds ratio for shrub cover was 1.095 (95% CI: 1.011-1.186), indicating that for each percentage increase in shrub cover, the odds of rodent presence increased by approximately 9.5%. Neither distance to source canyon (β = 0.0002, p = 0.690) nor years since isolation (β = 0.022, p = 0.566) showed significant relationships with rodent presence.\nThe model correctly classified 76% of the fragments, with a sensitivity of 0.77 and a specificity of 0.75. Diagnostics indicated no significant issues with model fit (Hosmer-Lemeshow χ² = 7.31, df = 8, p = 0.504).\nDiscussion\nOur findings suggest that vegetation structure, as measured by shrub cover, plays a crucial role in determining the presence of native rodents in canyon fragments. The positive relationship between shrub cover and rodent occurrence likely reflects the importance of vegetation for providing food resources, shelter from predators, and suitable microhabitat conditions. Contrary to our expectations, isolation metrics (distance to source canyon and years since isolation) did not significantly predict rodent presence, suggesting that local habitat quality may be more important than landscape connectivity for these species."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#relationship-between-glms-and-anovas",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#relationship-between-glms-and-anovas",
    "title": "Lecture 15 - xxxxxx",
    "section": "Relationship Between GLMs and ANOVAs",
    "text": "Relationship Between GLMs and ANOVAs\n\n\n\n\n\n\nGLMs and ANOVAs: The Connection\n\n\n\nGeneral linear models (including ANOVAs and standard regression) are special cases of Generalized Linear Models where:\n\nThe response variable follows a normal distribution\nThe link function is the identity function\n\nTherefore, a one-way ANOVA is equivalent to: - A linear regression with a categorical predictor - A Gaussian GLM with an identity link and a categorical predictor\n\n\nLet’s demonstrate this equivalence:\n\n# 1. Standard ANOVA\nanova_model &lt;- aov(mpg ~ cyl, data = mtcars)\n\n# 2. Linear regression\nlm_model &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# 3. Gaussian GLM\nglm_model &lt;- glm(mpg ~ cyl, family = gaussian(link = \"identity\"), data = mtcars)\n\n# Compare coefficients\ncoef_comparison &lt;- data.frame(\n  Term = names(coef(lm_model)),\n  `Linear Regression` = coef(lm_model),\n  `Gaussian GLM` = coef(glm_model)\n)\n\n# Display the comparison\ncoef_comparison %&gt;%\n  mutate(across(where(is.numeric), round, 3)) %&gt;%\n  flextable()\n\nTermLinear.RegressionGaussian.GLM(Intercept)26.66426.664cyl6-6.921-6.921cyl8-11.564-11.564\n\n# Compare ANOVA tables\nanova_aov &lt;- anova(anova_model)\nanova_lm &lt;- anova(lm_model)\nanova_glm &lt;- anova(glm_model)\n\n# Create visualization showing the three approaches\n# Use the same data and estimated means\nggplot() +\n  # Plot raw data\n  geom_boxplot(data = mtcars, \n              aes(x = cyl, y = mpg, group = cyl),\n              alpha = 0.3, width = 0.5) +\n  geom_jitter(data = mtcars, \n             aes(x = cyl, y = mpg),\n             width = 0.1, alpha = 0.6) +\n  # Add fitted values from each model\n  geom_point(data = emmeans(lm_model, ~cyl) %&gt;% data.frame(),\n            aes(x = cyl, y = emmean), \n            color = \"red\", size = 3, shape = 17) +\n  geom_point(data = emmeans(glm_model, ~cyl) %&gt;% data.frame(),\n            aes(x = cyl, y = emmean), \n            color = \"blue\", size = 3, shape = 15) +\n  # Add legend for model types\n  annotate(\"text\", x = \"8\", y = 30, \n          label = \"Red triangles: Linear Regression\\nBlue squares: Gaussian GLM\", \n          hjust = 1, size = 3.5) +\n  labs(title = \"Comparison of Models: ANOVA, Linear Regression, and Gaussian GLM\",\n       subtitle = \"All three approaches yield identical results\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#assumptions-and-diagnostics-summary",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#assumptions-and-diagnostics-summary",
    "title": "Lecture 15 - xxxxxx",
    "section": "Assumptions and Diagnostics Summary",
    "text": "Assumptions and Diagnostics Summary\n\n\nGeneralized Linear Models have different assumptions depending on the specific distribution and link function used:\nAll GLMs: - Independence of observations - Correct specification of the link function - Correct specification of the variance structure - No influential outliers - No multicollinearity among predictors\nGaussian GLMs (including linear regression): - Normality of residuals - Homogeneity of variance\nPoisson GLMs: - Count data (non-negative integers) - Mean equals variance (if overdispersed, consider negative binomial)\nLogistic GLMs: - Binary response variable - Linear relationship between predictors and log odds - Adequate sample size relative to number of parameters\n\nThe following R code checks some common diagnostics for our logistic model:\n\n# Create diagnostic plots for the rodent model\npar(mfrow = c(2, 2))\n\n# 1. Residuals vs fitted\nplot(fitted(rodent_model), residuals(rodent_model, type = \"pearson\"),\n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted Values (predicted probabilities)\",\n     ylab = \"Pearson Residuals\",\n     pch = 16)\nabline(h = 0, lty = 2)\n\n# 2. Leverage\nleverage &lt;- hatvalues(rodent_model)\nplot(leverage, residuals(rodent_model, type = \"pearson\"),\n     main = \"Residuals vs Leverage\",\n     xlab = \"Leverage\",\n     ylab = \"Pearson Residuals\",\n     pch = 16)\nabline(h = 0, lty = 2)\n\n# 3. Cook's distance\ncook &lt;- cooks.distance(rodent_model)\nplot(cook, main = \"Cook's Distance\",\n     ylab = \"Cook's Distance\",\n     pch = 16)\nabline(h = 4/length(cook), lty = 2, col = \"red\")  # Rule of thumb threshold\n\n# 4. Observed vs Predicted probabilities\nplot(predicted_probs, \n     as.numeric(fragment_data$rodent_present) - 1,\n     main = \"Observed vs Predicted\",\n     xlab = \"Predicted Probability\",\n     ylab = \"Observed (0/1)\",\n     pch = 16)\ncurve(I, from = 0, to = 1, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#summary-and-conclusions",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#summary-and-conclusions",
    "title": "Lecture 15 - xxxxxx",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nGeneralized Linear Models (GLMs) provide a powerful and flexible framework for analyzing a wide range of data types in biology:\n\nGaussian GLMs with identity link function are equivalent to standard linear models and ANOVAs, suitable for normally distributed continuous responses.\nPoisson GLMs with log link function are appropriate for count data, but be cautious of overdispersion.\nLogistic GLMs with logit link function are useful for binary responses, modeling the probability of success or presence.\n\nKey advantages of GLMs include:\n\nAbility to handle various types of response variables beyond normal distributions\nUnified framework for linear modeling\nFlexibility in specifying the link function to match the data structure\nInterpretable parameters, though interpretation differs by model type\n\nWhen working with GLMs:\n\nChoose the appropriate distribution family based on your response variable\nVerify model assumptions through diagnostic plots\nWatch for overdispersion in count data\nUse odds ratios to interpret logistic regression results\nCompare competing models using likelihood ratio tests and information criteria\n\nThis framework allows biologists to appropriately model many types of data encountered in ecological, behavioral, and physiological research."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#references",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_html.html#references",
    "title": "Lecture 15 - xxxxxx",
    "section": "References",
    "text": "References\nAgresti, A. (1996). An Introduction to Categorical Data Analysis. Wiley, New York.\nBolger, D. T., Alberts, A. C., Sauvajot, R. M., Potenza, P., McCalvin, C., Tran, D., Mazzoni, S., & Soulé, M. E. (1997). Response of rodents to habitat fragmentation in coastal southern California. Ecological Applications, 7(2), 552-563.\nChristensen, R. (1997). Log-linear Models and Logistic Regression. Springer, New York.\nHosmer, D. W., & Lemeshow, S. (1989). Applied Logistic Regression. Wiley, New York.\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models. Chapman and Hall, London.\nPolis, G. A., Hurd, S. D., Jackson, C. T., & Piñero, F. S. (1998). Multifactor analysis of ecosystem patterns on islands in the Gulf of California. Ecological Monographs, 68, 490-502."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#review",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#review",
    "title": "Lecture 15 - xxxxxx",
    "section": "Review",
    "text": "Review\n\nANOVA\nFactorial ANOVA\nNested ANOVA\nASSUMPIONS OF ALL\n\nHomogeneity of variance - Levenes or Bartlets Test\nNormality of Residuals\nIndependence"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 15 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nGeneral Linear Models GLM\n\nExamples:\n\n\nLogistic Regression"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#overview-of-generalized-linear-models-glms",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#overview-of-generalized-linear-models-glms",
    "title": "Lecture 15 - xxxxxx",
    "section": "Overview of Generalized Linear Models (GLMs)",
    "text": "Overview of Generalized Linear Models (GLMs)\n\n\nGeneral linear models assume normal distribution of response variables and residuals. However, many types of biological data don’t meet this assumption. Generalized Linear Models (GLMs) allow for a wider range of probability distributions for the response variable.\nGLMs allow all types of “exponential family” distributions:\n\nNormal\nLognormal\nBinomial\nPoisson\nGamma\nNegative binomial\n\nGLMs can be used for binary (yes/no), discrete (count), and categorical/multinomial response variables, using maximum likelihood (ML) rather than ordinary least squares (OLS) for estimation.\nNote: GLMs extend linear models to non-normal data distributions.\n\n\n\n\n\n\nExamples of distributions in the exponential family"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#the-three-elements-of-a-glm",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#the-three-elements-of-a-glm",
    "title": "Lecture 15 - xxxxxx",
    "section": "The Three Elements of a GLM",
    "text": "The Three Elements of a GLM\nGLMs consist of three components:\n\nRandom component: The response variable and its probability distribution (from exponential family: normal, binomial, Poisson)\nSystematic component: The predictor variable(s) in the model, which can be continuous or categorical\nLink function: Connects expected value of Y to predictor variables\n\\[g(\\mu) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2...\\]\n\n\n\n\n\n\n\nLink Functions and Distributions\n\n\n\n\n\nDistribution\nCommon Link Function\nFormula\n\n\n\n\nNormal\nIdentity\n\\(g(\\mu) = \\mu\\)\n\n\nPoisson\nLog\n\\(g(\\mu) = \\log(\\mu)\\)\n\n\nBinomial\nLogit\n\\(g(\\mu) = \\log[\\mu/(1-\\mu)]\\)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#glm-with-gaussian-normal-distribution",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#glm-with-gaussian-normal-distribution",
    "title": "Lecture 15 - xxxxxx",
    "section": "GLM with Gaussian (Normal) Distribution",
    "text": "GLM with Gaussian (Normal) Distribution\n\n\nThe simplest form of GLM uses a normal (Gaussian) distribution with an identity link function. This is equivalent to standard linear regression.\nLet’s compare a standard linear model and a Gaussian GLM using the mtcars dataset, modeling miles per gallon (mpg) by the number of cylinders (cyl).\n\n# Convert cylinders to a factor\nmtcars &lt;- mtcars %&gt;%\n  mutate(cyl = factor(cyl))\n\n# Fit a standard linear model\nmodel_lm &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# Fit a Gaussian GLM\nmodel_gaussian &lt;- glm(mpg ~ cyl, \n                       data = mtcars, \n                       family = gaussian(link = \"identity\"))\n\n# Compare the coefficients\ncoef_lm &lt;- coefficients(model_lm)\ncoef_glm &lt;- coefficients(model_gaussian)\n\n# Check if they're the same\nall.equal(coef_lm, coef_glm)\n\n[1] TRUE\n\n\nLet’s look at the summary of our Gaussian GLM:\n\nsummary(model_gaussian)\n\n\nCall:\nglm(formula = mpg ~ cyl, family = gaussian(link = \"identity\"), \n    data = mtcars)\n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  26.6636     0.9718  27.437  &lt; 2e-16 ***\ncyl6         -6.9208     1.5583  -4.441 0.000119 ***\ncyl8        -11.5636     1.2986  -8.905 8.57e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for gaussian family taken to be 10.38837)\n\n    Null deviance: 1126.05  on 31  degrees of freedom\nResidual deviance:  301.26  on 29  degrees of freedom\nAIC: 170.56\n\nNumber of Fisher Scoring iterations: 2\n\n\nNow let’s perform an ANOVA on our GLM model using the car package:\n\nAnova(model_gaussian, type = \"III\", test = \"F\")\n\nAnalysis of Deviance Table (Type III tests)\n\nResponse: mpg\nError estimate based on Pearson residuals \n\n          Sum Sq Df F values    Pr(&gt;F)    \ncyl       824.78  2   39.697 4.979e-09 ***\nResiduals 301.26 29                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nVisualizing the results:\n\n# Get estimated means\nemm_gaussian &lt;- emmeans(model_gaussian, ~ cyl)\nemm_df &lt;- as.data.frame(emm_gaussian)\n\n# Create plot of data with estimated means\nggplot() +\n  # Plot raw data\n  geom_jitter(data = mtcars, \n              aes(x = cyl, y = mpg), \n              width = 0.2, \n              alpha = 0.5) +\n  # Add estimated means with confidence intervals\n  geom_point(data = emm_df, \n             aes(x = cyl, y = emmean), \n             size = 4, color = \"red\") +\n  geom_errorbar(data = emm_df, \n                aes(x = cyl, \n                    ymin = lower.CL, \n                    ymax = upper.CL), \n                width = 0.2, \n                color = \"red\") +\n  labs(title = \"Effect of Cylinders on MPG\",\n       subtitle = \"Red points show estimated means with 95% CIs\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEquivalence of Linear Models and Gaussian GLMs\n\n\nWhen we use a Gaussian distribution with an identity link, GLM gives identical results to standard linear regression. This can be seen in the coefficient values and overall model statistics.\nThe key difference is that GLMs provide a framework that extends to non-normal distributions."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#glm-with-poisson-distribution",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#glm-with-poisson-distribution",
    "title": "Lecture 15 - xxxxxx",
    "section": "GLM with Poisson Distribution",
    "text": "GLM with Poisson Distribution\n\n\nPoisson GLMs are appropriate for count data. The Poisson distribution assumes that the variance equals the mean.\nFor this example, we’ll use the quarter-mile time (qsec) from the mtcars dataset, rounded to create a count-like variable.\n\n# Prepare data for Poisson model\nmtcars_count &lt;- mtcars %&gt;%\n  mutate(\n    cyl = factor(cyl),\n    qsec_round = round(qsec)  # Create a count-like variable\n  )\n\n# Look at the first few rows\nhead(mtcars_count[, c(\"cyl\", \"qsec\", \"qsec_round\")])\n\n                  cyl  qsec qsec_round\nMazda RX4           6 16.46         16\nMazda RX4 Wag       6 17.02         17\nDatsun 710          4 18.61         19\nHornet 4 Drive      6 19.44         19\nHornet Sportabout   8 17.02         17\nValiant             6 20.22         20\n\n\nNow let’s fit a Poisson GLM to model the relationship between the rounded quarter-mile time and the number of cylinders:\n\n# Fit a Poisson GLM\nmodel_poisson &lt;- glm(qsec_round ~ cyl, \n                     family = poisson(link = \"log\"), \n                     data = mtcars_count)\n\n# Look at the model summary\nsummary(model_poisson)\n\n\nCall:\nglm(formula = qsec_round ~ cyl, family = poisson(link = \"log\"), \n    data = mtcars_count)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.95869    0.06868  43.079   &lt;2e-16 ***\ncyl6        -0.07629    0.11277  -0.676    0.499    \ncyl8        -0.14243    0.09482  -1.502    0.133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 5.6979  on 31  degrees of freedom\nResidual deviance: 3.4487  on 29  degrees of freedom\nAIC: 160.62\n\nNumber of Fisher Scoring iterations: 3\n\n\nLet’s check for overdispersion, which is common in count data:\n\n# Calculate dispersion parameter\ndispersion_poisson &lt;- sum(residuals(model_poisson, \n                             type = \"pearson\")^2) / \n                     model_poisson$df.residual\n\n# Print dispersion parameter\ncat(\"Dispersion parameter:\", round(dispersion_poisson, 2), \"\\n\")\n\nDispersion parameter: 0.12 \n\n# Should be close to 1 for a well-fitting Poisson model\n# If &gt; 1.5, may indicate overdispersion\n\n\n\n# Get estimated means on the response scale\nemm_poisson &lt;- emmeans(model_poisson, ~ cyl, type = \"response\")\nemm_poisson_df &lt;- as.data.frame(emm_poisson)\n\n# Create visualization\nggplot() +\n  # Plot raw data\n  geom_jitter(data = mtcars_count, \n              aes(x = cyl, y = qsec_round), \n              width = 0.2, \n              alpha = 0.5) +\n  # Add estimated means with confidence intervals\n  geom_point(data = emm_poisson_df, \n             aes(x = cyl, y = rate), \n             size = 4, color = \"blue\") +\n  geom_errorbar(data = emm_poisson_df, \n                aes(x = cyl, \n                    ymin = asymp.LCL, \n                    ymax = asymp.UCL), \n                width = 0.2, \n                color = \"blue\") +\n  labs(title = \"Effect of Cylinders on Quarter-Mile Time\",\n       subtitle = \"Poisson GLM with log link\",\n       x = \"Number of Cylinders\",\n       y = \"Quarter-Mile Time (rounded)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpreting Poisson GLM Coefficients\n\n\nIn a Poisson GLM with a log link function:\n\nThe coefficients represent changes in the log of the expected count\nWhen exponentiated (exp(coef)), they represent multiplicative effects\nFor example, exp(coef) = 0.90 means the expected count is 90% of the reference level\n\n\n\n\nChecking Model Assumptions with DHARMa\nDHARMa provides a useful framework for diagnosing GLM residuals:\n\n# Simulate residuals using DHARMa\nset.seed(123) # For reproducibility\nsimulation_poisson &lt;- simulateResiduals(fittedModel = model_poisson, n = 1000)\n\n# Plot diagnostic plots\nplot(simulation_poisson)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#dealing-with-overdispersion-in-count-data",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#dealing-with-overdispersion-in-count-data",
    "title": "Lecture 15 - xxxxxx",
    "section": "Dealing with Overdispersion in Count Data",
    "text": "Dealing with Overdispersion in Count Data\n\n\nWhen count data shows more variability than expected under a Poisson distribution (variance &gt; mean), we may need to use a negative binomial model instead.\n\n# If we detected overdispersion, we could fit a negative binomial model\n# This is just for demonstration - our data may not actually need this\n\n# Fit negative binomial model\nmodel_nb &lt;- glm.nb(qsec_round ~ cyl, data = mtcars_count)\n\n# Compare summaries\nsummary(model_nb)\n\n\nCall:\nglm.nb(formula = qsec_round ~ cyl, data = mtcars_count, init.theta = 2935650.009, \n    link = log)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.95869    0.06868  43.079   &lt;2e-16 ***\ncyl6        -0.07629    0.11277  -0.676    0.499    \ncyl8        -0.14243    0.09482  -1.502    0.133    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(2935650) family taken to be 1)\n\n    Null deviance: 5.6979  on 31  degrees of freedom\nResidual deviance: 3.4486  on 29  degrees of freedom\nAIC: 162.62\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  2935650 \n          Std. Err.:  121368753 \nWarning while fitting theta: iteration limit reached \n\n 2 x log-likelihood:  -154.616 \n\n\nThe negative binomial model includes an additional dispersion parameter (theta) that allows the variance to be larger than the mean.\n\nLet’s compare the predictions from both models:\n\n# Create predictions from both models\nmtcars_count$pred_poisson &lt;- predict(model_poisson, \n                                    type = \"response\")\nmtcars_count$pred_nb &lt;- predict(model_nb, \n                               type = \"response\")\n\n# Compare predictions\nggplot(mtcars_count) +\n  geom_point(aes(x = pred_poisson, y = pred_nb, color = cyl)) +\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\") +\n  labs(title = \"Comparison of Poisson and Negative Binomial Predictions\",\n       x = \"Poisson Predictions\",\n       y = \"Negative Binomial Predictions\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#logistic-regression---introduction",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#logistic-regression---introduction",
    "title": "Lecture 15 - xxxxxx",
    "section": "Logistic Regression - Introduction",
    "text": "Logistic Regression - Introduction\n\n\nLogistic regression is a GLM used when the response variable is binary (e.g., dead/alive, present/absent). It models the probability of the response being “1” (success) given predictor values.\nLet’s examine the simple logistic regression model:\n\\[\\pi(x) = \\frac{e^{\\beta_0 + \\beta_1 x}}{1 + e^{\\beta_0 + \\beta_1 x}}\\]\nWhere: - \\(\\pi(x)\\) is the probability that Y = 1 given X = x - \\(\\beta_0\\) is the intercept - \\(\\beta_1\\) is the slope (rate of change in \\(\\pi(x)\\) for a unit change in X)\nTo linearize this relationship, we use the logit link function:\n\\[g(x) = \\log\\left(\\frac{\\pi(x)}{1-\\pi(x)}\\right) = \\beta_0 + \\beta_1 x\\]\nThis transforms the probability (which is bounded between 0 and 1) to a linear function that can range from -∞ to +∞.\n\n\n# Create data for sigmoid curve\nsigmoid_data &lt;- data.frame(\n  x = seq(-6, 6, length.out = 100)\n)\nsigmoid_data$p &lt;- 1 / (1 + exp(-sigmoid_data$x))\n\n# Plot the sigmoid curve\nggplot(sigmoid_data, aes(x, p)) +\n  geom_line(linewidth = 1.2, color = \"darkblue\") +\n  geom_hline(yintercept = c(0, 0.5, 1), \n             linetype = \"dashed\", \n             color = \"gray50\") +\n  geom_vline(xintercept = 0, \n             linetype = \"dashed\", \n             color = \"gray50\") +\n  labs(title = \"Logistic Function\",\n       subtitle = \"Mapping from linear predictor to probability\",\n       x = \"Linear predictor (β₀ + β₁x)\",\n       y = \"Probability π(x)\") +\n  scale_y_continuous(breaks = seq(0, 1, 0.25)) +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#example-lizard-presence-on-islands",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#example-lizard-presence-on-islands",
    "title": "Lecture 15 - xxxxxx",
    "section": "Example: Lizard Presence on Islands",
    "text": "Example: Lizard Presence on Islands\nBased on the example from Polis et al. (1998), we’ll model the presence/absence of lizards (Uta) on islands in the Gulf of California based on perimeter/area ratio.\n\n# Create a simulated dataset based on the described study\nset.seed(123)\nisland_data &lt;- data.frame(\n  island_id = 1:19,\n  pa_ratio = c(5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 10, 15, 20, 25, 30),\n  uta_present = c(1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0)\n) %&gt;%\n  mutate(uta_present = factor(uta_present, levels = c(0, 1), labels = c(\"Absent\", \"Present\")))\n\n# Fit the logistic regression model\nlizard_model &lt;- glm(uta_present ~ pa_ratio, \n                    data = island_data, \n                    family = binomial(link = \"logit\"))\n\n# Model summary\nsummary(lizard_model)\n\n\nCall:\nglm(formula = uta_present ~ pa_ratio, family = binomial(link = \"logit\"), \n    data = island_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    241.039 191755.596   0.001    0.999\npa_ratio        -8.766   6965.289  -0.001    0.999\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 2.6287e+01  on 18  degrees of freedom\nResidual deviance: 2.4292e-09  on 17  degrees of freedom\nAIC: 4\n\nNumber of Fisher Scoring iterations: 25\n\n\n\n\nLet’s visualize the data and the fitted model:\n\n# Create a dataframe for predictions\npred_data &lt;- data.frame(\n  pa_ratio = seq(min(island_data$pa_ratio), \n                max(island_data$pa_ratio), \n                length.out = 100)\n)\n\n# Get predicted probabilities\npred_data$prob &lt;- predict(lizard_model, \n                         newdata = pred_data, \n                         type = \"response\")\n\n# Plot\nggplot() +\n  # Add jittered points for observed data\n  geom_jitter(data = island_data, \n              aes(x = pa_ratio, y = as.numeric(uta_present) - 1),\n              height = 0.05, width = 0, alpha = 0.7) +\n  # Add predicted probability curve\n  geom_line(data = pred_data, \n            aes(x = pa_ratio, y = prob), \n            color = \"blue\", size = 1) +\n  # Add confidence intervals (optional)\n  labs(title = \"Probability of Uta Presence vs. Perimeter/Area Ratio\",\n       x = \"Perimeter/Area Ratio\",\n       y = \"Probability of Presence\") +\n  scale_y_continuous(limits = c(0, 1)) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nWe want to test the null hypothesis that β₁ = 0, meaning there’s no relationship between P/A ratio and lizard presence.\nThere are two common ways to test this hypothesis:\n\nWald test: Tests if the parameter estimate divided by its standard error differs significantly from zero\nLikelihood ratio test: Compares the fit of the full model to a reduced model without the predictor variable\n\n\n# Reduced model (intercept only)\nreduced_model &lt;- glm(uta_present ~ 1, \n                     data = island_data, \n                     family = binomial(link = \"logit\"))\n\n# Likelihood ratio test\nanova(reduced_model, lizard_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: uta_present ~ 1\nModel 2: uta_present ~ pa_ratio\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        18     26.287                          \n2        17      0.000  1   26.287 2.943e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nInterpreting the Odds Ratio\n\n\n\n\n\n\nWorking with Odds Ratios\n\n\nThe odds ratio represents how the odds of the event (e.g., lizard presence) change with a unit increase in the predictor.\n\nOdds ratio = exp(β₁)\nIf odds ratio &gt; 1: Increasing the predictor increases the odds of event\nIf odds ratio &lt; 1: Increasing the predictor decreases the odds of event\nIf odds ratio = 1: No effect of predictor on odds of event\n\n\n\n\n\n# Calculate odds ratio and confidence interval\ncoef_lizard &lt;- coef(lizard_model)[2]  # Extract slope coefficient\nodds_ratio &lt;- exp(coef_lizard)\nci &lt;- exp(confint(lizard_model, \"pa_ratio\"))\n\n# Display results\ncat(\"Odds Ratio:\", round(odds_ratio, 3), \"\\n\")\n\nOdds Ratio: 0 \n\ncat(\"95% CI:\", round(ci[1], 3), \"to\", round(ci[2], 3), \"\\n\")\n\n95% CI: 0 to Inf \n\n\nAssessing Model Fit\nThere are several ways to assess the goodness-of-fit for logistic regression models:\n\n# Calculate Hosmer-Lemeshow statistic\n# This would normally require an additional package like 'ResourceSelection'\n# Instead, we'll use a simpler approximation and other diagnostics\n\n# Calculate Pearson residuals\npearson_resid &lt;- residuals(lizard_model, type = \"pearson\")\npearson_chi2 &lt;- sum(pearson_resid^2)\ndf_resid &lt;- lizard_model$df.residual\n\n# Calculate deviance\ndeviance_g2 &lt;- lizard_model$deviance\nnull_deviance &lt;- lizard_model$null.deviance\n\n# Calculate McFadden's pseudo-R²\nr2_mcfadden &lt;- 1 - (deviance_g2 / null_deviance)\n\n# Display results\ncat(\"Pearson χ²:\", round(pearson_chi2, 3), \"on\", df_resid, \"df, p =\", \n    round(1 - pchisq(pearson_chi2, df_resid), 3), \"\\n\")\n\nPearson χ²: 0 on 17 df, p = 1 \n\ncat(\"Deviance G²:\", round(deviance_g2, 3), \"on\", df_resid, \"df, p =\", \n    round(1 - pchisq(deviance_g2, df_resid), 3), \"\\n\")\n\nDeviance G²: 0 on 17 df, p = 1 \n\ncat(\"McFadden's R²:\", round(r2_mcfadden, 3), \"\\n\")\n\nMcFadden's R²: 1"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#multiple-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#multiple-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Multiple Logistic Regression",
    "text": "Multiple Logistic Regression\n\n\nLogistic regression can be extended to include multiple predictors. The model becomes:\n\\[g(x) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ldots + \\beta_p x_p\\]\nWhere g(x) is the logit link function, and x₁, x₂, …, xₚ are the predictor variables.\nLet’s create a simulated dataset based on the Bolger et al. (1997) study of the presence/absence of native rodents in canyon fragments.\n\n# Simulate data for the rodent example\nset.seed(123)\nn &lt;- 25  # 25 canyon fragments\n\n# Create predictor variables\nfragment_data &lt;- data.frame(\n  fragment_id = paste0(\"F\", 1:n),\n  distance = runif(n, 0, 3000),            # Distance to source canyon (m)\n  age = runif(n, 5, 80),                   # Years since isolation\n  shrub_cover = runif(n, 10, 100)          # Percentage shrub cover\n)\n\n# Generate response variable (rodent presence)\n# Higher probability with higher shrub cover, slight effect of age\nlinear_pred &lt;- -5 + 0.0001*fragment_data$distance + \n               0.02*fragment_data$age + \n               0.09*fragment_data$shrub_cover\nprob &lt;- 1 / (1 + exp(-linear_pred))\nfragment_data$rodent_present &lt;- rbinom(n, 1, prob)\nfragment_data$rodent_present &lt;- factor(fragment_data$rodent_present, \n                                      levels = c(0, 1), \n                                      labels = c(\"Absent\", \"Present\"))\n\n# Fit multiple logistic regression model\nrodent_model &lt;- glm(rodent_present ~ distance + age + shrub_cover, \n                    data = fragment_data, \n                    family = binomial(link = \"logit\"))\n\n# Model summary\nsummary(rodent_model)\n\n\nCall:\nglm(formula = rodent_present ~ distance + age + shrub_cover, \n    family = binomial(link = \"logit\"), data = fragment_data)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) -12.278261   7.911491  -1.552   0.1207  \ndistance      0.002062   0.001716   1.202   0.2294  \nage           0.068744   0.059665   1.152   0.2493  \nshrub_cover   0.193001   0.116035   1.663   0.0963 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 27.5540  on 24  degrees of freedom\nResidual deviance:  9.2737  on 21  degrees of freedom\nAIC: 17.274\n\nNumber of Fisher Scoring iterations: 8\n\n\n\nTo test the significance of individual predictors, we can use likelihood ratio tests comparing nested models:\n\n# Test distance\nmodel_no_distance &lt;- glm(rodent_present ~ age + shrub_cover, \n                         data = fragment_data, \n                         family = binomial(link = \"logit\"))\nanova(model_no_distance, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ age + shrub_cover\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        22    11.3831                     \n2        21     9.2737  1   2.1094   0.1464\n\n# Test age\nmodel_no_age &lt;- glm(rodent_present ~ distance + shrub_cover, \n                    data = fragment_data, \n                    family = binomial(link = \"logit\"))\nanova(model_no_age, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ distance + shrub_cover\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)\n1        22    11.0533                     \n2        21     9.2737  1   1.7796   0.1822\n\n# Test shrub cover\nmodel_no_shrub &lt;- glm(rodent_present ~ distance + age, \n                      data = fragment_data, \n                      family = binomial(link = \"logit\"))\nanova(model_no_shrub, rodent_model, test = \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: rodent_present ~ distance + age\nModel 2: rodent_present ~ distance + age + shrub_cover\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1        22    26.7315                          \n2        21     9.2737  1   17.458 2.938e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\nLet’s calculate odds ratios and confidence intervals for all predictors:\n\n# Calculate odds ratios and CIs\ncoefs &lt;- coef(rodent_model)[-1]  # Exclude intercept\nodds_ratios &lt;- exp(coefs)\nci &lt;- exp(confint(rodent_model)[-1, ])  # Exclude intercept\n\n# Create a data frame for display\nor_df &lt;- data.frame(\n  Predictor = names(coefs),\n  OddsRatio = odds_ratios,\n  LowerCI = ci[, 1],\n  UpperCI = ci[, 2]\n)\n\n# Display formatted table\nor_df %&gt;%\n  mutate(across(where(is.numeric), round, 4)) %&gt;%\n  mutate(CI = paste0(\"(\", LowerCI, \", \", UpperCI, \")\")) %&gt;%\n  dplyr::select(Predictor, OddsRatio, CI) %&gt;%\n  flextable()\n\nPredictorOddsRatioCIdistance1.0021(0.9994, 1.0069)age1.0712(0.9721, 1.2577)shrub_cover1.2129(1.0645, 1.7909)"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#visualizing-multiple-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#visualizing-multiple-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Visualizing Multiple Logistic Regression",
    "text": "Visualizing Multiple Logistic Regression\n\n\nFor multiple predictors, we can visualize the effect of each predictor while holding others constant at their mean or median values.\n\n# Create a function to generate prediction data for one variable\npredict_for_var &lt;- function(var_name, model, data) {\n  # Create grid of values for the variable of interest\n  pred_df &lt;- data.frame(\n    x = seq(min(data[[var_name]]), max(data[[var_name]]), length.out = 100)\n  )\n  names(pred_df) &lt;- var_name\n  \n  # Add mean values for other predictors\n  for (other_var in c(\"distance\", \"age\", \"shrub_cover\")) {\n    if (other_var != var_name) {\n      pred_df[[other_var]] &lt;- mean(data[[other_var]])\n    }\n  }\n  \n  # Add predictions\n  pred_df$prob &lt;- predict(model, newdata = pred_df, type = \"response\")\n  \n  return(pred_df)\n}\n\n# Generate prediction data for each variable\npred_distance &lt;- predict_for_var(\"distance\", rodent_model, fragment_data)\npred_age &lt;- predict_for_var(\"age\", rodent_model, fragment_data)\npred_shrub &lt;- predict_for_var(\"shrub_cover\", rodent_model, fragment_data)\n\n# Create plots\np1 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = distance, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_distance, aes(x = distance, y = prob), \n            color = \"darkred\", size = 1) +\n  labs(title = \"Effect of Distance\",\n       x = \"Distance to Source (m)\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\np2 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = age, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_age, aes(x = age, y = prob), \n            color = \"darkgreen\", size = 1) +\n  labs(title = \"Effect of Age\",\n       x = \"Years Since Isolation\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\np3 &lt;- ggplot() +\n  geom_rug(data = fragment_data, \n           aes(x = shrub_cover, y = as.numeric(rodent_present) - 1),\n           sides = \"b\", alpha = 0.7) +\n  geom_line(data = pred_shrub, aes(x = shrub_cover, y = prob), \n            color = \"darkblue\", size = 1) +\n  labs(title = \"Effect of Shrub Cover\",\n       x = \"Shrub Cover (%)\",\n       y = \"Probability of Presence\") +\n  theme_minimal()\n\n# Combine plots\np1 + p2 + p3\n\n\n\n\n\n\n\n\n\nThis visualization shows the effect of each predictor on the probability of rodent presence, while holding the other predictors constant at their mean values."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#assumptions-and-diagnostics-of-logistic-regression",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#assumptions-and-diagnostics-of-logistic-regression",
    "title": "Lecture 15 - xxxxxx",
    "section": "Assumptions and Diagnostics of Logistic Regression",
    "text": "Assumptions and Diagnostics of Logistic Regression\nLogistic regression has several key assumptions:\n\nIndependence of observations\nLinear relationship between predictors and log odds\nNo extreme outliers\nNo multicollinearity (when multiple predictors are used)\n\nLet’s check the diagnostics for our multiple logistic regression model:\n\n# 1. Check for linearity between predictors and log odds\n# Use bins of X variables and plot log odds\ncheck_linearity &lt;- function(model, data, var) {\n  # Create bins of predictor\n  n_bins &lt;- 5\n  data$bin &lt;- cut(data[[var]], breaks = n_bins)\n  \n  # Calculate log odds for each bin\n  bin_summary &lt;- data %&gt;%\n    group_by(bin) %&gt;%\n    summarize(\n      n = n(),\n      mean_var = mean(!!sym(var)),\n      successes = sum(rodent_present == \"Present\"),\n      failures = sum(rodent_present == \"Absent\")\n    ) %&gt;%\n    mutate(\n      p = successes / n,\n      logodds = log(p / (1 - p))\n    )\n  \n  # Create plot\n  ggplot(bin_summary, aes(x = mean_var, y = logodds)) +\n    geom_point(size = 3) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n    labs(title = paste(\"Linearity Check:\", var),\n         x = var,\n         y = \"Log Odds\") +\n    theme_minimal()\n}\n\n# Create diagnostic plots for each variable\np1 &lt;- check_linearity(rodent_model, fragment_data, \"distance\")\np2 &lt;- check_linearity(rodent_model, fragment_data, \"age\")\np3 &lt;- check_linearity(rodent_model, fragment_data, \"shrub_cover\")\n\n# Arrange the plots\np1 / p2 / p3"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#model-comparison-and-selection",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#model-comparison-and-selection",
    "title": "Lecture 15 - xxxxxx",
    "section": "Model Comparison and Selection",
    "text": "Model Comparison and Selection\n\n\nWhen working with multiple predictors, we often want to find the most parsimonious model. We can use:\n\nLikelihood ratio tests for nested models\nInformation criteria (AIC, BIC) for non-nested models\nClassification metrics like accuracy, sensitivity, and specificity\n\nLet’s compare models and calculate AIC values:\n\n# Calculate AIC for our models\nmodels &lt;- list(\n  \"Full\" = rodent_model,\n  \"No Distance\" = model_no_distance,\n  \"No Age\" = model_no_age,\n  \"No Shrub\" = model_no_shrub,\n  \"Intercept Only\" = glm(rodent_present ~ 1, \n                        data = fragment_data, \n                        family = binomial)\n)\n\n# Calculate AIC and BIC\nmodel_comparison &lt;- data.frame(\n  Model = names(models),\n  Parameters = sapply(models, function(m) length(coef(m))),\n  AIC = sapply(models, AIC),\n  BIC = sapply(models, BIC),\n  Deviance = sapply(models, function(m) m$deviance)\n)\n\n# Show model comparison table\nmodel_comparison %&gt;%\n  arrange(AIC) %&gt;%\n  mutate(across(where(is.numeric), round, 2)) %&gt;%\n  flextable()\n\nModelParametersAICBICDevianceNo Age317.0520.7111.05Full417.2722.159.27No Distance317.3821.0411.38Intercept Only129.5530.7727.55No Shrub332.7336.3926.73\n\n\n\nWe can also evaluate the predictive performance of our model:\n\n# Get predictions\npredicted_probs &lt;- predict(rodent_model, type = \"response\")\npredicted_class &lt;- ifelse(predicted_probs &gt; 0.5, \"Present\", \"Absent\")\n\n# Create confusion matrix\ntrue_class &lt;- fragment_data$rodent_present\nconf_matrix &lt;- table(Predicted = predicted_class, Actual = true_class)\n\n# Calculate metrics\naccuracy &lt;- sum(diag(conf_matrix)) / sum(conf_matrix)\nsensitivity &lt;- conf_matrix[\"Present\", \"Present\"] / sum(conf_matrix[, \"Present\"])\nspecificity &lt;- conf_matrix[\"Absent\", \"Absent\"] / sum(conf_matrix[, \"Absent\"])\n\n# Display results\nconf_matrix\n\n         Actual\nPredicted Absent Present\n  Absent       5       2\n  Present      1      17\n\ncat(\"\\nAccuracy:\", round(accuracy, 3), \"\\n\")\n\n\nAccuracy: 0.88 \n\ncat(\"Sensitivity:\", round(sensitivity, 3), \"\\n\")\n\nSensitivity: 0.895 \n\ncat(\"Specificity:\", round(specificity, 3), \"\\n\")\n\nSpecificity: 0.833"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#publication-quality-figure-and-scientific-write-up",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#publication-quality-figure-and-scientific-write-up",
    "title": "Lecture 15 - xxxxxx",
    "section": "Publication-Quality Figure and Scientific Write-Up",
    "text": "Publication-Quality Figure and Scientific Write-Up\nLet’s create a publication-quality figure for our multiple logistic regression model and show how we would write up the results for a scientific publication.\n\n# Create a more polished visualization for shrub cover effect\npolished_pred &lt;- predict_for_var(\"shrub_cover\", rodent_model, fragment_data)\n\n# Calculate confidence intervals\npred_se &lt;- predict(rodent_model, \n                  newdata = polished_pred, \n                  type = \"link\", \n                  se.fit = TRUE)\n\n# Convert to data frame with CIs\nci_data &lt;- data.frame(\n  shrub_cover = polished_pred$shrub_cover,\n  fit = pred_se$fit,\n  se = pred_se$se.fit\n)\n\n# Calculate upper and lower bounds of CI on link scale\nci_data$lower_link &lt;- ci_data$fit - 1.96 * ci_data$se\nci_data$upper_link &lt;- ci_data$fit + 1.96 * ci_data$se\n\n# Transform back to probability scale\nci_data$prob &lt;- plogis(ci_data$fit)\nci_data$lower_prob &lt;- plogis(ci_data$lower_link)\nci_data$upper_prob &lt;- plogis(ci_data$upper_link)\n\n# Create plot\nggplot() +\n  # Add jittered points for raw data\n  geom_jitter(data = fragment_data, \n             aes(x = shrub_cover, \n                 y = as.numeric(rodent_present == \"Present\")),\n             width = 0, height = 0.05, alpha = 0.6, size = 3) +\n  # Add fitted probability curve\n  geom_line(data = ci_data, \n           aes(x = shrub_cover, y = prob), \n           color = \"darkblue\", size = 1.2) +\n  # Add confidence intervals\n  geom_ribbon(data = ci_data, \n             aes(x = shrub_cover, \n                 ymin = lower_prob, \n                 ymax = upper_prob), \n             alpha = 0.2, fill = \"darkblue\") +\n  # Customize appearance\n  labs(title = \"Effect of Shrub Cover on Native Rodent Presence\",\n       subtitle = \"Probability of occurrence in canyon fragments\",\n       x = \"Percentage Shrub Cover\",\n       y = \"Probability of Rodent Presence\") +\n  scale_y_continuous(limits = c(0, 1), \n                     breaks = seq(0, 1, 0.2)) +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\"),\n    axis.title = element_text(face = \"bold\"),\n    legend.position = \"none\",\n    panel.grid.minor = element_blank(),\n    panel.border = element_rect(fill = NA, color = \"gray80\")\n  )\n\n\n\n\n\n\n\n\nScientific Write-Up Example\n\n\nResults\nThe presence of native rodents in canyon fragments was modeled using multiple logistic regression with three predictors: distance to nearest source canyon, years since isolation, and percentage of shrub cover. The model was statistically significant (χ² = 12.63, df = 3, p = 0.005) and explained 38.7% of the variation in rodent presence (McFadden’s R² = 0.387).\nAmong the predictors, only shrub cover had a statistically significant effect on rodent presence (β = 0.091, SE = 0.041, p = 0.026). The odds ratio for shrub cover was 1.095 (95% CI: 1.011-1.186), indicating that for each percentage increase in shrub cover, the odds of rodent presence increased by approximately 9.5%. Neither distance to source canyon (β = 0.0002, p = 0.690) nor years since isolation (β = 0.022, p = 0.566) showed significant relationships with rodent presence.\nThe model correctly classified 76% of the fragments, with a sensitivity of 0.77 and a specificity of 0.75. Diagnostics indicated no significant issues with model fit (Hosmer-Lemeshow χ² = 7.31, df = 8, p = 0.504).\nDiscussion\nOur findings suggest that vegetation structure, as measured by shrub cover, plays a crucial role in determining the presence of native rodents in canyon fragments. The positive relationship between shrub cover and rodent occurrence likely reflects the importance of vegetation for providing food resources, shelter from predators, and suitable microhabitat conditions. Contrary to our expectations, isolation metrics (distance to source canyon and years since isolation) did not significantly predict rodent presence, suggesting that local habitat quality may be more important than landscape connectivity for these species."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#relationship-between-glms-and-anovas",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#relationship-between-glms-and-anovas",
    "title": "Lecture 15 - xxxxxx",
    "section": "Relationship Between GLMs and ANOVAs",
    "text": "Relationship Between GLMs and ANOVAs\n\n\n\n\n\n\nGLMs and ANOVAs: The Connection\n\n\nGeneral linear models (including ANOVAs and standard regression) are special cases of Generalized Linear Models where:\n\nThe response variable follows a normal distribution\nThe link function is the identity function\n\nTherefore, a one-way ANOVA is equivalent to: - A linear regression with a categorical predictor - A Gaussian GLM with an identity link and a categorical predictor\n\n\n\nLet’s demonstrate this equivalence:\n\n# 1. Standard ANOVA\nanova_model &lt;- aov(mpg ~ cyl, data = mtcars)\n\n# 2. Linear regression\nlm_model &lt;- lm(mpg ~ cyl, data = mtcars)\n\n# 3. Gaussian GLM\nglm_model &lt;- glm(mpg ~ cyl, family = gaussian(link = \"identity\"), data = mtcars)\n\n# Compare coefficients\ncoef_comparison &lt;- data.frame(\n  Term = names(coef(lm_model)),\n  `Linear Regression` = coef(lm_model),\n  `Gaussian GLM` = coef(glm_model)\n)\n\n# Display the comparison\ncoef_comparison %&gt;%\n  mutate(across(where(is.numeric), round, 3)) %&gt;%\n  flextable()\n\nTermLinear.RegressionGaussian.GLM(Intercept)26.66426.664cyl6-6.921-6.921cyl8-11.564-11.564\n\n# Compare ANOVA tables\nanova_aov &lt;- anova(anova_model)\nanova_lm &lt;- anova(lm_model)\nanova_glm &lt;- anova(glm_model)\n\n# Create visualization showing the three approaches\n# Use the same data and estimated means\nggplot() +\n  # Plot raw data\n  geom_boxplot(data = mtcars, \n              aes(x = cyl, y = mpg, group = cyl),\n              alpha = 0.3, width = 0.5) +\n  geom_jitter(data = mtcars, \n             aes(x = cyl, y = mpg),\n             width = 0.1, alpha = 0.6) +\n  # Add fitted values from each model\n  geom_point(data = emmeans(lm_model, ~cyl) %&gt;% data.frame(),\n            aes(x = cyl, y = emmean), \n            color = \"red\", size = 3, shape = 17) +\n  geom_point(data = emmeans(glm_model, ~cyl) %&gt;% data.frame(),\n            aes(x = cyl, y = emmean), \n            color = \"blue\", size = 3, shape = 15) +\n  # Add legend for model types\n  annotate(\"text\", x = \"8\", y = 30, \n          label = \"Red triangles: Linear Regression\\nBlue squares: Gaussian GLM\", \n          hjust = 1, size = 3.5) +\n  labs(title = \"Comparison of Models: ANOVA, Linear Regression, and Gaussian GLM\",\n       subtitle = \"All three approaches yield identical results\",\n       x = \"Number of Cylinders\",\n       y = \"Miles Per Gallon\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#assumptions-and-diagnostics-summary",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#assumptions-and-diagnostics-summary",
    "title": "Lecture 15 - xxxxxx",
    "section": "Assumptions and Diagnostics Summary",
    "text": "Assumptions and Diagnostics Summary\n\n\nGeneralized Linear Models have different assumptions depending on the specific distribution and link function used:\nAll GLMs: - Independence of observations - Correct specification of the link function - Correct specification of the variance structure - No influential outliers - No multicollinearity among predictors\nGaussian GLMs (including linear regression): - Normality of residuals - Homogeneity of variance\nPoisson GLMs: - Count data (non-negative integers) - Mean equals variance (if overdispersed, consider negative binomial)\nLogistic GLMs: - Binary response variable - Linear relationship between predictors and log odds - Adequate sample size relative to number of parameters\n\nThe following R code checks some common diagnostics for our logistic model:\n\n# Create diagnostic plots for the rodent model\npar(mfrow = c(2, 2))\n\n# 1. Residuals vs fitted\nplot(fitted(rodent_model), residuals(rodent_model, type = \"pearson\"),\n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted Values (predicted probabilities)\",\n     ylab = \"Pearson Residuals\",\n     pch = 16)\nabline(h = 0, lty = 2)\n\n# 2. Leverage\nleverage &lt;- hatvalues(rodent_model)\nplot(leverage, residuals(rodent_model, type = \"pearson\"),\n     main = \"Residuals vs Leverage\",\n     xlab = \"Leverage\",\n     ylab = \"Pearson Residuals\",\n     pch = 16)\nabline(h = 0, lty = 2)\n\n# 3. Cook's distance\ncook &lt;- cooks.distance(rodent_model)\nplot(cook, main = \"Cook's Distance\",\n     ylab = \"Cook's Distance\",\n     pch = 16)\nabline(h = 4/length(cook), lty = 2, col = \"red\")  # Rule of thumb threshold\n\n# 4. Observed vs Predicted probabilities\nplot(predicted_probs, \n     as.numeric(fragment_data$rodent_present) - 1,\n     main = \"Observed vs Predicted\",\n     xlab = \"Predicted Probability\",\n     ylab = \"Observed (0/1)\",\n     pch = 16)\ncurve(I, from = 0, to = 1, add = TRUE, col = \"red\")"
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#summary-and-conclusions",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#summary-and-conclusions",
    "title": "Lecture 15 - xxxxxx",
    "section": "Summary and Conclusions",
    "text": "Summary and Conclusions\nGeneralized Linear Models (GLMs) provide a powerful and flexible framework for analyzing a wide range of data types in biology:\n\nGaussian GLMs with identity link function are equivalent to standard linear models and ANOVAs, suitable for normally distributed continuous responses.\nPoisson GLMs with log link function are appropriate for count data, but be cautious of overdispersion.\nLogistic GLMs with logit link function are useful for binary responses, modeling the probability of success or presence.\n\nKey advantages of GLMs include:\n\nAbility to handle various types of response variables beyond normal distributions\nUnified framework for linear modeling\nFlexibility in specifying the link function to match the data structure\nInterpretable parameters, though interpretation differs by model type\n\nWhen working with GLMs:\n\nChoose the appropriate distribution family based on your response variable\nVerify model assumptions through diagnostic plots\nWatch for overdispersion in count data\nUse odds ratios to interpret logistic regression results\nCompare competing models using likelihood ratio tests and information criteria\n\nThis framework allows biologists to appropriately model many types of data encountered in ecological, behavioral, and physiological research."
  },
  {
    "objectID": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#references",
    "href": "lectures/lecture_15/15_01_lecture_powerpoint_slides.html#references",
    "title": "Lecture 15 - xxxxxx",
    "section": "References",
    "text": "References\nAgresti, A. (1996). An Introduction to Categorical Data Analysis. Wiley, New York.\nBolger, D. T., Alberts, A. C., Sauvajot, R. M., Potenza, P., McCalvin, C., Tran, D., Mazzoni, S., & Soulé, M. E. (1997). Response of rodents to habitat fragmentation in coastal southern California. Ecological Applications, 7(2), 552-563.\nChristensen, R. (1997). Log-linear Models and Logistic Regression. Springer, New York.\nHosmer, D. W., & Lemeshow, S. (1989). Applied Logistic Regression. Wiley, New York.\nMcCullagh, P., & Nelder, J. A. (1989). Generalized Linear Models. Chapman and Hall, London.\nPolis, G. A., Hurd, S. D., Jackson, C. T., & Piñero, F. S. (1998). Multifactor analysis of ecosystem patterns on islands in the Gulf of California. Ecological Monographs, 68, 490-502."
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "",
    "text": "MLR model\nRegression parameters\nAnalysis of variance\nNull hypotheses\nExplained variance\nAssumptions and diagnostics\nCollinearity\nInteractions\nDummy variables\nModel selection\nImportance of predictors"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#multiple-regression",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#multiple-regression",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "",
    "text": "MLR model\nRegression parameters\nAnalysis of variance\nNull hypotheses\nExplained variance\nAssumptions and diagnostics\nCollinearity\nInteractions\nDummy variables\nModel selection\nImportance of predictors"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#anova",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#anova",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance: single and multi-factor designs\n\nExamples: diatoms, circadian rhythms\nPredictor variables: fixed vs. random\nANOVA model\nAnalysis and partitioning of variance\nNull hypothesis\nAssumptions and diagnostics\nPost F Tests - Tukey and others\nReporting the results"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#key-insight",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#key-insight",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Key Insight",
    "text": "Key Insight\nBoth regression and ANOVA:\n\nPartition the total variation in Y\nUse F-tests for significance\nAre based on the General Linear Model\nTest if explanatory variables predict Y ANOVA is fundamentally connected to regression analysis - both are special cases of the General Linear Model.\n\n\n\nModelFormTestsRegressionY = β₀ + β₁X + εH₀: β₁ = 0ANOVAYᵢⱼ = μ + Aᵢ + εᵢⱼH₀: μ₁ = μ₂ = ... = μₖ"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#connection-to-regression",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#connection-to-regression",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Connection to Regression",
    "text": "Connection to Regression\nThis is exactly the same calculation as R² in regression: \\[R^2 = \\frac{SS_{regression}}{SS_{total}}\\]"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#key-anova-principles",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#key-anova-principles",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Key ANOVA Principles",
    "text": "Key ANOVA Principles\n\nPurpose: ANOVA (Analysis of Variance) compares means across multiple groups simultaneously\nConnection to Regression:\n\nBoth are special cases of the General Linear Model\nANOVA with categorical predictors = Regression with dummy variables\nBoth partition variance into explained and unexplained components\n\nThe Analysis of Variance:\n\nPartitions total variation into components\nTests whether differences among groups exceed what would be expected by chance\nUses F-tests to compare variance between groups to variance within groups\n\nSum of Squares Partitioning:\n\nSS(Total) = SS(Between Groups) + SS(Within Groups)\nSame as SS(Total) = SS(Regression) + SS(Error) in regression\n\nFixed vs. Random Effects:\n\nFixed effects: specific groups of interest (most common)\nRandom effects: sampling from a larger population"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#anova-assumptions",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_html.html#anova-assumptions",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "ANOVA Assumptions",
    "text": "ANOVA Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#multiple-regression",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#multiple-regression",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Multiple Regression",
    "text": "Multiple Regression\n\nMLR model\nRegression parameters\nAnalysis of variance\nNull hypotheses\nExplained variance\nAssumptions and diagnostics\nCollinearity\nInteractions\nDummy variables\nModel selection\nImportance of predictors"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#anova",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#anova",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "ANOVA",
    "text": "ANOVA\nAnalysis of variance: single and multi-factor designs\n\nExamples: diatoms, circadian rhythms\nPredictor variables: fixed vs. random\nANOVA model\nAnalysis and partitioning of variance\nNull hypothesis\nAssumptions and diagnostics\nPost F Tests - Tukey and others\nReporting the results"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#key-insight",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#key-insight",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Key Insight",
    "text": "Key Insight\nBoth regression and ANOVA:\n\nPartition the total variation in Y\nUse F-tests for significance\nAre based on the General Linear Model\nTest if explanatory variables predict Y ANOVA is fundamentally connected to regression analysis - both are special cases of the General Linear Model.\n\n\n\nModelFormTestsRegressionY = β₀ + β₁X + εH₀: β₁ = 0ANOVAYᵢⱼ = μ + Aᵢ + εᵢⱼH₀: μ₁ = μ₂ = ... = μₖ"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#connection-to-regression",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#connection-to-regression",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Connection to Regression",
    "text": "Connection to Regression\nThis is exactly the same calculation as R² in regression: \\[R^2 = \\frac{SS_{regression}}{SS_{total}}\\]"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#key-anova-principles",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#key-anova-principles",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "Key ANOVA Principles",
    "text": "Key ANOVA Principles\n\nPurpose: ANOVA (Analysis of Variance) compares means across multiple groups simultaneously\nConnection to Regression:\n\nBoth are special cases of the General Linear Model\nANOVA with categorical predictors = Regression with dummy variables\nBoth partition variance into explained and unexplained components\n\nThe Analysis of Variance:\n\nPartitions total variation into components\nTests whether differences among groups exceed what would be expected by chance\nUses F-tests to compare variance between groups to variance within groups\n\nSum of Squares Partitioning:\n\nSS(Total) = SS(Between Groups) + SS(Within Groups)\nSame as SS(Total) = SS(Regression) + SS(Error) in regression\n\nFixed vs. Random Effects:\n\nFixed effects: specific groups of interest (most common)\nRandom effects: sampling from a larger population"
  },
  {
    "objectID": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#anova-assumptions",
    "href": "lectures/lecture_12/12_01_lecture_powerpoint_slides.html#anova-assumptions",
    "title": "Lecture 12 - Single factor analysis of variance - ANOVA",
    "section": "ANOVA Assumptions",
    "text": "ANOVA Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_html.html",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "",
    "text": "Multifactor ANOVA\n\nExample\nLinear model\nAnalysis of variance\nNull hypotheses\nInteractions and main effects\nUnequal sample size\nAssumptions"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\nWhere:\n\n\\(y_{ijk}\\) is the response variable\n\nvalue of the k-th replicate in j-th level of B in the i-th level of A\n(algal biomass in 3rd quadrat, in 2nd patch in low grazing treatment)\n\n\\(\\mu\\) is the overall mean\n\n(overall average algal biomass)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-1",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-1",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\n\n\\(\\alpha_i\\) is the fixed effect of factor \\(i\\)\n(difference between average biomass in all low grazing level quadrats and overall mean)\n\\(\\beta_{j(i)}\\) is the random effect of factor \\(j\\) nested within factor \\(i\\)\nusually random variable, measuring variance among all possible levels of B within each level of A\n(variance among all possible patches that may have been used in the low grazing treatment)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-2",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-2",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\n\n\\(\\varepsilon_{ijk}\\) is the error term\nαi: is the effect of the ith level of A: µi- µ\nunexplained variance associated with the kth replicate in jth level of B in the ith level of A\n\n(difference bw observed algal biomass in 3rd quadrat in 2nd patch in low grazing treatment and predicted biomass - average biomass in 2nd patch in low grazing treatment)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#conclusions",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_html.html#conclusions",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "Conclusions?",
    "text": "Conclusions?\n“significant variation between replicate patches within each treatment, but no significant difference in amount of filamentous algae between treatments”"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\nWhere:\n\n\\(y_{ijk}\\) is the response variable\n\nvalue of the k-th replicate in j-th level of B in the i-th level of A\n(algal biomass in 3rd quadrat, in 2nd patch in low grazing treatment)\n\n\\(\\mu\\) is the overall mean\n\n(overall average algal biomass)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-1",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-1",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\n\n\\(\\alpha_i\\) is the fixed effect of factor \\(i\\)\n(difference between average biomass in all low grazing level quadrats and overall mean)\n\\(\\beta_{j(i)}\\) is the random effect of factor \\(j\\) nested within factor \\(i\\)\nusually random variable, measuring variance among all possible levels of B within each level of A\n(variance among all possible patches that may have been used in the low grazing treatment)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-2",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#the-linear-model-for-a-nested-design-is-y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk-2",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "The linear model for a nested design is: \\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]\n\n\\(\\varepsilon_{ijk}\\) is the error term\nαi: is the effect of the ith level of A: µi- µ\nunexplained variance associated with the kth replicate in jth level of B in the ith level of A\n\n(difference bw observed algal biomass in 3rd quadrat in 2nd patch in low grazing treatment and predicted biomass - average biomass in 2nd patch in low grazing treatment)"
  },
  {
    "objectID": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#conclusions",
    "href": "lectures/lecture_14/14_01_lecture_powerpoint_slides.html#conclusions",
    "title": "Lecture 14 - Multifactor ANOVA",
    "section": "Conclusions?",
    "text": "Conclusions?\n“significant variation between replicate patches within each treatment, but no significant difference in amount of filamentous algae between treatments”"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "",
    "text": "# Load necessary libraries\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(cowplot)\nlibrary(car)\nlibrary(lme4)\nlibrary(lmerTest)\nlibrary(emmeans)\nlibrary(knitr)\n\nlibrary(viridis)\n\n# Set theme for consistent plotting\ntheme_set(theme_cowplot())\n\n# Custom theme for consistent plotting\nmy_theme &lt;- theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 10),\n    axis.title = element_text(size = 9),\n    axis.text = element_text(size = 8)\n  )"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html#dataset-overview",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html#dataset-overview",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nFirst, let’s load and explore the dataset:\n\n# Read in the andrew dataset\nandrew &lt;- read_csv(\"data/andrew.csv\")\n\n# Convert TREAT to factor with meaningful labels\nandrew$TREAT &lt;- factor(andrew$TREAT, \n                      levels = c(\"con\", \"t0.66\", \"t0.33\", \"rem\"),\n                      labels = c(\"Control\", \"66% Density\", \"33% Density\", \"Removed\"))\n\n# Convert PATCH to factor\nandrew$PATCH &lt;- factor(andrew$PATCH)\n\n# Display the first few rows of the dataset\nhead(andrew)\n\n# A tibble: 6 × 4\n  TREAT   PATCH  QUAD ALGAE\n  &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control 1         1     0\n2 Control 1         2     0\n3 Control 1         3     0\n4 Control 1         4     6\n5 Control 1         5     2\n6 Control 2         1     0\n\n# Calculate summary statistics\nsummary_stats &lt;- andrew %&gt;%\n  group_by(TREAT) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(ALGAE),\n    sd = sd(ALGAE),\n    se = sd / sqrt(n),\n    min = min(ALGAE),\n    max = max(ALGAE)\n  )\n\n# Display summary statistics\nsummary_stats \n\n# A tibble: 4 × 7\n  TREAT           n  mean    sd    se   min   max\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control        20   1.3  3.18 0.711     0    13\n2 66% Density    20  21.6 25.1  5.62      0    79\n3 33% Density    20  19   25.7  5.74      0    71\n4 Removed        20  39.2 28.7  6.41      0    83\n\n\nLet’s visualize the data distribution by treatment:\n\n# Create boxplot of algae cover by treatment\nggplot(andrew, aes(x = TREAT, y = ALGAE, fill = TREAT)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 1) +\n  scale_fill_viridis_d(option = \"D\", end = 0.85) +\n  labs(\n    title = \"Effect of Urchin Density on Filamentous Algae Cover\",\n    x = \"Urchin Density Treatment\",\n    y = \"Filamentous Algae Cover (%)\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html#corrected-nested-anova-with-proper-f-tests",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html#corrected-nested-anova-with-proper-f-tests",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Corrected Nested ANOVA with Proper F-tests",
    "text": "Corrected Nested ANOVA with Proper F-tests\nLet’s calculate the correct F-ratios and p-values for the nested design:\n\n# Extract MS values\nMS_treat &lt;- nested_summary[[1]][\"TREAT      \", \"Mean Sq\"] \nMS_patch &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Mean Sq\"]\nMS_residual &lt;- nested_summary[[1]][\"Residuals\", \"Mean Sq\"]\n\n# Extract df values\ndf_treat &lt;- nested_summary[[1]][\"TREAT      \", \"Df\"]\ndf_patch &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Df\"]\ndf_residual &lt;- nested_summary[[1]][\"Residuals\", \"Df\"]\n\n# Calculate correct F ratios for nested design\nF_treat &lt;- MS_treat / MS_patch\nF_patch &lt;- MS_patch / MS_residual\n\n# Calculate p-values using the correct denominator df\np_treat &lt;- pf(F_treat, df_treat, df_patch, lower.tail = FALSE)\np_patch &lt;- pf(F_patch, df_patch, df_residual, lower.tail = FALSE)\n\n# Create ANOVA table with corrected F-tests\nanova_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (within treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  SS = c(nested_summary[[1]][\"TREAT      \", \"Sum Sq\"], \n         nested_summary[[1]][\"TREAT:PATCH\", \"Sum Sq\"], \n         nested_summary[[1]][\"Residuals\", \"Sum Sq\"]),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(p_treat, p_patch, NA)\n)\n\n# Format p-values\nanova_table$p &lt;- ifelse(anova_table$p &lt; 0.001, \"&lt;0.001\", \n                       ifelse(is.na(anova_table$p), NA, \n                              format(anova_table$p, digits = 3)))\n\n# Display corrected ANOVA table\nanova_table\n\n                      Source df       SS       MS        F        p\n1                  Treatment  3 14429.14 4809.712 2.717102 9.13e-02\n2 Patches (within treatment) 12 21241.95 1770.162 5.928207   &lt;0.001\n3                   Residual 64 19110.40  298.600       NA     &lt;NA&gt;\n\n\nWith the corrected nested ANOVA, we find:\n\nThe treatment effect is not significant (F = 2.72, p = 0.0913) when tested against the patch variation.\nThere is significant variation among patches within treatments (F = 5.93, p &lt; 0.001)\n\nThis is a different conclusion than the one-way ANOVA, which found a significant treatment effect."
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html#visual-decomposition-of-variance-components",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html#visual-decomposition-of-variance-components",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Visual Decomposition of Variance Components",
    "text": "Visual Decomposition of Variance Components\nFirst, let’s create a visual representation of how variance is partitioned in a standard one-way ANOVA, and then contrast it with how a nested ANOVA further divides the variance components.\n\n\n[1] \"Treatment means:\"\n\n\n# A tibble: 4 × 2\n  TREAT       treat_mean\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Control            1.3\n2 66% Density       21.6\n3 33% Density       19  \n4 Removed           39.2\n\n\n[1] \"First few rows of joined patch_means:\"\n\n\n# A tibble: 6 × 4\n  TREAT       PATCH patch_mean treat_mean\n  &lt;fct&gt;       &lt;fct&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Control     1            1.6        1.3\n2 Control     2            0          1.3\n3 Control     3            1          1.3\n4 Control     4            2.6        1.3\n5 66% Density 5           28.4       21.6\n6 66% Density 6           36.8       21.6\n\n\n[1] \"treat_mean column is present in patch_means\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots above visually demonstrate the key differences in how variance is partitioned between one-way and nested ANOVA:\n\nOne-way ANOVA (first plot):\n\nTotal variance is split into just two components: Among Groups (Treatment) and Within Groups (Error)\nThe Within Groups component includes all variation not explained by treatments\n\nNested ANOVA (second plot):\n\nTotal variance is split into three components: Among Treatments, Among Patches within Treatments, and Within Patches (Residual Error)\nThe important addition is the “Among Patches within Treatments” component, which captures the spatial heterogeneity\nThe actual residual error (Within Patches) is smaller than the Within Groups error in one-way ANOVA\n\n\nThis visualization demonstrates why we get different conclusions: in one-way ANOVA, the patch-to-patch variation is incorrectly included in the error term, leading to an artificially inflated F-ratio for treatments."
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html#numerical-decomposition-of-variance",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html#numerical-decomposition-of-variance",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Numerical Decomposition of Variance",
    "text": "Numerical Decomposition of Variance\n\n# Calculate sums of squares for both models\nSS_Total &lt;- sum(nested_summary[[1]][, \"Sum Sq\"])\nSS_Treatment &lt;- nested_summary[[1]][\"TREAT      \", \"Sum Sq\"]\nSS_Patch_within_Treatment &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Sum Sq\"]\nSS_Error_Nested &lt;- nested_summary[[1]][\"Residuals\", \"Sum Sq\"]\nSS_Error_OneWay &lt;- oneway_summary[[1]][\"Residuals\", \"Sum Sq\"]\n\n# Calculate percentages for visualization\npercent_treatment_oneway &lt;- (SS_Treatment / SS_Total) * 100\npercent_error_oneway &lt;- (SS_Error_OneWay / SS_Total) * 100\n\npercent_treatment_nested &lt;- (SS_Treatment / SS_Total) * 100\npercent_patch_nested &lt;- (SS_Patch_within_Treatment / SS_Total) * 100\npercent_error_nested &lt;- (SS_Error_Nested / SS_Total) * 100\n\n# Create data frame for visualization\nss_comparison &lt;- data.frame(\n  Model = c(\n    rep(\"One-way ANOVA\", 2), \n    rep(\"Nested ANOVA\", 3)\n  ),\n  Source = c(\n    \"Treatment\", \"Error (within)\",\n    \"Treatment\", \"Patch(Treatment)\", \"Error\"\n  ),\n  Percent = c(\n    percent_treatment_oneway, percent_error_oneway,\n    percent_treatment_nested, percent_patch_nested, percent_error_nested\n  ),\n  SS = c(\n    SS_Treatment, SS_Error_OneWay,\n    SS_Treatment, SS_Patch_within_Treatment, SS_Error_Nested\n  )\n)\n\n# Add factor levels for ordering\nss_comparison$Source &lt;- factor(\n  ss_comparison$Source,\n  levels = c(\"Treatment\", \"Patch(Treatment)\", \"Error\", \"Error (within)\")\n)\n\n# Create colors for the sources\nsource_colors &lt;- c(\n  \"Treatment\" = \"#1b9e77\", \n  \"Patch(Treatment)\" = \"#d95f02\", \n  \"Error\" = \"#7570b3\",\n  \"Error (within)\" = \"#7570b3\"\n)\n\n# Create the stacked bar plot\np1 &lt;- ggplot(ss_comparison, aes(x = Model, y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", position = \"stack\", color = \"black\") +\n  scale_fill_manual(values = source_colors) +\n  labs(\n    title = \"Comparison of Variance Partitioning\",\n    subtitle = \"One-way vs. Nested ANOVA\",\n    x = \"\",\n    y = \"Percentage of Total Variance\",\n    fill = \"Source of Variation\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percent)),\n    position = position_stack(vjust = 0.5),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"right\",\n    axis.text.x = element_text(face = \"bold\", size = 12)\n  )\n\n# Create a pie chart version for one-way ANOVA\noneway_data &lt;- ss_comparison %&gt;% filter(Model == \"One-way ANOVA\")\noneway_data$ypos &lt;- cumsum(oneway_data$Percent) - 0.5 * oneway_data$Percent\n\np2 &lt;- ggplot(oneway_data, aes(x = \"\", y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\") +\n  coord_polar(\"y\", start = 0) +\n  labs(\n    title = \"One-way ANOVA\",\n    subtitle = \"Variance Components\",\n    fill = \"Source of Variation\"\n  ) +\n  scale_fill_manual(values = source_colors) +\n  geom_text(\n    aes(y = ypos, label = sprintf(\"%.1f%%\", Percent)),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n# Create a pie chart version for nested ANOVA\nnested_data &lt;- ss_comparison %&gt;% filter(Model == \"Nested ANOVA\")\nnested_data$ypos &lt;- cumsum(nested_data$Percent) - 0.5 * nested_data$Percent\n\np3 &lt;- ggplot(nested_data, aes(x = \"\", y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\") +\n  coord_polar(\"y\", start = 0) +\n  labs(\n    title = \"Nested ANOVA\",\n    subtitle = \"Variance Components\",\n    fill = \"Source of Variation\"\n  ) +\n  scale_fill_manual(values = source_colors) +\n  geom_text(\n    aes(y = ypos, label = sprintf(\"%.1f%%\", Percent)),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n# Display all plots\np1\n\n\n\n\n\n\n\n\n\n# Combine the pie charts\np2 + p3 + plot_layout(ncol = 2)"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anova_visual_html.html#alternative-code-for-mixed-models",
    "href": "lectures/lecture_14/14_04_nested_anova_visual_html.html#alternative-code-for-mixed-models",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Alternative Code for Mixed Models",
    "text": "Alternative Code for Mixed Models\nFor advanced users, we could also fit this as a mixed model with lmerTest:\n\nlibrary(lmerTest)\n\n# Mixed model with patches nested within treatments\nmixed_model_alt &lt;- lmer(ALGAE ~ TREAT + (1|TREAT:PATCH), data = andrew)\nanova(mixed_model_alt, type = 3, ddf = \"Satterthwaite\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# We could also try a simpler random effects structure\nsimple_mixed_model &lt;- lmer(ALGAE ~ TREAT + (1|PATCH), data = andrew)\nanova(simple_mixed_model, type = 3, ddf = \"Satterthwaite\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMixed models provide a more flexible approach to handle nested designs and are the recommended approach in modern statistical practice."
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#dataset-overview",
    "href": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#dataset-overview",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Dataset Overview",
    "text": "Dataset Overview\nFirst, let’s load and explore the dataset:\n\n# Read in the andrew dataset\nandrew &lt;- read_csv(\"data/andrew.csv\")\n\n# Convert TREAT to factor with meaningful labels\nandrew$TREAT &lt;- factor(andrew$TREAT, \n                      levels = c(\"con\", \"t0.66\", \"t0.33\", \"rem\"),\n                      labels = c(\"Control\", \"66% Density\", \"33% Density\", \"Removed\"))\n\n# Convert PATCH to factor\nandrew$PATCH &lt;- factor(andrew$PATCH)\n\n# Display the first few rows of the dataset\nhead(andrew)\n\n# A tibble: 6 × 4\n  TREAT   PATCH  QUAD ALGAE\n  &lt;fct&gt;   &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control 1         1     0\n2 Control 1         2     0\n3 Control 1         3     0\n4 Control 1         4     6\n5 Control 1         5     2\n6 Control 2         1     0\n\n# Calculate summary statistics\nsummary_stats &lt;- andrew %&gt;%\n  group_by(TREAT) %&gt;%\n  summarise(\n    n = n(),\n    mean = mean(ALGAE),\n    sd = sd(ALGAE),\n    se = sd / sqrt(n),\n    min = min(ALGAE),\n    max = max(ALGAE)\n  )\n\n# Display summary statistics\nsummary_stats \n\n# A tibble: 4 × 7\n  TREAT           n  mean    sd    se   min   max\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control        20   1.3  3.18 0.711     0    13\n2 66% Density    20  21.6 25.1  5.62      0    79\n3 33% Density    20  19   25.7  5.74      0    71\n4 Removed        20  39.2 28.7  6.41      0    83\n\n\nLet’s visualize the data distribution by treatment:\n\n# Create boxplot of algae cover by treatment\nggplot(andrew, aes(x = TREAT, y = ALGAE, fill = TREAT)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 1) +\n  scale_fill_viridis_d(option = \"D\", end = 0.85) +\n  labs(\n    title = \"Effect of Urchin Density on Filamentous Algae Cover\",\n    x = \"Urchin Density Treatment\",\n    y = \"Filamentous Algae Cover (%)\"\n  ) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#corrected-nested-anova-with-proper-f-tests",
    "href": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#corrected-nested-anova-with-proper-f-tests",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Corrected Nested ANOVA with Proper F-tests",
    "text": "Corrected Nested ANOVA with Proper F-tests\nLet’s calculate the correct F-ratios and p-values for the nested design:\n\n# Extract MS values\nMS_treat &lt;- nested_summary[[1]][\"TREAT      \", \"Mean Sq\"] \nMS_patch &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Mean Sq\"]\nMS_residual &lt;- nested_summary[[1]][\"Residuals\", \"Mean Sq\"]\n\n# Extract df values\ndf_treat &lt;- nested_summary[[1]][\"TREAT      \", \"Df\"]\ndf_patch &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Df\"]\ndf_residual &lt;- nested_summary[[1]][\"Residuals\", \"Df\"]\n\n# Calculate correct F ratios for nested design\nF_treat &lt;- MS_treat / MS_patch\nF_patch &lt;- MS_patch / MS_residual\n\n# Calculate p-values using the correct denominator df\np_treat &lt;- pf(F_treat, df_treat, df_patch, lower.tail = FALSE)\np_patch &lt;- pf(F_patch, df_patch, df_residual, lower.tail = FALSE)\n\n# Create ANOVA table with corrected F-tests\nanova_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (within treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  SS = c(nested_summary[[1]][\"TREAT      \", \"Sum Sq\"], \n         nested_summary[[1]][\"TREAT:PATCH\", \"Sum Sq\"], \n         nested_summary[[1]][\"Residuals\", \"Sum Sq\"]),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(p_treat, p_patch, NA)\n)\n\n# Format p-values\nanova_table$p &lt;- ifelse(anova_table$p &lt; 0.001, \"&lt;0.001\", \n                       ifelse(is.na(anova_table$p), NA, \n                              format(anova_table$p, digits = 3)))\n\n# Display corrected ANOVA table\nanova_table\n\n                      Source df       SS       MS        F        p\n1                  Treatment  3 14429.14 4809.712 2.717102 9.13e-02\n2 Patches (within treatment) 12 21241.95 1770.162 5.928207   &lt;0.001\n3                   Residual 64 19110.40  298.600       NA     &lt;NA&gt;\n\n\nWith the corrected nested ANOVA, we find:\n\nThe treatment effect is not significant (F = 2.72, p = 0.0913) when tested against the patch variation.\nThere is significant variation among patches within treatments (F = 5.93, p &lt; 0.001)\n\nThis is a different conclusion than the one-way ANOVA, which found a significant treatment effect."
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#visual-decomposition-of-variance-components",
    "href": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#visual-decomposition-of-variance-components",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Visual Decomposition of Variance Components",
    "text": "Visual Decomposition of Variance Components\nFirst, let’s create a visual representation of how variance is partitioned in a standard one-way ANOVA, and then contrast it with how a nested ANOVA further divides the variance components.\n\n\n[1] \"Treatment means:\"\n\n\n# A tibble: 4 × 2\n  TREAT       treat_mean\n  &lt;fct&gt;            &lt;dbl&gt;\n1 Control            1.3\n2 66% Density       21.6\n3 33% Density       19  \n4 Removed           39.2\n\n\n[1] \"First few rows of joined patch_means:\"\n\n\n# A tibble: 6 × 4\n  TREAT       PATCH patch_mean treat_mean\n  &lt;fct&gt;       &lt;fct&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 Control     1            1.6        1.3\n2 Control     2            0          1.3\n3 Control     3            1          1.3\n4 Control     4            2.6        1.3\n5 66% Density 5           28.4       21.6\n6 66% Density 6           36.8       21.6\n\n\n[1] \"treat_mean column is present in patch_means\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plots above visually demonstrate the key differences in how variance is partitioned between one-way and nested ANOVA:\n\nOne-way ANOVA (first plot):\n\nTotal variance is split into just two components: Among Groups (Treatment) and Within Groups (Error)\nThe Within Groups component includes all variation not explained by treatments\n\nNested ANOVA (second plot):\n\nTotal variance is split into three components: Among Treatments, Among Patches within Treatments, and Within Patches (Residual Error)\nThe important addition is the “Among Patches within Treatments” component, which captures the spatial heterogeneity\nThe actual residual error (Within Patches) is smaller than the Within Groups error in one-way ANOVA\n\n\nThis visualization demonstrates why we get different conclusions: in one-way ANOVA, the patch-to-patch variation is incorrectly included in the error term, leading to an artificially inflated F-ratio for treatments."
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#numerical-decomposition-of-variance",
    "href": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#numerical-decomposition-of-variance",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Numerical Decomposition of Variance",
    "text": "Numerical Decomposition of Variance\n\n# Calculate sums of squares for both models\nSS_Total &lt;- sum(nested_summary[[1]][, \"Sum Sq\"])\nSS_Treatment &lt;- nested_summary[[1]][\"TREAT      \", \"Sum Sq\"]\nSS_Patch_within_Treatment &lt;- nested_summary[[1]][\"TREAT:PATCH\", \"Sum Sq\"]\nSS_Error_Nested &lt;- nested_summary[[1]][\"Residuals\", \"Sum Sq\"]\nSS_Error_OneWay &lt;- oneway_summary[[1]][\"Residuals\", \"Sum Sq\"]\n\n# Calculate percentages for visualization\npercent_treatment_oneway &lt;- (SS_Treatment / SS_Total) * 100\npercent_error_oneway &lt;- (SS_Error_OneWay / SS_Total) * 100\n\npercent_treatment_nested &lt;- (SS_Treatment / SS_Total) * 100\npercent_patch_nested &lt;- (SS_Patch_within_Treatment / SS_Total) * 100\npercent_error_nested &lt;- (SS_Error_Nested / SS_Total) * 100\n\n# Create data frame for visualization\nss_comparison &lt;- data.frame(\n  Model = c(\n    rep(\"One-way ANOVA\", 2), \n    rep(\"Nested ANOVA\", 3)\n  ),\n  Source = c(\n    \"Treatment\", \"Error (within)\",\n    \"Treatment\", \"Patch(Treatment)\", \"Error\"\n  ),\n  Percent = c(\n    percent_treatment_oneway, percent_error_oneway,\n    percent_treatment_nested, percent_patch_nested, percent_error_nested\n  ),\n  SS = c(\n    SS_Treatment, SS_Error_OneWay,\n    SS_Treatment, SS_Patch_within_Treatment, SS_Error_Nested\n  )\n)\n\n# Add factor levels for ordering\nss_comparison$Source &lt;- factor(\n  ss_comparison$Source,\n  levels = c(\"Treatment\", \"Patch(Treatment)\", \"Error\", \"Error (within)\")\n)\n\n# Create colors for the sources\nsource_colors &lt;- c(\n  \"Treatment\" = \"#1b9e77\", \n  \"Patch(Treatment)\" = \"#d95f02\", \n  \"Error\" = \"#7570b3\",\n  \"Error (within)\" = \"#7570b3\"\n)\n\n# Create the stacked bar plot\np1 &lt;- ggplot(ss_comparison, aes(x = Model, y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", position = \"stack\", color = \"black\") +\n  scale_fill_manual(values = source_colors) +\n  labs(\n    title = \"Comparison of Variance Partitioning\",\n    subtitle = \"One-way vs. Nested ANOVA\",\n    x = \"\",\n    y = \"Percentage of Total Variance\",\n    fill = \"Source of Variation\"\n  ) +\n  geom_text(\n    aes(label = sprintf(\"%.1f%%\", Percent)),\n    position = position_stack(vjust = 0.5),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    legend.position = \"right\",\n    axis.text.x = element_text(face = \"bold\", size = 12)\n  )\n\n# Create a pie chart version for one-way ANOVA\noneway_data &lt;- ss_comparison %&gt;% filter(Model == \"One-way ANOVA\")\noneway_data$ypos &lt;- cumsum(oneway_data$Percent) - 0.5 * oneway_data$Percent\n\np2 &lt;- ggplot(oneway_data, aes(x = \"\", y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\") +\n  coord_polar(\"y\", start = 0) +\n  labs(\n    title = \"One-way ANOVA\",\n    subtitle = \"Variance Components\",\n    fill = \"Source of Variation\"\n  ) +\n  scale_fill_manual(values = source_colors) +\n  geom_text(\n    aes(y = ypos, label = sprintf(\"%.1f%%\", Percent)),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n# Create a pie chart version for nested ANOVA\nnested_data &lt;- ss_comparison %&gt;% filter(Model == \"Nested ANOVA\")\nnested_data$ypos &lt;- cumsum(nested_data$Percent) - 0.5 * nested_data$Percent\n\np3 &lt;- ggplot(nested_data, aes(x = \"\", y = Percent, fill = Source)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"black\") +\n  coord_polar(\"y\", start = 0) +\n  labs(\n    title = \"Nested ANOVA\",\n    subtitle = \"Variance Components\",\n    fill = \"Source of Variation\"\n  ) +\n  scale_fill_manual(values = source_colors) +\n  geom_text(\n    aes(y = ypos, label = sprintf(\"%.1f%%\", Percent)),\n    color = \"white\",\n    fontface = \"bold\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14, hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  )\n\n# Display all plots\np1\n\n\n\n\n\n\n\n\n\n# Combine the pie charts\np2 + p3 + plot_layout(ncol = 2)"
  },
  {
    "objectID": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#alternative-code-for-mixed-models",
    "href": "lectures/lecture_14/14_04_nested_anovat_visual_slides.html#alternative-code-for-mixed-models",
    "title": "Lecture 14 - NESTED ANOVA Visualization",
    "section": "Alternative Code for Mixed Models",
    "text": "Alternative Code for Mixed Models\nFor advanced users, we could also fit this as a mixed model with lmerTest:\n\nlibrary(lmerTest)\n\n# Mixed model with patches nested within treatments\nmixed_model_alt &lt;- lmer(ALGAE ~ TREAT + (1|TREAT:PATCH), data = andrew)\nanova(mixed_model_alt, type = 3, ddf = \"Satterthwaite\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# We could also try a simpler random effects structure\nsimple_mixed_model &lt;- lmer(ALGAE ~ TREAT + (1|PATCH), data = andrew)\nanova(simple_mixed_model, type = 3, ddf = \"Satterthwaite\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nMixed models provide a more flexible approach to handle nested designs and are the recommended approach in modern statistical practice."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Semester - Fall 2025\nInstructor: Bill Perry\n\n\n\n\nWhen: Monday and Wednesday 14:00 - 15:50\nEmail: wlperry@d.umn.edu\n\n\nWhere: We will see\nAssistant: someone please\n\n\nHow: Mostly working through the ideas in the PowerPoint slides using code some of which is prewritten\nAssistant email: someone@d.umn.edu\n\n\nRequired materials: The textbook and a laptop and tenacity\nOffice hours: TBD and by appointment or walk in\n\n\n\n\n\nWelcome to Ecological Statistics! This is a practical course that will introduce you to the topics of experimental design, hypothesis testing, data analysis and visualization in the ecological sciences context. The goal of the course is to enable you to interpret statistical methods and results in the published literature, carry out your own data analyses and conduct a productive conversation with a statistician. We will learn about common statistical approaches in ecology and use the R statistical computing environment to get practice implementing these approaches. Additionally, this course will provide opportunities for students to practice scientific writing skills.\n\n\n\nLearn some useful data skills nd organizaiton and statistical methods and R skills.\n\n\n\nThe course will be taught in a combined lecture and computer lab format with mostly hands on activities assuming you have read the materials. Some of our meetings will take a primarily lecture and discussion format, others a primarily computer lab format, but most will have elements of both. Lecture portions will be dedicated to taking up homework questions, going over course concepts and discussing examples. Computer laboratory elements will be dedicated to using the R statistical computing package for data analysis and visualization.\nMaterials such as lecture slides, computer labs, homework, and assignments will be distributed through the course Canvas site or on the website. The course will mostly be based on the textbook The Analysis of Biological Data (3rd Edition) Experimental Design and Data Analysis for Biologists (1st edition) by Michael C. Whitlock and Dolph Schluter. The text is required and we can talk about how to get it. Other textbooks and resources that you may find of use are:\n\nThe R Book (1st or 2nd edition) by MJ Crawley. Highly recommended as an all-around reference book.\nA Primer of Ecological Statistics (1st or 2nd edition) by NJ Gotelli and AM Ellison.\nBiostatistical Analysis (4th or 5th edition) by JH Zar.\nHandbook of Biological Statistics by JH McDonald (online at http://www.biostathandbook.com/).\nInstitute for Digital Research and Education (https://stats.idre.ucla.edu/other/dae/).\nA Compendium of Clean Graphs in R (http://shinyapps.org/apps/RGraphCompendium/index.php)\nNumerical Ecology with R by D Borcard, F Gillet and P Legendre. A practical introduction to common multivariate methods in ecology.\nOrdination Methods for Ecologists by Mike Palmer (https://ordination.okstate.edu/)\n\n\n\n\nYour grade will be based on homework, in-class assignments, and take-home assignments:\nI will be making this out of points as we get there\n\n\n\nHomework\n10%\n\n\nIn-class assignments\n20%\n\n\nTake-home assignments (4 assignments)\n70%\n\n\n\n\n\n\nyou are required to read the class readings (see ‘course schedule’) ahead of each class. Homework questions based on the readings will also need to be completed ahead of many classes. Homework questions will be assigned one class meeting before they are due and will need to be submitted before the start of class. We will spend the portions class taking up and discussing homework questions. I will cold-call students to present and explain their answers to the rest of the class. Homework assignments will receive one of four possible grades: 100% (A) for work that meets or exceeds expectations, 85% (B) for work that meets most expectations, 65% (D) for work that misses most expectations or 0% (F) for work deemed unacceptable.\n\n\n\nmost computer lab exercises will be accompanied by in-class questions and disucssion. You will submit answers to these questions and your functional R code for the exercise by the end of the day of each exercise. In-class assignments will be submitted as QUARTO files. In-class assignments will be graded on the same scale as homework.\nTake-home assignments: four assignments will be completed outside of class. Each assignment will involve the independent analysis, presentation and interpretation of a dataset. The written report will include 3 sections: abstract, statistical materials and methods and results (statistical results and figures/ tables), prepared to “publication quality” standards. Assignments will be graded out of 100% and assessed on metrics including statistical literacy (performing analyses correctly), graphical presentation of data, adherence to correct statistical reporting norms, grammar and the quality/functionality of the accompanying R code. The assignments will comprise 70% of your final grade; A1 will be worth 10% of the final grade, A2 15%, A3 20%, and A4 25%.\n\n\n\nwill be assigned on a straight 10% scale, with 90-100% receiving some form of A, 80-89% some form of B, etc. + and – grades will fall on the upper (&gt;X7) and lower (&lt;X4) end of the ranges, respectively. Percentage grades below 60% are equivalent to an ‘F’ letter grade.\n\n\n\n10% of total possible grade will be deducted per day late in absence of valid excuse. A grade of 0 will be given for assignments that are more than 3 days late.\n\n\n\nValid excuses for missed class or late work consist of subpoenas, jury duty, military duty, religious observances, illness, bereavement for immediate family and NCAA varsity intercollegiate athletics. Conflicts with work, vacations, weddings, travel, or some other private situation that was foreseen will not be accommodated.  For further information on excused absences see https://evcaa.d.umn.edu/excused-absences\n\n\n\nI take plagiarism and academic dishonesty very seriously and will invoke the full weight of UMD-approved sanctions at the first instance of plagiarism. I am happy to answer questions on what is considered a violation of academic integrity in this class. Please also refer to UMD’s academic integrity policy at: https://evcaa.d.umn.edu/student-academic-integrity\n\n\n\nI will enforce, and expect you to follow the University’s Student Code of Conduct. Appropriate classroom conduct promotes an environment of academic achievement and integrity. Disruptive classroom behaviour that substantially or repeatedly interrupts either the instructor’s ability to teach, or student learning, is prohibited. Disruptive behaviour includes inappropriate use of technology in the classroom. Examples include ringing cell phones, text-messaging, watching videos of funny cats (and other videos), playing computer games, doing email, or surfing the Internet on your computer instead of note-taking or other instructor-sanctioned activities. See more here: https://regents.umn.edu/sites/regents.umn.edu/files/2022-07/policy_student_conduct_code.pdf\n\n\n\nIndividuals who have any disability, either permanent or temporary, which might affect their ability to perform in this course are encouraged to inform the instructor at the start of the quarter. Methods, materials or testing may be modified to provide for equitable participation.\n\n\n\nThe University of Minnesota is committed to the policy that all of its students shall have equal educational opportunities. The University expressly forbids discrimination on the basis of race, color, gender, sexual orientation, disability, veteran’s status, ethnicity, religion, creed, national origin or marital status. If you believe that your Ecology instructor has not followed this policy, you are invited to bring this to the attention of the Biology Department Head (207 Swenson Science Building; 218-726-8123). Your conference will be kept confidential.\nYou may review other relevant UMD policy statements at: https://evcaa.d.umn.edu/recommended-syllabi-policy-statements"
  },
  {
    "objectID": "syllabus.html#aim-and-scope",
    "href": "syllabus.html#aim-and-scope",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Ecological Statistics! This is a practical course that will introduce you to the topics of experimental design, hypothesis testing, data analysis and visualization in the ecological sciences context. The goal of the course is to enable you to interpret statistical methods and results in the published literature, carry out your own data analyses and conduct a productive conversation with a statistician. We will learn about common statistical approaches in ecology and use the R statistical computing environment to get practice implementing these approaches. Additionally, this course will provide opportunities for students to practice scientific writing skills."
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "Syllabus",
    "section": "",
    "text": "Learn some useful data skills nd organizaiton and statistical methods and R skills."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Syllabus",
    "section": "",
    "text": "The course will be taught in a combined lecture and computer lab format with mostly hands on activities assuming you have read the materials. Some of our meetings will take a primarily lecture and discussion format, others a primarily computer lab format, but most will have elements of both. Lecture portions will be dedicated to taking up homework questions, going over course concepts and discussing examples. Computer laboratory elements will be dedicated to using the R statistical computing package for data analysis and visualization.\nMaterials such as lecture slides, computer labs, homework, and assignments will be distributed through the course Canvas site or on the website. The course will mostly be based on the textbook The Analysis of Biological Data (3rd Edition) Experimental Design and Data Analysis for Biologists (1st edition) by Michael C. Whitlock and Dolph Schluter. The text is required and we can talk about how to get it. Other textbooks and resources that you may find of use are:\n\nThe R Book (1st or 2nd edition) by MJ Crawley. Highly recommended as an all-around reference book.\nA Primer of Ecological Statistics (1st or 2nd edition) by NJ Gotelli and AM Ellison.\nBiostatistical Analysis (4th or 5th edition) by JH Zar.\nHandbook of Biological Statistics by JH McDonald (online at http://www.biostathandbook.com/).\nInstitute for Digital Research and Education (https://stats.idre.ucla.edu/other/dae/).\nA Compendium of Clean Graphs in R (http://shinyapps.org/apps/RGraphCompendium/index.php)\nNumerical Ecology with R by D Borcard, F Gillet and P Legendre. A practical introduction to common multivariate methods in ecology.\nOrdination Methods for Ecologists by Mike Palmer (https://ordination.okstate.edu/)"
  },
  {
    "objectID": "syllabus.html#evaluation-scheme",
    "href": "syllabus.html#evaluation-scheme",
    "title": "Syllabus",
    "section": "",
    "text": "Your grade will be based on homework, in-class assignments, and take-home assignments:\nI will be making this out of points as we get there\n\n\n\nHomework\n10%\n\n\nIn-class assignments\n20%\n\n\nTake-home assignments (4 assignments)\n70%"
  },
  {
    "objectID": "syllabus.html#homework",
    "href": "syllabus.html#homework",
    "title": "Syllabus",
    "section": "",
    "text": "you are required to read the class readings (see ‘course schedule’) ahead of each class. Homework questions based on the readings will also need to be completed ahead of many classes. Homework questions will be assigned one class meeting before they are due and will need to be submitted before the start of class. We will spend the portions class taking up and discussing homework questions. I will cold-call students to present and explain their answers to the rest of the class. Homework assignments will receive one of four possible grades: 100% (A) for work that meets or exceeds expectations, 85% (B) for work that meets most expectations, 65% (D) for work that misses most expectations or 0% (F) for work deemed unacceptable."
  },
  {
    "objectID": "syllabus.html#in-class-assignments",
    "href": "syllabus.html#in-class-assignments",
    "title": "Syllabus",
    "section": "",
    "text": "most computer lab exercises will be accompanied by in-class questions and disucssion. You will submit answers to these questions and your functional R code for the exercise by the end of the day of each exercise. In-class assignments will be submitted as QUARTO files. In-class assignments will be graded on the same scale as homework.\nTake-home assignments: four assignments will be completed outside of class. Each assignment will involve the independent analysis, presentation and interpretation of a dataset. The written report will include 3 sections: abstract, statistical materials and methods and results (statistical results and figures/ tables), prepared to “publication quality” standards. Assignments will be graded out of 100% and assessed on metrics including statistical literacy (performing analyses correctly), graphical presentation of data, adherence to correct statistical reporting norms, grammar and the quality/functionality of the accompanying R code. The assignments will comprise 70% of your final grade; A1 will be worth 10% of the final grade, A2 15%, A3 20%, and A4 25%."
  },
  {
    "objectID": "syllabus.html#final-letter-grades",
    "href": "syllabus.html#final-letter-grades",
    "title": "Syllabus",
    "section": "",
    "text": "will be assigned on a straight 10% scale, with 90-100% receiving some form of A, 80-89% some form of B, etc. + and – grades will fall on the upper (&gt;X7) and lower (&lt;X4) end of the ranges, respectively. Percentage grades below 60% are equivalent to an ‘F’ letter grade."
  },
  {
    "objectID": "syllabus.html#latemissed-assignment-policy",
    "href": "syllabus.html#latemissed-assignment-policy",
    "title": "Syllabus",
    "section": "",
    "text": "10% of total possible grade will be deducted per day late in absence of valid excuse. A grade of 0 will be given for assignments that are more than 3 days late."
  },
  {
    "objectID": "syllabus.html#valid-excuses",
    "href": "syllabus.html#valid-excuses",
    "title": "Syllabus",
    "section": "",
    "text": "Valid excuses for missed class or late work consist of subpoenas, jury duty, military duty, religious observances, illness, bereavement for immediate family and NCAA varsity intercollegiate athletics. Conflicts with work, vacations, weddings, travel, or some other private situation that was foreseen will not be accommodated.  For further information on excused absences see https://evcaa.d.umn.edu/excused-absences"
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "",
    "text": "I take plagiarism and academic dishonesty very seriously and will invoke the full weight of UMD-approved sanctions at the first instance of plagiarism. I am happy to answer questions on what is considered a violation of academic integrity in this class. Please also refer to UMD’s academic integrity policy at: https://evcaa.d.umn.edu/student-academic-integrity"
  },
  {
    "objectID": "syllabus.html#student-conduct-code",
    "href": "syllabus.html#student-conduct-code",
    "title": "Syllabus",
    "section": "",
    "text": "I will enforce, and expect you to follow the University’s Student Code of Conduct. Appropriate classroom conduct promotes an environment of academic achievement and integrity. Disruptive classroom behaviour that substantially or repeatedly interrupts either the instructor’s ability to teach, or student learning, is prohibited. Disruptive behaviour includes inappropriate use of technology in the classroom. Examples include ringing cell phones, text-messaging, watching videos of funny cats (and other videos), playing computer games, doing email, or surfing the Internet on your computer instead of note-taking or other instructor-sanctioned activities. See more here: https://regents.umn.edu/sites/regents.umn.edu/files/2022-07/policy_student_conduct_code.pdf"
  },
  {
    "objectID": "syllabus.html#access-for-students-with-disabilities",
    "href": "syllabus.html#access-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "",
    "text": "Individuals who have any disability, either permanent or temporary, which might affect their ability to perform in this course are encouraged to inform the instructor at the start of the quarter. Methods, materials or testing may be modified to provide for equitable participation."
  },
  {
    "objectID": "syllabus.html#promotion-of-bias-free-instruction",
    "href": "syllabus.html#promotion-of-bias-free-instruction",
    "title": "Syllabus",
    "section": "",
    "text": "The University of Minnesota is committed to the policy that all of its students shall have equal educational opportunities. The University expressly forbids discrimination on the basis of race, color, gender, sexual orientation, disability, veteran’s status, ethnicity, religion, creed, national origin or marital status. If you believe that your Ecology instructor has not followed this policy, you are invited to bring this to the attention of the Biology Department Head (207 Swenson Science Building; 218-726-8123). Your conference will be kept confidential.\nYou may review other relevant UMD policy statements at: https://evcaa.d.umn.edu/recommended-syllabi-policy-statements"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Instructor",
    "section": "",
    "text": "Department of Biology\nUniversity of Minnesota Duluth\nI am a professor specializing in aquatic biology, biogeochemistry, invasion biology, and biostatistics and data analysis. My research focuses on statistical methods for aquatic ecology but am open to any data and science. I am particularily adept at working wtih high frequency data from field monitors."
  },
  {
    "objectID": "about.html#bill-perry",
    "href": "about.html#bill-perry",
    "title": "About the Instructor",
    "section": "",
    "text": "Department of Biology\nUniversity of Minnesota Duluth\nI am a professor specializing in aquatic biology, biogeochemistry, invasion biology, and biostatistics and data analysis. My research focuses on statistical methods for aquatic ecology but am open to any data and science. I am particularily adept at working wtih high frequency data from field monitors."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About the Instructor",
    "section": "Research Interests",
    "text": "Research Interests\n\nAquatic Ecology - lakes and streams\nInvasion Biology\nEnvironmental Monitoring\nStatistical methods for ecological data\nData visualization"
  },
  {
    "objectID": "about.html#teaching-philosophy",
    "href": "about.html#teaching-philosophy",
    "title": "About the Instructor",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nMy goal is to make statistics accessible and relevant by doing code and data collection in the classroom. The hands-on approach to learning, using real-world data and practical examples to illustrate statistical concepts is essential and actively doing the work. I use of modern tools like R and the tidyverse to prepare students for careers in biological research."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About the Instructor",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, University of Notre Dame\nMS in Biology, University of Cincinnati\nBS in Biology, University of Cincinnati"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About the Instructor",
    "section": "Contact Information",
    "text": "Contact Information\nFeel free to contact me with any questions about the course or research opportunities:\n\nEmail: wlperry@d.umn.edu\nOffice: TBD\nOffice Hours: by appointment"
  },
  {
    "objectID": "test_overviews/Common_Statistical_Tests.html",
    "href": "test_overviews/Common_Statistical_Tests.html",
    "title": "Common Statistical Tests",
    "section": "",
    "text": "This is a list of the common statistical tests we have used in class\nI hope this serves as a resource you can use in the future when you are doing data analysis.\nI have provided the common tests we have done with a lot of detail that will help you remember how to approach these questions and how to run the tests.\nMy hope is to organize them into:\n\ntypes of data used for the test\nassumptions of the test\nhypotheses tested\nexample data\nmechanics of the test\nexplicit tests of the assumptions\nfollow up tests if appropriate\nexample graphs and associated code\nhow to write up the references section\n\n\n\n\nStatistical Test\nwebpage\ncode\n\n\n\n\nTwo Sample Test\n\nr_code_only\n\n\nParametric Two Sample TTest\nweb\ncode\n\n\nNonParametric Welches Two Sample Test\nweb\ncode\n\n\nNonParametric Permutation Two Sample Test\nweb\ncode\n\n\nNonParametric Mann Whitney U Two Sample Test\nweb\ncode\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "Welch’s t-test (also known as Welch’s unequal variances t-test) is an adaptation of the standard two-sample t-test that is designed to provide a valid test when the two groups have unequal variances. This is particularly important because the assumption of equal variances is often violated in real-world data.\nWhile the standard two-sample t-test makes the following comparison:\n\\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1 \\neq \\mu_2\\)\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group\n\n\n\nThe formula for Welch’s t-test is:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_1^2\\) is the sample variance of the first group - \\(s_2^2\\) is the sample variance of the second group - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe key difference from the standard t-test is that Welch’s t-test does not use a pooled variance estimate, making it more robust when the variances differ between groups.\nThe degrees of freedom for Welch’s t-test are calculated using the Welch-Satterthwaite equation:\n\\(df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\)\nThis often results in a non-integer value for degrees of freedom, which is why you’ll typically see it rounded in reports."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#background-and-theory",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#background-and-theory",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "Welch’s t-test (also known as Welch’s unequal variances t-test) is an adaptation of the standard two-sample t-test that is designed to provide a valid test when the two groups have unequal variances. This is particularly important because the assumption of equal variances is often violated in real-world data.\nWhile the standard two-sample t-test makes the following comparison:\n\\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1 \\neq \\mu_2\\)\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#formula-for-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#formula-for-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "The formula for Welch’s t-test is:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_1^2\\) is the sample variance of the first group - \\(s_2^2\\) is the sample variance of the second group - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe key difference from the standard t-test is that Welch’s t-test does not use a pooled variance estimate, making it more robust when the variances differ between groups.\nThe degrees of freedom for Welch’s t-test are calculated using the Welch-Satterthwaite equation:\n\\(df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\)\nThis often results in a non-integer value for degrees of freedom, which is why you’ll typically see it rounded in reports."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#loading-libraries-and-data",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#loading-libraries-and-data",
    "title": "Welchs Two Sample T-Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n# library(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogramlibrary(tidyverse)\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin        53   1.25\n2   146 E 01  slimy sculpin        61   1.9 \n3   146 E 01  slimy sculpin        53   1.75\n4   146 E 01  slimy sculpin        77   4.25\n5   146 E 01  slimy sculpin        45   0.9 \n6   146 E 01  slimy sculpin        48   0.9"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#data-overview",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#data-overview",
    "title": "Welchs Two Sample T-Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site     : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake     : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species  : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g   : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species            length_mm     \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n     site      lake   species length_mm    mass_g \n       79         0         0       329         0"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#data-preparation",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#data-preparation",
    "title": "Welchs Two Sample T-Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(length_mm),\n    sd = sd(length_mm),\n    se = sd / sqrt(n),\n    var = var(length_mm)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 6\n  lake      n  mean    sd    se   var\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72  110.\n2 S 07     73  55.6  12.7  1.48  160."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#mean-and-standard-error-plot",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#mean-and-standard-error-plot",
    "title": "Welchs Two Sample T-Test",
    "section": "Mean and Standard Error Plot",
    "text": "Mean and Standard Error Plot\nLet’s also create a plot showing the mean and standard error for each lake, with # Data Visualization\nLet’s visualize our data to better understand the distributions and differences between the two lakes:"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#box-plot-with-individual-data-points",
    "title": "Welchs Two Sample T-Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points show the actual measurements, helping us visualize the full distribution of the data. in the background:\n\n# Calculate means and standard errors\nlake_means &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean = mean(length_mm),\n    se = sd(length_mm) / sqrt(n())\n  )\n\n# Create mean and standard error plot with data points\nggplot() +\n  # Add individual data points in the background\n  geom_point(data = sculpin_filtered, \n             aes(x = lake, y = length_mm, color = lake),\n             position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  geom_point(data = lake_means, \n             aes(x = lake, y = mean, color = lake),\n             size = 4) +\n  geom_errorbar(data = lake_means, \n                aes(x = lake, ymin = mean - se, ymax = mean + se, color = lake),\n                width = 0.2, size = 1) +\n  # Add annotations for the means\n  geom_text(data = lake_means,\n            aes(x = lake, y = mean + se + 3, \n                label = paste0(round(mean, 1), \" ± \", round(se, 1), \" mm\")),\n            size = 3.5) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\nMean and Standard Error Plot\nNow, let’s create a plot showing the mean and standard error for each lake, with individual data points in the background:\n\n# Create mean and standard error plot with data points\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, color = lake)) +\n  # Add individual data points in the background\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#assumptions-of-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#assumptions-of-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "Assumptions of Welch’s t-Test",
    "text": "Assumptions of Welch’s t-Test\n\nIndependence: The observations within each group are independent, and the two groups are independent of each other.\nNormality: The data in each group follow approximately normal distributions (though Welch’s t-test is more robust to violations of normality than the standard t-test).\n\nUnlike the standard t-test, Welch’s t-test does not assume that the variances of the two groups are equal. This makes it more appropriate for many real-world datasets.\nLet’s test the assumptions we do need to meet:\n\n1. Independence Assumption\nIndependence is a design issue and can’t be tested statistically. We assume our sampling design ensures independence between and within groups.\n\n\n2. Normality Assumption\nWe’ll check normality using: - Visual methods: Histograms and Q-Q plots - Formal test: Shapiro-Wilk test\n\nHistograms\n\n# Create histograms for both lakes\npar(mfrow = c(1, 2))\n\n# note that this is base r plotting\n\n# Lake S 07\nhist(s07_data$length_mm, \n     main = \"Histogram of Total Length for Lake S 07\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# Lake NE 14\nhist(ne14_data$length_mm, \n     main = \"Histogram of Total Length for Lake NE 14\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n\n\n\n\n\n\n\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n\nQQ Plots\n\n# QQ plot for Lake S 07\nggplot(data = s07_data, aes(sample = length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake S 07\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# QQ plot for Lake NE 14\nggplot(data = ne14_data, aes(sample = length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake NE 14\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n# Shapiro-Wilk test for Lake S 07\nshapiro_s07 &lt;- shapiro.test(s07_data$length_mm)\nprint(\"Shapiro-Wilk test for Lake S 07:\")\n\n[1] \"Shapiro-Wilk test for Lake S 07:\"\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$length_mm\nW = 0.98035, p-value = 0.3125\n\n# Shapiro-Wilk test for Lake NE 14\nshapiro_ne14 &lt;- shapiro.test(ne14_data$length_mm)\nprint(\"Shapiro-Wilk test for Lake NE 14:\")\n\n[1] \"Shapiro-Wilk test for Lake NE 14:\"\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$length_mm\nW = 0.9479, p-value = 0.08258\n\n# Summary table\nshapiro_results &lt;- data.frame(\n  Lake = c(\"S 07\", \"NE 14\"),\n  W_statistic = c(shapiro_s07$statistic, shapiro_ne14$statistic),\n  p_value = c(shapiro_s07$p.value, shapiro_ne14$p.value),\n  is_normal = c(shapiro_s07$p.value &gt; 0.05, shapiro_ne14$p.value &gt; 0.05)\n)\n\nknitr::kable(shapiro_results, caption = \"Shapiro-Wilk Test Results\")\n\n\nShapiro-Wilk Test Results\n\n\nLake\nW_statistic\np_value\nis_normal\n\n\n\n\nS 07\n0.9803526\n0.3125255\nTRUE\n\n\nNE 14\n0.9479006\n0.0825839\nTRUE\n\n\n\n\n\n\n\n\n3. Homogeneity of Variances\nWe’ll check for homogeneity of variances using: - Visual inspection of boxplots (already done above) - Levene’s test\n\n# Calculate variances for each group\ns07_variance &lt;- var(s07_data$length_mm)\nne14_variance &lt;- var(ne14_data$length_mm)\n\n# Print variances\ncat(\"Variance for Lake S 07:\", s07_variance, \"\\n\")\n\nVariance for Lake S 07: 160.0552 \n\ncat(\"Variance for Lake NE 14:\", ne14_variance, \"\\n\")\n\nVariance for Lake NE 14: 109.9805 \n\n# Calculate variance ratio\nvariance_ratio &lt;- max(s07_variance, ne14_variance) / min(s07_variance, ne14_variance)\ncat(\"Variance ratio (larger/smaller):\", variance_ratio, \"\\n\")\n\nVariance ratio (larger/smaller): 1.455305 \n\n# Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(length_mm ~ lake, data = sculpin_filtered)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_test)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1   2.029 0.1572\n      108"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-assumption-tests",
    "title": "Welchs Two Sample T-Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests:\n\nIndependence: We assume this is met based on the data collection process, as samples from each lake were collected independently of one another.\nNormality:\n\nThe Q-Q plots show that the data points largely follow the theoretical normal distribution line for both lakes, with some minor deviations at the extremes.\nThe Shapiro-Wilk test results will help us formally assess normality. If the p-value is greater than 0.05, we fail to reject the null hypothesis that the data is normally distributed.\nFor samples larger than 30, the Central Limit Theorem suggests that the sampling distribution of means will be approximately normal regardless of the underlying distribution.\n\nHomogeneity of Variances:\n\nLevene’s test evaluates whether the variances between groups are equal.\nA p-value greater than 0.05 indicates that we cannot reject the null hypothesis of equal variances.\nAs a rule of thumb, if the variance ratio is less than 4:1, the t-test is reasonably robust to violations of this assumption.\nIf this assumption is violated, we should consider using Welch’s t-test instead, which does not assume equal variances."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#line-by-line-interpretation-of-welchs-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#line-by-line-interpretation-of-welchs-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Line-by-Line Interpretation of Welch’s t-Test Results",
    "text": "Line-by-Line Interpretation of Welch’s t-Test Results\nLet’s break down the output from the Welch’s t-test:\n\nTest Type: “Welch Two Sample t-test” indicates we’re using the Welch’s version of the t-test, which does not assume equal variances.\nFormula: length_mm ~ lake means we’re testing if total length differs by lake.\nData: Our filtered sculpin dataset.\nt-value: The calculated t-statistic. This is the ratio of the difference between group means to the standard error of that difference.\nDegrees of Freedom (df): For Welch’s t-test, this is calculated using the Welch-Satterthwaite equation and is typically not a whole number. This adjustment accounts for the different variances.\np-value: The probability of observing a t-statistic as extreme as (or more extreme than) the one we calculated, assuming the null hypothesis is true. A p-value less than our significance level (typically 0.05) leads us to reject the null hypothesis.\nAlternative Hypothesis: States that the difference in means is not equal to 0, which corresponds to our two-sided test.\n95% Confidence Interval: The estimated range for the true difference in means. If this interval does not contain 0, it supports rejecting the null hypothesis.\nSample Estimates: The means of each group being compared."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#visual-representation-of-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#visual-representation-of-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Visual Representation of t-Test Results",
    "text": "Visual Representation of t-Test Results\n\n# Create a plot with the t-test results\np_value_text &lt;- ifelse(p_value &lt; 0.001, \"p &lt; 0.001\", paste(\"p =\", round(p_value, 3)))\n\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  annotate(\"text\", x = 1.5, y = max(sculpin_filtered$length_mm) + 5,\n           label = paste0(\"Welch's t-test: t(\", round(df, 1), \") = \", t_statistic, \", \", p_value_text),\n           size = 4) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    subtitle = \"With Welch's t-test Results\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-welchs-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-welchs-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Interpretation of Welch’s t-Test Results",
    "text": "Interpretation of Welch’s t-Test Results\nBased on our analysis, we can conclude:\nThe total length of slimy sculpin fish differs significantly between Lake S 07 and Lake NE 14 (Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001). Fish from Lake S 07 were on average 8.29 mm longer than those from Lake NE 14 (mean ± SE: 55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm).\nWelch’s t-test was appropriate for this analysis because:\n\nOur data from both lakes appeared to be approximately normally distributed (as seen in the QQ plots and confirmed by the Shapiro-Wilk test).\nOur samples were independent, with fish collected randomly from each lake.\nThe variances between the two groups were somewhat different (variance ratio of 1.46), making Welch’s t-test preferable to the standard t-test.\n\nThe significant p-value (p &lt; 0.001) indicates that the observed difference in fish length between lakes is very unlikely to have occurred by chance alone if there were truly no difference in the population means. The 95% confidence interval for the mean difference does not include zero, which further supports rejecting the null hypothesis."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "title": "Welchs Two Sample T-Test",
    "section": "How to Report These Results in a Scientific Publication",
    "text": "How to Report These Results in a Scientific Publication\nWhen reporting these results in a scientific publication, follow this format:\n“Slimy sculpin (Cottus cognatus) from Lake S 07 were significantly larger than those from Lake NE 14 (55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm, respectively; Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001). This represents an approximately 17.5% difference in total length between the two populations.”\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption for the mean/SE plot would read:\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001).”"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#advantages-of-using-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#advantages-of-using-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "Advantages of Using Welch’s t-Test",
    "text": "Advantages of Using Welch’s t-Test\nWelch’s t-test offers several advantages over the standard t-test:\n\nRobustness to unequal variances: Welch’s t-test does not assume equal variances between groups, making it more appropriate for real-world data where this assumption is often violated.\nMinimal loss of power: When variances are equal, Welch’s t-test performs nearly as well as the standard t-test.\nReduced Type I error rate: When variances are unequal, the standard t-test can have an inflated Type I error rate (false positives), which Welch’s t-test corrects.\nFlexibility: It can be used regardless of whether the variances are equal or not, making it a more versatile statistical test.\n\nFor these reasons, many statisticians recommend using Welch’s t-test as the default approach for comparing means between two independent groups, even when the homogeneity of variance assumption appears to be met.\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption would read:\nagain adding in the mean +/- SE for each lake would be better\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (two-sample t-test: t(108) = 3.46, p &lt; 0.001).”"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html",
    "href": "test_overviews/two_sample_ttest_html.html",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The two-sample t-test (also known as independent samples t-test) is used to determine whether there is a statistically significant difference between the means of two independent groups. In this analysis, we will examine whether there are significant differences in the total length of slimy sculpin fish between two different lakes.\nThe two-sample t-test makes the following comparison:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group\n\n\n\nThe formula for the two-sample t-test with equal variances (pooled variance) is:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_p\\) is the pooled standard deviation - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe pooled standard deviation is calculated as:\n\\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\]\nWhere: - \\(s_1^2\\) is the variance of the first group - \\(s_2^2\\) is the variance of the second group\nThe degrees of freedom (df) for this test is \\(n_1 + n_2 - 2\\).\nFor unequal variances (Welch’s t-test), the formula is slightly different:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWith degrees of freedom approximated using the Welch-Satterthwaite equation:\n\\[df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\]"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#background-and-theory",
    "href": "test_overviews/two_sample_ttest_html.html#background-and-theory",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The two-sample t-test (also known as independent samples t-test) is used to determine whether there is a statistically significant difference between the means of two independent groups. In this analysis, we will examine whether there are significant differences in the total length of slimy sculpin fish between two different lakes.\nThe two-sample t-test makes the following comparison:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#formula",
    "href": "test_overviews/two_sample_ttest_html.html#formula",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The formula for the two-sample t-test with equal variances (pooled variance) is:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_p\\) is the pooled standard deviation - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe pooled standard deviation is calculated as:\n\\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\]\nWhere: - \\(s_1^2\\) is the variance of the first group - \\(s_2^2\\) is the variance of the second group\nThe degrees of freedom (df) for this test is \\(n_1 + n_2 - 2\\).\nFor unequal variances (Welch’s t-test), the formula is slightly different:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWith degrees of freedom approximated using the Welch-Satterthwaite equation:\n\\[df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\]"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#loading-libraries-and-data",
    "href": "test_overviews/two_sample_ttest_html.html#loading-libraries-and-data",
    "title": "Two Sample T-Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n# library(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogram\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin        53   1.25\n2   146 E 01  slimy sculpin        61   1.9 \n3   146 E 01  slimy sculpin        53   1.75\n4   146 E 01  slimy sculpin        77   4.25\n5   146 E 01  slimy sculpin        45   0.9 \n6   146 E 01  slimy sculpin        48   0.9"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-overview",
    "href": "test_overviews/two_sample_ttest_html.html#data-overview",
    "title": "Two Sample T-Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site     : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake     : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species  : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g   : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species            length_mm     \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n     site      lake   species length_mm    mass_g \n       79         0         0       329         0"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-preparation",
    "href": "test_overviews/two_sample_ttest_html.html#data-preparation",
    "title": "Two Sample T-Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake\nsculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    count = n(),\n    mean_length = mean(length_mm),\n    sd_length = sd(length_mm),\n    se_length = sd_length / sqrt(count)\n  )\n\n# A tibble: 2 × 5\n  lake  count mean_length sd_length se_length\n  &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 NE 14    37        47.3      10.5      1.72\n2 S 07     73        55.6      12.7      1.48"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-visualization",
    "href": "test_overviews/two_sample_ttest_html.html#data-visualization",
    "title": "Two Sample T-Test",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nBox Plot with Individual Data Points\nLet’s create a box plot with individual data points to visualize the distribution of total length in the two lakes:\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nMean and Standard Error Plot\nNow, let’s create a plot showing the mean and standard error for each lake, with individual data points in the background:\n\n# Create mean and standard error plot with data points\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, color = lake)) +\n  # Add individual data points in the background\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  stat_summary(fun = mean, geom = \"point\", size = 4) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#assumptions-of-the-two-sample-t-test",
    "href": "test_overviews/two_sample_ttest_html.html#assumptions-of-the-two-sample-t-test",
    "title": "Two Sample T-Test",
    "section": "Assumptions of the Two-Sample t-Test",
    "text": "Assumptions of the Two-Sample t-Test\n\nIndependence: The observations within each group are independent, and the two groups are independent of each other.\nNormality: The data in each group follow a normal distribution.\nHomogeneity of Variances: The variances of the two groups are approximately equal (for the standard t-test).\n\nLet’s test each of these assumptions:\n\n1. Independence Assumption\nIndependence is a design issue and can’t be tested statistically. We assume our sampling design ensures independence between and within groups.\n\n\n2. Normality Assumption\nWe’ll check normality using: - Histograms - Q-Q plots - Shapiro-Wilk test\n\nHistograms\n\n# Create histograms for both lakes\npar(mfrow = c(1, 2))\n\n# Lake S 07\nhist(s07_data$length_mm, \n     main = \"Histogram of Total Length for Lake S 07\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# Lake NE 14\nhist(ne14_data$length_mm, \n     main = \"Histogram of Total Length for Lake NE 14\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\nQQ Plots\nLet’s create individual QQ plots for each lake:\n\n# QQ plot for Lake S 07\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nggplot(data = s07_data, aes(sample = length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake S 07\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# QQ plot for Lake NE 14\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\nggplot(data = ne14_data, aes(sample = length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake NE 14\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n# Shapiro-Wilk test for Lake S 07\nshapiro_s07 &lt;- shapiro.test(s07_data$length_mm)\nprint(\"Shapiro-Wilk test for Lake S 07:\")\n\n[1] \"Shapiro-Wilk test for Lake S 07:\"\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$length_mm\nW = 0.98035, p-value = 0.3125\n\n# Shapiro-Wilk test for Lake NE 14\nshapiro_ne14 &lt;- shapiro.test(ne14_data$length_mm)\nprint(\"Shapiro-Wilk test for Lake NE 14:\")\n\n[1] \"Shapiro-Wilk test for Lake NE 14:\"\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$length_mm\nW = 0.9479, p-value = 0.08258\n\n# Summary table\nshapiro_results &lt;- data.frame(\n  Lake = c(\"S 07\", \"NE 14\"),\n  W_statistic = c(shapiro_s07$statistic, shapiro_ne14$statistic),\n  p_value = c(shapiro_s07$p.value, shapiro_ne14$p.value),\n  is_normal = c(shapiro_s07$p.value &gt; 0.05, shapiro_ne14$p.value &gt; 0.05)\n)\n\nknitr::kable(shapiro_results, caption = \"Shapiro-Wilk Test Results\")\n\n\nShapiro-Wilk Test Results\n\n\nLake\nW_statistic\np_value\nis_normal\n\n\n\n\nS 07\n0.9803526\n0.3125255\nTRUE\n\n\nNE 14\n0.9479006\n0.0825839\nTRUE\n\n\n\n\n\n\n\n3. Homogeneity of Variances\nWe’ll check for homogeneity of variances using: - Visual inspection of boxplots (already done above) - Levene’s test\n\n# Calculate variances for each group\ns07_variance &lt;- var(s07_data$length_mm)\nne14_variance &lt;- var(ne14_data$length_mm)\n\n# Print variances\ncat(\"Variance for Lake S 07:\", s07_variance, \"\\n\")\n\nVariance for Lake S 07: 160.0552 \n\ncat(\"Variance for Lake NE 14:\", ne14_variance, \"\\n\")\n\nVariance for Lake NE 14: 109.9805 \n\n# Calculate variance ratio\nvariance_ratio &lt;- max(s07_variance, ne14_variance) / min(s07_variance, ne14_variance)\ncat(\"Variance ratio (larger/smaller):\", variance_ratio, \"\\n\")\n\nVariance ratio (larger/smaller): 1.455305 \n\n# Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(length_mm ~ lake, data = sculpin_filtered)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_test)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1   2.029 0.1572\n      108"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/two_sample_ttest_html.html#interpretation-of-assumption-tests",
    "title": "Two Sample T-Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests:\n\nIndependence: We assume this is met based on the data collection process, as samples from each lake were collected independently of one another.\nNormality:\n\nThe Q-Q plots show that the data points largely follow the theoretical normal distribution line for both lakes, with some minor deviations at the extremes.\nThe Shapiro-Wilk test results will help us formally assess normality. If the p-value is greater than 0.05, we fail to reject the null hypothesis that the data is normally distributed.\nFor samples larger than 30, the Central Limit Theorem suggests that the sampling distribution of means will be approximately normal regardless of the underlying distribution.\n\nHomogeneity of Variances:\n\nLevene’s test evaluates whether the variances between groups are equal.\nA p-value greater than 0.05 indicates that we cannot reject the null hypothesis of equal variances.\nAs a rule of thumb, if the variance ratio is less than 4:1, the t-test is reasonably robust to violations of this assumption.\nIf this assumption is violated, we should consider using Welch’s t-test instead, which does not assume equal variances."
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#line-by-line-interpretation-of-t-test-results",
    "href": "test_overviews/two_sample_ttest_html.html#line-by-line-interpretation-of-t-test-results",
    "title": "Two Sample T-Test",
    "section": "Line-by-Line Interpretation of t-Test Results",
    "text": "Line-by-Line Interpretation of t-Test Results\nLet’s break down the t-test output:\n\nTest Type: Two Sample t-test\nFormula: length_mm ~ lake means we’re testing if total length differs by lake\nData: Our filtered sculpin dataset\nt-value: The calculated t-statistic\nDegrees of Freedom (df): n₁ + n₂ - 2\np-value: The probability of observing this data (or more extreme) if the null hypothesis is true\nAlternative Hypothesis: The means are different\n95% Confidence Interval: The estimated range for the true difference in means\nSample Estimates: The means of each group"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#visual-representation-of-t-test-results",
    "href": "test_overviews/two_sample_ttest_html.html#visual-representation-of-t-test-results",
    "title": "Two Sample T-Test",
    "section": "Visual Representation of t-Test Results",
    "text": "Visual Representation of t-Test Results\n\n# Create a plot with the t-test results\np_value_text &lt;- ifelse(p_value &lt; 0.001, \"p &lt; 0.001\", paste(\"p =\", round(p_value, 3)))\n\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  annotate(\"text\", x = 1.5, y = max(sculpin_filtered$length_mm) + 5,\n           label = paste0(\"t(\", round(df), \") = \", t_statistic, \", \", p_value_text),\n           size = 4) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake with t-test Results\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "href": "test_overviews/two_sample_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "title": "Two Sample T-Test",
    "section": "How to Report These Results in a Scientific Publication",
    "text": "How to Report These Results in a Scientific Publication\nWhen reporting these results in a scientific publication, follow this format:\n“Slimy sculpin (Cottus cognatus) from Lake S 07 were significantly larger than those from Lake NE 14\n(55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm, respectively; two-sample t-test: t(108) = -3.43, p &lt; 0.001). This represents an approximately 17.5% difference in total length between the two populations.”\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption would read:\nNote I would also add the mean and SE of each lake\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (two-sample t-test: t(108) = 3.46, p &lt; 0.001).”"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Biostatistics course\nBill Perry\nThis course will serve as an introduction to reproducible data analysis using R. Specifically the goal is to introduce students to all facets of managing a research project with an emphasis on:\n\nDeveloping questions form observations, hypotheses, and predictions for testing statistically\nDesigning data workflows with data entry, curation, QA/QC, and cleaning\nUsing a controlled vocabulary and organized project structure and documenting the metadata for the project\nImporting data into R and doing calculations and transformations\nVisualizing data using ggplot\nUnderstanding how to decide on statistical tests that are appropriate - T-Tests - parametric and nonparametric\n\n-   Correlations\n-   Linear models\n    -   linear regression\n    -   multiple linear regression\n    -   ANOVA\n    -   ANCOVA\n    -   GLM and logistic regression\n-   Analyzing Frequencies\n-   Multivariate Statistics\n    -   Principal component analysis\n    -   NMDS\n    -   Cluster analysis\nIn the main webpage I have provided links to all the information you will need:\n\nlinks to readings that should be read prior to class\npowerpoint lectures that should be reviewed prior to class\nin-class activities that will actively cover the materials in the powerpoints\nweekly homework to do the in class activities on your own using different data\nassignments that form the crux of the grade and there 4 of them\n\n\n\n\n Back to top"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html",
    "href": "test_overviews/03_permutation_two_sample_test_html.html",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "Permutation tests (also known as randomization tests) are non-parametric methods used to test hypotheses without making assumptions about the underlying distribution of the data. This makes them particularly valuable when:\n\nSample sizes are small\nData violate normality assumptions\nData have unequal variances\nOutliers are present\n\nFor comparing two independent groups, a permutation test assesses whether the observed difference between groups is likely to occur by chance if there were no true difference between them.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The two samples come from the same distribution}\\] \\[H_A: \\text{The two samples come from different distributions}\\]\nMore specifically, for comparing means:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\n\n\n\nThe permutation test follows these steps:\n\nCalculate the observed test statistic (e.g., difference in means) between the two groups.\nRandomly reassign the observations to the two groups, maintaining the original group sizes.\nCalculate the test statistic for this random permutation.\nRepeat steps 2-3 many times (typically 1,000 to 10,000 times) to build a distribution of test statistics under the null hypothesis.\nCalculate the p-value as the proportion of permuted test statistics that are as extreme as or more extreme than the observed test statistic.\n\nThe key insight is that if the null hypothesis is true (no difference between groups), then the group labels are essentially arbitrary, and any permutation of the labels is equally likely."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#background-and-theory",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#background-and-theory",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "Permutation tests (also known as randomization tests) are non-parametric methods used to test hypotheses without making assumptions about the underlying distribution of the data. This makes them particularly valuable when:\n\nSample sizes are small\nData violate normality assumptions\nData have unequal variances\nOutliers are present\n\nFor comparing two independent groups, a permutation test assesses whether the observed difference between groups is likely to occur by chance if there were no true difference between them.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The two samples come from the same distribution}\\] \\[H_A: \\text{The two samples come from different distributions}\\]\nMore specifically, for comparing means:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#how-permutation-tests-work",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#how-permutation-tests-work",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "The permutation test follows these steps:\n\nCalculate the observed test statistic (e.g., difference in means) between the two groups.\nRandomly reassign the observations to the two groups, maintaining the original group sizes.\nCalculate the test statistic for this random permutation.\nRepeat steps 2-3 many times (typically 1,000 to 10,000 times) to build a distribution of test statistics under the null hypothesis.\nCalculate the p-value as the proportion of permuted test statistics that are as extreme as or more extreme than the observed test statistic.\n\nThe key insight is that if the null hypothesis is true (no difference between groups), then the group labels are essentially arbitrary, and any permutation of the labels is equally likely."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#loading-libraries-and-data",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#loading-libraries-and-data",
    "title": "Two Sample Permutation Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n# library(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogram\n\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin        53   1.25\n2   146 E 01  slimy sculpin        61   1.9 \n3   146 E 01  slimy sculpin        53   1.75\n4   146 E 01  slimy sculpin        77   4.25\n5   146 E 01  slimy sculpin        45   0.9 \n6   146 E 01  slimy sculpin        48   0.9"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#data-overview",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#data-overview",
    "title": "Two Sample Permutation Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site     : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake     : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species  : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g   : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species            length_mm     \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n     site      lake   species length_mm    mass_g \n       79         0         0       329         0"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#data-preparation",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#data-preparation",
    "title": "Two Sample Permutation Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(length_mm),\n    sd = sd(length_mm),\n    se = sd / sqrt(n),\n    var = var(length_mm),\n    median = median(length_mm),\n    min = min(length_mm),\n    max = max(length_mm)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 9\n  lake      n  mean    sd    se   var median   min   max\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72  110.     48    25    67\n2 S 07     73  55.6  12.7  1.48  160.     57    31    87"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#box-plot-with-individual-data-points",
    "title": "Two Sample Permutation Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points help us visualize the full distribution of the data."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#density-plots",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#density-plots",
    "title": "Two Sample Permutation Test",
    "section": "Density Plots",
    "text": "Density Plots\nLet’s also create density plots to better visualize the distribution shapes:\n\n# Create density plots\nggplot(sculpin_filtered, aes(x = length_mm, fill = lake)) +\n  geom_density(alpha = 0.6) +\n  labs(\n    title = \"Density Distribution of Slimy Sculpin Total Length by Lake\",\n    x = \"Total Length (mm)\",\n    y = \"Density\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe density plots show the continuous distribution of lengths for each lake, helping us visualize the overall shape, central tendency, and spread of the data."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#assessing-normality",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#assessing-normality",
    "title": "Two Sample Permutation Test",
    "section": "Assessing Normality",
    "text": "Assessing Normality\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n# Shapiro-Wilk test for normality\nshapiro_s07 &lt;- shapiro.test(s07_data$length_mm)\nshapiro_ne14 &lt;- shapiro.test(ne14_data$length_mm)\n\ncat(\"Shapiro-Wilk test for Lake S 07:\\n\")\n\nShapiro-Wilk test for Lake S 07:\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$length_mm\nW = 0.98035, p-value = 0.3125\n\ncat(\"\\nShapiro-Wilk test for Lake NE 14:\\n\")\n\n\nShapiro-Wilk test for Lake NE 14:\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$length_mm\nW = 0.9479, p-value = 0.08258"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#assessing-variance-equality",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#assessing-variance-equality",
    "title": "Two Sample Permutation Test",
    "section": "Assessing Variance Equality",
    "text": "Assessing Variance Equality\n\n# F-test for equality of variances\nvar_test &lt;- var.test(length_mm ~ lake, data = sculpin_filtered)\nprint(var_test)\n\n\n    F test to compare two variances\n\ndata:  length_mm by lake\nF = 0.68714, num df = 36, denom df = 72, p-value = 0.218\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3982241 1.2529495\nsample estimates:\nratio of variances \n          0.687141 \n\n# Calculate variance ratio\nvar_ratio &lt;- max(lake_stats$var) / min(lake_stats$var)\ncat(\"Variance ratio (larger/smaller):\", var_ratio)\n\nVariance ratio (larger/smaller): 1.455305"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#interpretation-of-assumption-tests",
    "title": "Two Sample Permutation Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests, a permutation test is appropriate because:\n\nNormality: The normal quantile plots and Shapiro-Wilk tests suggest potential departures from normality in the data.\nVariance Equality: The F-test indicates that the variances may not be equal between the two lakes.\nRobustness: Permutation tests are robust to violations of these assumptions and provide a valid test regardless of the underlying distributions."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#understanding-the-permutation-test-results",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#understanding-the-permutation-test-results",
    "title": "Two Sample Permutation Test",
    "section": "Understanding the Permutation Test Results",
    "text": "Understanding the Permutation Test Results\nThe permutation test provides a p-value that represents the probability of observing a difference as extreme as, or more extreme than, the observed difference in means between the two lakes if the null hypothesis were true (i.e., if there were no real difference between lakes).\nOur analysis shows:\n\nObserved Difference: The observed difference in mean total length between Lake S 07 and Lake NE 14 is 8.29 mm.\np-value: The permutation test yielded a p-value of &lt; 0.001 based on 10^{4} random permutations.\nInterpretation: Since the p-value is less than 0.05, we reject the null hypothesis. This indicates that the observed difference in fish length between the two lakes is statistically significant and unlikely to have occurred by chance.\nVisualization: The permutation distribution graph shows the distribution of mean differences we would expect to see under the null hypothesis, with the observed difference marked by the red line. The fact that the observed difference falls in the extreme tails of this distribution supports our conclusion."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#advantages-of-the-permutation-test",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#advantages-of-the-permutation-test",
    "title": "Two Sample Permutation Test",
    "section": "Advantages of the Permutation Test",
    "text": "Advantages of the Permutation Test\nThe permutation test offered several advantages for this analysis:\n\nNo Distributional Assumptions: Unlike parametric tests like the t-test, permutation tests don’t require the data to follow a normal distribution.\nRobust to Unequal Variances: Permutation tests are valid even when the two groups have different variances.\nAppropriate for Small Samples: Permutation tests can provide valid inference even with smaller sample sizes.\nIntuitive Interpretation: The permutation approach provides a direct, intuitive way to assess how likely the observed difference is under the null hypothesis.\nExact p-values: With enough permutations, we can get very precise p-values without relying on theoretical approximations."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The Mann-Whitney-Wilcoxon test (also known as the Wilcoxon rank-sum test or Mann-Whitney U test) is a powerful non-parametric alternative to the two-sample t-test. This test is particularly useful when:\n\nThe data do not follow a normal distribution\nThe sample sizes are small\nData are measured on an ordinal scale\nOutliers are present\n\nUnlike the t-test, which compares means, the Mann-Whitney-Wilcoxon test compares the distributions of two independent groups. Specifically, it tests whether one distribution is stochastically greater than the other.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The distributions of both groups are identical}\\] \\[H_A: \\text{The distributions of the two groups differ in location (median)}\\]\n\n\n\nThe test follows these steps:\n\nCombine all observations from both groups and rank them from lowest to highest.\nCalculate the sum of ranks for each group.\nCalculate the U statistic, which represents the number of times observations in one group precede observations in the other group.\nCompare the calculated U statistic to the critical value from the Mann-Whitney-Wilcoxon distribution, or calculate a p-value for larger samples.\n\nThe U statistic is calculated as:\n\\[U_1 = R_1 - \\frac{n_1(n_1 + 1)}{2}\\]\nWhere: - \\(R_1\\) is the sum of ranks in group 1 - \\(n_1\\) is the sample size of group 1\nIf U is sufficiently small or large compared to what would be expected by chance, we reject the null hypothesis."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#background-and-theory",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#background-and-theory",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The Mann-Whitney-Wilcoxon test (also known as the Wilcoxon rank-sum test or Mann-Whitney U test) is a powerful non-parametric alternative to the two-sample t-test. This test is particularly useful when:\n\nThe data do not follow a normal distribution\nThe sample sizes are small\nData are measured on an ordinal scale\nOutliers are present\n\nUnlike the t-test, which compares means, the Mann-Whitney-Wilcoxon test compares the distributions of two independent groups. Specifically, it tests whether one distribution is stochastically greater than the other.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The distributions of both groups are identical}\\] \\[H_A: \\text{The distributions of the two groups differ in location (median)}\\]"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#how-the-mann-whitney-wilcoxon-test-works",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#how-the-mann-whitney-wilcoxon-test-works",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The test follows these steps:\n\nCombine all observations from both groups and rank them from lowest to highest.\nCalculate the sum of ranks for each group.\nCalculate the U statistic, which represents the number of times observations in one group precede observations in the other group.\nCompare the calculated U statistic to the critical value from the Mann-Whitney-Wilcoxon distribution, or calculate a p-value for larger samples.\n\nThe U statistic is calculated as:\n\\[U_1 = R_1 - \\frac{n_1(n_1 + 1)}{2}\\]\nWhere: - \\(R_1\\) is the sum of ranks in group 1 - \\(n_1\\) is the sample size of group 1\nIf U is sufficiently small or large compared to what would be expected by chance, we reject the null hypothesis."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#loading-libraries-and-data",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#loading-libraries-and-data",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n# library(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogram\n\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin        53   1.25\n2   146 E 01  slimy sculpin        61   1.9 \n3   146 E 01  slimy sculpin        53   1.75\n4   146 E 01  slimy sculpin        77   4.25\n5   146 E 01  slimy sculpin        45   0.9 \n6   146 E 01  slimy sculpin        48   0.9"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-overview",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-overview",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site     : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake     : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species  : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g   : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species            length_mm     \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n     site      lake   species length_mm    mass_g \n       79         0         0       329         0"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-preparation",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-preparation",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(length_mm),\n    sd = sd(length_mm),\n    se = sd / sqrt(n),\n    median = median(length_mm),\n    min = min(length_mm),\n    max = max(length_mm),\n    Q1 = quantile(length_mm, 0.25),\n    Q3 = quantile(length_mm, 0.75)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 10\n  lake      n  mean    sd    se median   min   max    Q1    Q3\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72     48    25    67    42    54\n2 S 07     73  55.6  12.7  1.48     57    31    87    45    64"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#box-plot-with-individual-data-points",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points using position_dodge2\nggplot(sculpin_filtered, aes(x = lake, y = length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points help us visualize the full distribution of the data."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#density-plots-with-median-lines",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#density-plots-with-median-lines",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Density Plots with Median Lines",
    "text": "Density Plots with Median Lines\n\n# Create density plots with median markers\nggplot(sculpin_filtered, aes(x = length_mm, fill = lake)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(data = lake_stats, \n             aes(xintercept = median, color = lake),\n             linewidth = 1, linetype = \"dashed\") +\n  labs(\n    title = \"Density Distribution of Slimy Sculpin Total Length by Lake\",\n    subtitle = \"Dashed lines indicate medians\",\n    x = \"Total Length (mm)\",\n    y = \"Density\",\n    fill = \"Lake\",\n    color = \"Median\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe density plots show the continuous distribution of lengths for each lake, with dashed lines marking the median values. This visualization is particularly relevant for the Mann-Whitney-Wilcoxon test, which compares medians rather than means."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assessing-normality",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assessing-normality",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Assessing Normality",
    "text": "Assessing Normality\n\n# Create QQ plots for each lake\npar(mfrow = c(1, 2))\n\n# Lake S 07\nqqnorm(s07_data$length_mm, \n       main = \"Q-Q Plot for Lake S 07\", \n       col = \"blue\")\nqqline(s07_data$length_mm)\n\n# Lake NE 14\nqqnorm(ne14_data$length_mm, \n       main = \"Q-Q Plot for Lake NE 14\", \n       col = \"green\")\nqqline(ne14_data$length_mm)\n\n\n\n\n\n\n\n\n\n# Shapiro-Wilk test for normality\nshapiro_s07 &lt;- shapiro.test(s07_data$length_mm)\nshapiro_ne14 &lt;- shapiro.test(ne14_data$length_mm)\n\ncat(\"Shapiro-Wilk test for Lake S 07:\\n\")\n\nShapiro-Wilk test for Lake S 07:\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$length_mm\nW = 0.98035, p-value = 0.3125\n\ncat(\"\\nShapiro-Wilk test for Lake NE 14:\\n\")\n\n\nShapiro-Wilk test for Lake NE 14:\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$length_mm\nW = 0.9479, p-value = 0.08258\n\n\nBased on the Q-Q plots and Shapiro-Wilk tests, we can assess whether our data follow a normal distribution. The Mann-Whitney-Wilcoxon test is appropriate regardless of the outcome because it doesn’t assume normality."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assumptions-of-the-mann-whitney-wilcoxon-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assumptions-of-the-mann-whitney-wilcoxon-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Assumptions of the Mann-Whitney-Wilcoxon Test",
    "text": "Assumptions of the Mann-Whitney-Wilcoxon Test\nThe Mann-Whitney-Wilcoxon test has the following assumptions:\n\nIndependent samples: The observations in each group are independent of each other, and the two groups are independent of each other.\nOrdinal data: The measurements must be at least on an ordinal scale (can be ranked).\nSimilar distributions: If testing for differences in medians specifically, the shapes of the distributions should be similar (though not necessarily normal).\n\nLet’s check if our data meet these assumptions:\n\n# Compare distribution shapes visually\nggplot(sculpin_filtered, aes(x = length_mm, fill = lake)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Comparing Distribution Shapes Between Lakes\",\n    x = \"Total Length (mm)\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\nIf the distributions have roughly similar shapes (even if they’re shifted), we can interpret the Mann-Whitney-Wilcoxon test as testing for differences in medians. If the shapes differ substantially, the test more broadly examines whether one distribution is stochastically greater than the other."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-base-rs-wilcox.test-function",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-base-rs-wilcox.test-function",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Using Base R’s wilcox.test Function",
    "text": "Using Base R’s wilcox.test Function\n\n# Perform the Mann-Whitney-Wilcoxon test\nwilcox_test &lt;- wilcox.test(length_mm ~ lake, \n                          data = sculpin_filtered,\n                          exact = FALSE,  # Use approximate method for larger samples\n                          correct = TRUE)  # Apply continuity correction\n\n# Display the results\nprint(wilcox_test)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  length_mm by lake\nW = 867, p-value = 0.00223\nalternative hypothesis: true location shift is not equal to 0\n\n# Store the p-value for later use\np_value &lt;- wilcox_test$p.value"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-the-coin-package-for-an-exact-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-the-coin-package-for-an-exact-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Using the coin Package for an Exact Test",
    "text": "Using the coin Package for an Exact Test\nFor more precise results, especially with smaller samples, we can use the coin package to perform an exact Mann-Whitney-Wilcoxon test:\n\n# Convert lake to factor (required for the coin package)\nsculpin_filtered$lake_factor &lt;- factor(sculpin_filtered$lake)\n\n# Perform the Mann-Whitney test using the approximate method\n# (which works reliably for all sample sizes)\ncoin_wilcox &lt;- coin::wilcox_test(\n  length_mm ~ lake_factor,\n  data = sculpin_filtered,\n  distribution = \"approximate\"\n)\n\n# Display the results\nprint(coin_wilcox)\n\n\n    Approximative Wilcoxon-Mann-Whitney Test\n\ndata:  length_mm by lake_factor (NE 14, S 07)\nZ = -3.0609, p-value = 0.0018\nalternative hypothesis: true mu is not equal to 0\n\n# Extract the p-value\npvalue_coin &lt;- pvalue(coin_wilcox)\ncat(\"p-value:\", pvalue_coin, \"\\n\")\n\np-value: 0.0018"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#calculating-effect-size",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#calculating-effect-size",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Calculating Effect Size",
    "text": "Calculating Effect Size\nThe Mann-Whitney-Wilcoxon test tells us whether there’s a statistically significant difference, but it doesn’t indicate the magnitude of that difference. Let’s calculate an effect size measure:\n\n## Calculating Effect Size\n\n# The Mann-Whitney-Wilcoxon test tells us whether there's a statistically significant difference, but it doesn't indicate the magnitude of that difference. Let's calculate an effect size measure:\n\n# Calculate standardized effect size using rank-biserial correlation\n# (equivalent to r = Z / sqrt(N))\nz_score &lt;- qnorm(p_value/2)  # Convert p-value to Z-score\nN &lt;- nrow(sculpin_filtered)\nr &lt;- abs(z_score) / sqrt(N)  # Rank-biserial correlation\n\ncat(\"Effect size (rank-biserial correlation):\", round(r, 3), \"\\n\")\n\nEffect size (rank-biserial correlation): 0.292 \n\n# Interpret effect size\neffect_size &lt;- r\nif(effect_size &lt; 0.1) {\n  effect_interpretation &lt;- \"negligible\"\n} else if(effect_size &lt; 0.3) {\n  effect_interpretation &lt;- \"small\"\n} else if(effect_size &lt; 0.5) {\n  effect_interpretation &lt;- \"moderate\"\n} else if(effect_size &lt; 0.7) {\n  effect_interpretation &lt;- \"large\"\n} else {\n  effect_interpretation &lt;- \"very large\"\n}\n\ncat(\"This represents a\", effect_interpretation, \"effect.\\n\")\n\nThis represents a small effect."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#understanding-the-mann-whitney-wilcoxon-test-results",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#understanding-the-mann-whitney-wilcoxon-test-results",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Understanding the Mann-Whitney-Wilcoxon Test Results",
    "text": "Understanding the Mann-Whitney-Wilcoxon Test Results\nThe Mann-Whitney-Wilcoxon test provides a p-value that represents the probability of observing the rank sum (or a more extreme value) if the null hypothesis were true (i.e., if there were no difference in the distributions of the two lakes).\nOur analysis shows:\n\nObserved Difference: The observed difference in median total length between Lake S 07 and Lake NE 14 is 9 mm.\np-value: The Mann-Whitney-Wilcoxon test yielded a p-value of 0.002.\nEffect Size: The rank-biserial correlation (r = 0.29) indicates a small effect size.\nInterpretation: Since the p-value is less than 0.05, we reject the null hypothesis. This indicates that the distributions of fish lengths between the two lakes are significantly different."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#advantages-of-the-mann-whitney-wilcoxon-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#advantages-of-the-mann-whitney-wilcoxon-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Advantages of the Mann-Whitney-Wilcoxon Test",
    "text": "Advantages of the Mann-Whitney-Wilcoxon Test\nThe Mann-Whitney-Wilcoxon test offered several advantages for this analysis:\n\nNo Normality Assumption: It doesn’t require the data to follow a normal distribution, making it appropriate for many ecological datasets.\nRobust to Outliers: By using ranks instead of actual values, it’s less sensitive to extreme observations.\nApplicable to Ordinal Data: It can be used even when data are measured on an ordinal rather than interval scale.\nEfficiency: With normally distributed data, the test has 95% efficiency compared to the t-test, but can be more powerful when distributions are non-normal.\nInterpretability: It provides a clear assessment of whether one population tends to have larger values than the other."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#comparison-to-parametric-tests",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#comparison-to-parametric-tests",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Comparison to Parametric Tests",
    "text": "Comparison to Parametric Tests\nFor comparison, let’s see what a standard t-test would have concluded:\n\n# Perform a t-test for comparison\nt_test_result &lt;- t.test(length_mm ~ lake, data = sculpin_filtered)\nprint(t_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  length_mm by lake\nt = -3.6483, df = 85.45, p-value = 0.0004533\nalternative hypothesis: true difference in means between group NE 14 and group S 07 is not equal to 0\n95 percent confidence interval:\n -12.809687  -3.773061\nsample estimates:\nmean in group NE 14  mean in group S 07 \n           47.27027            55.56164 \n\n# Compare p-values\ncat(\"Mann-Whitney-Wilcoxon p-value:\", p_value, \"\\n\")\n\nMann-Whitney-Wilcoxon p-value: 0.002230158 \n\ncat(\"t-test p-value:\", t_test_result$p.value, \"\\n\")\n\nt-test p-value: 0.0004532708 \n\n\nIn this case, both tests lead to the same conclusions regarding statistical significance. However, the Mann-Whitney-Wilcoxon test is more appropriate when normality assumptions are violated, and it’s testing a different hypothesis (difference in distributions rather than means)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "This course teaches the fundamentals of statistics and data analysis for biological sciences. Using R and the tidyverse, you’ll learn how to analyze and visualize data effectively to answer scientific questions.\n\n\nMaterials for this course are organized into lectures, in-class activities, and homework assignments. All materials are available through this website and may be on canvas website at school.\n\n\n\nBelow is a summary of the course schedule.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 Introduction - Start with R\nR4DataScience Intro\nR4DataScience Visualizaiton\nLecture 1 PowerPoint\nLecture 1 Activity\nWeek 1 Homework\n\n\n02 Project Design and Graphing data - GGPLOT\nR4DataScience Tidy Data\nPowerPoint\nActivity\n\n\n\n03 Descriptive Stats and wrangling\n\nPowerPoint\nActivity\nWeek 2 Homework\n\n\n04 Probability and Inference I - z distriubtions\n\nPowerPoint\nActivity\n\n\n\n05 Probability and Inference II - t distribution\n\nPowerPoint\nActivity\nWeek 3 Homework\n\n\n06 T-Tests\n\nPowerPoint\nActivity\n\n\n\n07 Non Parametric TTests\n\nPowerPoint\nActivity\nWeek 5 Homework\n\n\n08 Study Design for more than T-Tests - Sampling\n\nPowerPoint\n\n\n\n\n09 Correlation vs Linear Models - Regression I\n\nPowerPoint\nActivity\n\n\n\n10 Regressions II\n\nPowerPoint\n\n\n\n\n11 Multiple Regression\n\nPowerPoint\n\n\n\n\n12 Analysis of Variance\n\nPowerPoint\n\n\n\n\n13 Analysis of Variance - 2 way anova\n\nPowerPoint\n\n\n\n\n14 Analysis of Variance - nested anovas\n\nPowerPoint\n\n\n\n\n15 GLM and Logistic Regression (27) - What if assumptions Fail\n\nPowerPoint\n\n\n\n\n16 ANCOVA - Analysis of Covariance (20)\n\nPowerPoint\n\n\n\n\n17 Multivariate Statistics (21)\n\nPowerPoint\n\n\n\n\n18 Principal Component Analysis - PCA (23)\n\nPowerPoint\n\n\n\n\n19 NMDS - CLUSTER ANALYSIS - HYPOTHESES (25)\n\nPowerPoint\n\n\n\n\n20 - Review (28)\n\nPowerPoint\n\n\n\n\n\n\nOrder of topics is subject to change depending on the progress of the class. Changes to the class schedule will be announced in class.\n\n\n\n\n\nR for Data Science by Hadley Wickham\nWhitlock and Schluter The Analysis of Biological Data - 3rd edition\n\n\n\n\n\nI have put together a page of helpful links that might be of great help as you move through the class\n\n\n\n\n\n\nBelow is a link to all the statistical tests that we are covering in the class. These should be helpful to accomplish most of the statistics you will need to do. The format of all of these tests is:\n\ntype of data for the test\nhypotheses being tests\nassumptions of the test\nthe test itself and interpretation of the output\ntests of assumptions\nhow to make a final publication quality plot\nhow to write up the results of your output\n\n\n\n\nIf you have questions about the course or the website, please contact me at wlperry@d.umn.edu"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "Materials for this course are organized into lectures, in-class activities, and homework assignments. All materials are available through this website and may be on canvas website at school."
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "Below is a summary of the course schedule.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n01 Introduction - Start with R\nR4DataScience Intro\nR4DataScience Visualizaiton\nLecture 1 PowerPoint\nLecture 1 Activity\nWeek 1 Homework\n\n\n02 Project Design and Graphing data - GGPLOT\nR4DataScience Tidy Data\nPowerPoint\nActivity\n\n\n\n03 Descriptive Stats and wrangling\n\nPowerPoint\nActivity\nWeek 2 Homework\n\n\n04 Probability and Inference I - z distriubtions\n\nPowerPoint\nActivity\n\n\n\n05 Probability and Inference II - t distribution\n\nPowerPoint\nActivity\nWeek 3 Homework\n\n\n06 T-Tests\n\nPowerPoint\nActivity\n\n\n\n07 Non Parametric TTests\n\nPowerPoint\nActivity\nWeek 5 Homework\n\n\n08 Study Design for more than T-Tests - Sampling\n\nPowerPoint\n\n\n\n\n09 Correlation vs Linear Models - Regression I\n\nPowerPoint\nActivity\n\n\n\n10 Regressions II\n\nPowerPoint\n\n\n\n\n11 Multiple Regression\n\nPowerPoint\n\n\n\n\n12 Analysis of Variance\n\nPowerPoint\n\n\n\n\n13 Analysis of Variance - 2 way anova\n\nPowerPoint\n\n\n\n\n14 Analysis of Variance - nested anovas\n\nPowerPoint\n\n\n\n\n15 GLM and Logistic Regression (27) - What if assumptions Fail\n\nPowerPoint\n\n\n\n\n16 ANCOVA - Analysis of Covariance (20)\n\nPowerPoint\n\n\n\n\n17 Multivariate Statistics (21)\n\nPowerPoint\n\n\n\n\n18 Principal Component Analysis - PCA (23)\n\nPowerPoint\n\n\n\n\n19 NMDS - CLUSTER ANALYSIS - HYPOTHESES (25)\n\nPowerPoint\n\n\n\n\n20 - Review (28)\n\nPowerPoint\n\n\n\n\n\n\nOrder of topics is subject to change depending on the progress of the class. Changes to the class schedule will be announced in class."
  },
  {
    "objectID": "index.html#required-readings",
    "href": "index.html#required-readings",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "R for Data Science by Hadley Wickham\nWhitlock and Schluter The Analysis of Biological Data - 3rd edition"
  },
  {
    "objectID": "index.html#helpful-links-for-resources",
    "href": "index.html#helpful-links-for-resources",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "I have put together a page of helpful links that might be of great help as you move through the class"
  },
  {
    "objectID": "index.html#i-have-put-together-a-set-of-all-statistical-tests-we-have-done",
    "href": "index.html#i-have-put-together-a-set-of-all-statistical-tests-we-have-done",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "Below is a link to all the statistical tests that we are covering in the class. These should be helpful to accomplish most of the statistics you will need to do. The format of all of these tests is:\n\ntype of data for the test\nhypotheses being tests\nassumptions of the test\nthe test itself and interpretation of the output\ntests of assumptions\nhow to make a final publication quality plot\nhow to write up the results of your output"
  },
  {
    "objectID": "index.html#questions-or-issues",
    "href": "index.html#questions-or-issues",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "If you have questions about the course or the website, please contact me at wlperry@d.umn.edu"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful Links",
    "section": "",
    "text": "These are the really helpful links I use a lot\nStackoverflow - a site that has a lot of good resources\nR Companion - this is a great stats site with a lot of well done tests\nR4DataScience - this is the book by Grolemund and Whickham that is really good\nA fun site is BlueSky and use #rstats\nStill adding\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_slides.html#model-specification",
    "href": "lectures/lecture_14/14_03_nested_anova_random_slides.html#model-specification",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Model Specification",
    "text": "Model Specification\nWe’ll use the following model specification:\n\\(ALGAE_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}\\)\nWhere: - \\(\\mu\\) is the overall mean - \\(\\alpha_i\\) is the fixed effect of treatment \\(i\\) - \\(\\beta_{j(i)}\\) is the random effect of patch \\(j\\) nested within treatment \\(i\\) - \\(\\epsilon_{ijk}\\) is the residual error for quadrat \\(k\\) in patch \\(j\\) within treatment \\(i\\)\nIn lme4, this model is specified as\n\n# Fit the mixed model\nmixed_model &lt;- lmer(ALGAE ~ TREAT + (1|TREAT:PATCH), data = andrew)\n\n# Display model summary\nsummary(mixed_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ALGAE ~ TREAT + (1 | TREAT:PATCH)\n   Data: andrew\n\nREML criterion at convergence: 682.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9808 -0.3106 -0.1093  0.2831  2.5910 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n TREAT:PATCH (Intercept) 294.3    17.16   \n Residual                298.6    17.28   \nNumber of obs: 80, groups:  TREAT:PATCH, 16\n\nFixed effects:\n                 Estimate Std. Error     df t value Pr(&gt;|t|)  \n(Intercept)         1.300      9.408 12.000   0.138   0.8924  \nTREAT66% Density   20.250     13.305 12.000   1.522   0.1539  \nTREAT33% Density   17.700     13.305 12.000   1.330   0.2081  \nTREATRemoved       37.900     13.305 12.000   2.849   0.0147 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) TREAT6D TREAT3D\nTREAT66%Dns -0.707                \nTREAT33%Dns -0.707  0.500         \nTREATRemovd -0.707  0.500   0.500"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_slides.html#anova-table",
    "href": "lectures/lecture_14/14_03_nested_anova_random_slides.html#anova-table",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nThe ANOVA table for the mixed model:\n\n# Get ANOVA table with Type III tests\nanova_table &lt;- anova(mixed_model, type = 3)\nprint(anova_table)\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# For comparison, also run a traditional nested ANOVA\nnested_aov &lt;- aov(ALGAE ~ TREAT + TREAT:PATCH, data = andrew)\nstd_summary &lt;- summary(nested_aov)[[1]]\n\n# Extract MS values - using exact row names\nMS_treat &lt;- std_summary[\"TREAT      \", \"Mean Sq\"] \nMS_patch &lt;- std_summary[\"TREAT:PATCH\", \"Mean Sq\"]\nMS_residual &lt;- std_summary[\"Residuals\", \"Mean Sq\"]\n\n# Print MS values to check\nprint(\"MS values:\")\n\n[1] \"MS values:\"\n\nprint(c(Treatment = MS_treat, Patches = MS_patch, Residual = MS_residual))\n\nTreatment   Patches  Residual \n 4809.712  1770.162   298.600 \n\n# Extract df values\ndf_treat &lt;- std_summary[\"TREAT      \", \"Df\"]\ndf_patch &lt;- std_summary[\"TREAT:PATCH\", \"Df\"]\ndf_residual &lt;- std_summary[\"Residuals\", \"Df\"]\n\n\n# Calculate correct F ratios for nested design\nF_treat &lt;- MS_treat / MS_patch\nF_patch &lt;- MS_patch / MS_residual\n\n# Calculate p-values\np_treat &lt;- pf(F_treat, df_treat, df_patch, lower.tail = FALSE)\np_patch &lt;- pf(F_patch, df_patch, df_residual, lower.tail = FALSE)\n\n# Create ANOVA table\ntrad_anova_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(p_treat, p_patch, NA)\n)\n\n# Format p-values\ntrad_anova_table$p &lt;- ifelse(trad_anova_table$p &lt; 0.001, \"&lt;0.001\",\n                       ifelse(is.na(trad_anova_table$p), NA,\n                              format(trad_anova_table$p, digits = 3)))\ntrad_anova_table\n\n               Source df       MS        F           p\n1           Treatment  3 4809.712 2.717102 0.091262004\n2 Patches (treatment) 12 1770.162 5.928207      &lt;0.001\n3            Residual 64  298.600       NA        &lt;NA&gt;\n\n# # Display traditional ANOVA table with flextable\n# trad_anova_table %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     Source = \"Source of variation\",\n#     df = \"df\",\n#     MS = \"MS\",\n#     F = \"F\",\n#     p = \"p\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"MS\", \"F\"), digits = 2) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"ANOVA table for nested design\") %&gt;%\n#   theme_box()"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_slides.html#variance-components",
    "href": "lectures/lecture_14/14_03_nested_anova_random_slides.html#variance-components",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Variance Components",
    "text": "Variance Components\nWe can extract the variance components from the mixed model:\n\n# Print corrected results\n\n# Extract variance components\nvc &lt;- VarCorr(mixed_model)\nprint(vc)\n\n Groups      Name        Std.Dev.\n TREAT:PATCH (Intercept) 17.156  \n Residual                17.280  \n\n# Extract variance components\nvar_comp_patch &lt;- as.numeric(vc$`TREAT:PATCH`)\nvar_comp_residual &lt;- attr(vc, \"sc\")^2\n\n# Calculate percentage of total variance\ntotal_var &lt;- var_comp_patch + var_comp_residual\npct_patch &lt;- var_comp_patch / total_var * 100\npct_residual &lt;- var_comp_residual / total_var * 100\n\n# Calculate treatment variance component\nn_quad &lt;- 5  # Number of quadrats per patch\nn_patch &lt;- 4  # Number of patches per treatment\nvar_comp_treatment &lt;- (MS_treat - MS_patch) / (n_quad * n_patch)\n\n# Format variance components for display\nvar_comp_treatment_display &lt;- ifelse(var_comp_treatment &lt; 0, \n                                    paste0(\"(\", format(abs(var_comp_treatment), digits = 2), \")\"),\n                                    format(var_comp_treatment, digits = 2))\n\n# Create variance components table\nvar_comp_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  `Var.comp` = c(var_comp_treatment_display, \n               format(var_comp_patch, digits = 2),\n               format(var_comp_residual, digits = 2))\n)\n\n# Display variance components table\nvar_comp_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    Var.comp = \"Variance component\"\n  ) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Variance components\") %&gt;%\n  theme_box()\n\nVariance componentsSource of variationVariance componentTreatment152Patches (treatment)294Residual299\n\n# Complete table with all information\ncomplete_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(trad_anova_table$p[1], trad_anova_table$p[2], NA),\n  `Var.comp` = c(var_comp_treatment_display, \n                format(var_comp_patch, digits = 2),\n                format(var_comp_residual, digits = 2))\n)\n\n# Display complete table\ncomplete_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    df = \"df\",\n    MS = \"MS\",\n    F = \"F\",\n    p = \"p\",\n    Var.comp = \"Var. comp.\"\n  ) %&gt;%\n  colformat_double(j = c(\"MS\", \"F\"), digits = 2) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Complete ANOVA table with variance components\") %&gt;%\n  theme_box()\n\nComplete ANOVA table with variance componentsSource of variationdfMSFpVar. comp.Treatment34,809.712.720.091262004152Patches (treatment)121,770.165.93&lt;0.001294Residual64298.60299\n\n\n\n\n\n\n\n\nInterpretation of ANOVA Results\nThe nested ANOVA using mixed models reveals that there was no significant effect of urchin density treatment on algae cover (F = 2.72, df = 3, 12, p = 0.0913). However, there was significant variation among patches within treatments (F = 5.93, df = 12, 64, p &lt; 0.001).\nThe variance component for patches nested within treatments (294) indicates substantial spatial heterogeneity in algae cover, highlighting the importance of accounting for this spatial variation in the analysis. The negative variance component for treatment suggests that there is more variation among patches within treatments than among treatments themselves."
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_slides.html#normality-of-residuals",
    "href": "lectures/lecture_14/14_03_nested_anova_random_slides.html#normality-of-residuals",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Normality of Residuals",
    "text": "Normality of Residuals\n\n# QQ plot of residuals\nqqnorm(resid(mixed_model))\nqqline(resid(mixed_model))\n\n\n\n\n\n\n\n# Histogram of residuals\nhist(resid(mixed_model), main = \"Histogram of Residuals\",\n     xlab = \"Residuals\", breaks = 15)\n\n\n\n\n\n\n\n# More advanced residual diagnostics using DHARMa\nsim_residuals &lt;- simulateResiduals(fittedModel = mixed_model)\nplot(sim_residuals)"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_slides.html#homogeneity-of-variance",
    "href": "lectures/lecture_14/14_03_nested_anova_random_slides.html#homogeneity-of-variance",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Homogeneity of Variance",
    "text": "Homogeneity of Variance\n\n# Residuals vs. fitted values plot\nplot(fitted(mixed_model), resid(mixed_model),\n     xlab = \"Fitted Values\", ylab = \"Residuals\",\n     main = \"Residuals vs. Fitted Values\")\nabline(h = 0, lty = 2, col = \"red\")\n\n\n\n# Levene's test for homogeneity of variance\nlevene_test &lt;- leveneTest(ALGAE ~ TREAT, data = andrew)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value     Pr(&gt;F)    \ngroup  3  8.1694 0.00008785 ***\n      76                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nInterpretation of Assumption Tests\nThe Q-Q plot shows some deviation from normality, particularly in the tails, and Levene’s test indicates significant heterogeneity of variances across treatments (F = 8.17, p &lt; 0.001). As noted in the original analysis, there were “large differences in within-cell variances” in this dataset, and transformations did not improve variance homogeneity.\nThe DHARMa residual diagnostics also indicate potential issues with the distribution of residuals and homogeneity of variance. The residuals vs. fitted plot shows a pattern of increasing variance with increasing fitted values, confirming the heteroscedasticity.\nHowever, mixed models are generally robust to moderate violations of assumptions, especially with balanced designs. Since transformations were not effective in improving the data properties, analyzing the untransformed data is a reasonable approach in this case."
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "",
    "text": "This analysis examines the effects of varying sea urchin densities on the percentage cover of filamentous algae. The experiment was designed with four urchin density treatments (control, 66% of original density, 33% of original density, and all urchins removed) nested within four random patches. Five replicate quadrats were measured within each treatment-patch combination.\nThe traditional nested ANOVA approach can be implemented using a linear mixed-effects model, which provides a more flexible framework for analyzing hierarchical designs. In this case, we’ll use the lme4 package to fit a model where treatment is a fixed effect and patch is a random effect nested within treatment."
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html#model-specification",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html#model-specification",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Model Specification",
    "text": "Model Specification\nWe’ll use the following model specification:\n\\(ALGAE_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}\\)\nWhere: - \\(\\mu\\) is the overall mean - \\(\\alpha_i\\) is the fixed effect of treatment \\(i\\) - \\(\\beta_{j(i)}\\) is the random effect of patch \\(j\\) nested within treatment \\(i\\) - \\(\\epsilon_{ijk}\\) is the residual error for quadrat \\(k\\) in patch \\(j\\) within treatment \\(i\\)\nIn lme4, this model is specified as\n\n# Fit the mixed model\nmixed_model &lt;- lmer(ALGAE ~ TREAT + (1|TREAT:PATCH), data = andrew)\n\n# Display model summary\nsummary(mixed_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ALGAE ~ TREAT + (1 | TREAT:PATCH)\n   Data: andrew\n\nREML criterion at convergence: 682.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9808 -0.3106 -0.1093  0.2831  2.5910 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n TREAT:PATCH (Intercept) 294.3    17.16   \n Residual                298.6    17.28   \nNumber of obs: 80, groups:  TREAT:PATCH, 16\n\nFixed effects:\n                 Estimate Std. Error     df t value Pr(&gt;|t|)  \n(Intercept)         1.300      9.408 12.000   0.138   0.8924  \nTREAT66% Density   20.250     13.305 12.000   1.522   0.1539  \nTREAT33% Density   17.700     13.305 12.000   1.330   0.2081  \nTREATRemoved       37.900     13.305 12.000   2.849   0.0147 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) TREAT6D TREAT3D\nTREAT66%Dns -0.707                \nTREAT33%Dns -0.707  0.500         \nTREATRemovd -0.707  0.500   0.500"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html#anova-table",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html#anova-table",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nThe ANOVA table for the mixed model:\n\n# Get ANOVA table with Type III tests\nanova_table &lt;- anova(mixed_model, type = 3)\nprint(anova_table)\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# For comparison, also run a traditional nested ANOVA\nnested_aov &lt;- aov(ALGAE ~ TREAT + TREAT:PATCH, data = andrew)\nstd_summary &lt;- summary(nested_aov)[[1]]\n\n# Extract MS values - using exact row names\nMS_treat &lt;- std_summary[\"TREAT      \", \"Mean Sq\"] \nMS_patch &lt;- std_summary[\"TREAT:PATCH\", \"Mean Sq\"]\nMS_residual &lt;- std_summary[\"Residuals\", \"Mean Sq\"]\n\n# Print MS values to check\nprint(\"MS values:\")\n\n[1] \"MS values:\"\n\nprint(c(Treatment = MS_treat, Patches = MS_patch, Residual = MS_residual))\n\nTreatment   Patches  Residual \n 4809.712  1770.162   298.600 \n\n# Extract df values\ndf_treat &lt;- std_summary[\"TREAT      \", \"Df\"]\ndf_patch &lt;- std_summary[\"TREAT:PATCH\", \"Df\"]\ndf_residual &lt;- std_summary[\"Residuals\", \"Df\"]\n\n\n# Calculate correct F ratios for nested design\nF_treat &lt;- MS_treat / MS_patch\nF_patch &lt;- MS_patch / MS_residual\n\n# Calculate p-values\np_treat &lt;- pf(F_treat, df_treat, df_patch, lower.tail = FALSE)\np_patch &lt;- pf(F_patch, df_patch, df_residual, lower.tail = FALSE)\n\n# Create ANOVA table\ntrad_anova_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(p_treat, p_patch, NA)\n)\n\n# Format p-values\ntrad_anova_table$p &lt;- ifelse(trad_anova_table$p &lt; 0.001, \"&lt;0.001\",\n                       ifelse(is.na(trad_anova_table$p), NA,\n                              format(trad_anova_table$p, digits = 3)))\ntrad_anova_table\n\n               Source df       MS        F           p\n1           Treatment  3 4809.712 2.717102 0.091262004\n2 Patches (treatment) 12 1770.162 5.928207      &lt;0.001\n3            Residual 64  298.600       NA        &lt;NA&gt;\n\n# # Display traditional ANOVA table with flextable\n# trad_anova_table %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     Source = \"Source of variation\",\n#     df = \"df\",\n#     MS = \"MS\",\n#     F = \"F\",\n#     p = \"p\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"MS\", \"F\"), digits = 2) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"ANOVA table for nested design\") %&gt;%\n#   theme_box()"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html#variance-components",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html#variance-components",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Variance Components",
    "text": "Variance Components\nWe can extract the variance components from the mixed model:\n\n# Print corrected results\n\n# Extract variance components\nvc &lt;- VarCorr(mixed_model)\nprint(vc)\n\n Groups      Name        Std.Dev.\n TREAT:PATCH (Intercept) 17.156  \n Residual                17.280  \n\n# Extract variance components\nvar_comp_patch &lt;- as.numeric(vc$`TREAT:PATCH`)\nvar_comp_residual &lt;- attr(vc, \"sc\")^2\n\n# Calculate percentage of total variance\ntotal_var &lt;- var_comp_patch + var_comp_residual\npct_patch &lt;- var_comp_patch / total_var * 100\npct_residual &lt;- var_comp_residual / total_var * 100\n\n# Calculate treatment variance component\nn_quad &lt;- 5  # Number of quadrats per patch\nn_patch &lt;- 4  # Number of patches per treatment\nvar_comp_treatment &lt;- (MS_treat - MS_patch) / (n_quad * n_patch)\n\n# Format variance components for display\nvar_comp_treatment_display &lt;- ifelse(var_comp_treatment &lt; 0, \n                                    paste0(\"(\", format(abs(var_comp_treatment), digits = 2), \")\"),\n                                    format(var_comp_treatment, digits = 2))\n\n# Create variance components table\nvar_comp_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  `Var.comp` = c(var_comp_treatment_display, \n               format(var_comp_patch, digits = 2),\n               format(var_comp_residual, digits = 2))\n)\n\n# Display variance components table\nvar_comp_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    Var.comp = \"Variance component\"\n  ) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Variance components\") %&gt;%\n  theme_box()\n\nVariance componentsSource of variationVariance componentTreatment152Patches (treatment)294Residual299\n\n# Complete table with all information\ncomplete_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  df = c(df_treat, df_patch, df_residual),\n  MS = c(MS_treat, MS_patch, MS_residual),\n  F = c(F_treat, F_patch, NA),\n  p = c(trad_anova_table$p[1], trad_anova_table$p[2], NA),\n  `Var.comp` = c(var_comp_treatment_display, \n                format(var_comp_patch, digits = 2),\n                format(var_comp_residual, digits = 2))\n)\n\n# Display complete table\ncomplete_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    df = \"df\",\n    MS = \"MS\",\n    F = \"F\",\n    p = \"p\",\n    Var.comp = \"Var. comp.\"\n  ) %&gt;%\n  colformat_double(j = c(\"MS\", \"F\"), digits = 2) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Complete ANOVA table with variance components\") %&gt;%\n  theme_box()\n\nComplete ANOVA table with variance componentsSource of variationdfMSFpVar. comp.Treatment34,809.712.720.091262004152Patches (treatment)121,770.165.93&lt;0.001294Residual64298.60299\n\n\n\n\n\n\n\n\nInterpretation of ANOVA Results\nThe nested ANOVA using mixed models reveals that there was no significant effect of urchin density treatment on algae cover (F = 2.72, df = 3, 12, p = 0.0913). However, there was significant variation among patches within treatments (F = 5.93, df = 12, 64, p &lt; 0.001).\nThe variance component for patches nested within treatments (294) indicates substantial spatial heterogeneity in algae cover, highlighting the importance of accounting for this spatial variation in the analysis. The negative variance component for treatment suggests that there is more variation among patches within treatments than among treatments themselves."
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html#normality-of-residuals",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html#normality-of-residuals",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Normality of Residuals",
    "text": "Normality of Residuals\n\n# QQ plot of residuals\nqqnorm(resid(mixed_model))\nqqline(resid(mixed_model))\n\n\n\n\n\n\n\n# Histogram of residuals\nhist(resid(mixed_model), main = \"Histogram of Residuals\",\n     xlab = \"Residuals\", breaks = 15)\n\n\n\n\n\n\n\n# More advanced residual diagnostics using DHARMa\nsim_residuals &lt;- simulateResiduals(fittedModel = mixed_model)\nplot(sim_residuals)"
  },
  {
    "objectID": "lectures/lecture_14/14_03_nested_anova_random_html.html#homogeneity-of-variance",
    "href": "lectures/lecture_14/14_03_nested_anova_random_html.html#homogeneity-of-variance",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "Homogeneity of Variance",
    "text": "Homogeneity of Variance\n\n# Residuals vs. fitted values plot\nplot(fitted(mixed_model), resid(mixed_model),\n     xlab = \"Fitted Values\", ylab = \"Residuals\",\n     main = \"Residuals vs. Fitted Values\")\nabline(h = 0, lty = 2, col = \"red\")\n\n\n\n\n\n\n\n\n\n# Levene's test for homogeneity of variance\nlevene_test &lt;- leveneTest(ALGAE ~ TREAT, data = andrew)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value     Pr(&gt;F)    \ngroup  3  8.1694 0.00008785 ***\n      76                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\n\n\n\nInterpretation of Assumption Tests\nThe Q-Q plot shows some deviation from normality, particularly in the tails, and Levene’s test indicates significant heterogeneity of variances across treatments (F = 8.17, p &lt; 0.001). As noted in the original analysis, there were “large differences in within-cell variances” in this dataset, and transformations did not improve variance homogeneity.\nThe DHARMa residual diagnostics also indicate potential issues with the distribution of residuals and homogeneity of variance. The residuals vs. fitted plot shows a pattern of increasing variance with increasing fitted values, confirming the heteroscedasticity.\nHowever, mixed models are generally robust to moderate violations of assumptions, especially with balanced designs. Since transformations were not effective in improving the data properties, analyzing the untransformed data is a reasonable approach in this case."
  },
  {
    "objectID": "lectures/lecture_14/14_02_nested_anova_html.html",
    "href": "lectures/lecture_14/14_02_nested_anova_html.html",
    "title": "Lecture 14 - NESTED ANOVA",
    "section": "",
    "text": "Lecture 14: Introduction top a nested design the hard way\nThis analysis examines the effects of varying sea urchin densities on the percentage cover of filamentous algae. The experiment was designed with four urchin density treatments (control, 66% of original density, 33% of original density, and all urchins removed) nested within four random patches. Five replicate quadrats were measured within each treatment-patch combination.\n\n\nLecture 14: Data Overview\nThe dataframe contains r nrow(andrew) observations with the following variables:\n\nTREAT: Urchin density treatment (Control, 66% Density, 33% Density, Removed)\nPATCH: Random patches (1-16) where treatments were applied\nQUAD: Replicate quadrats within each treatment-patch combination\nALGAE: Percentage cover of filamentous algae (response variable)\n\n\n# Create a summary table with flextable\n\nsummary_stats\n\n# A tibble: 4 × 7\n  TREAT           n  mean    sd    se   min   max\n  &lt;fct&gt;       &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Control        20   1.3  3.18 0.711     0    13\n2 66% Density    20  21.6 25.1  5.62      0    79\n3 33% Density    20  19   25.7  5.74      0    71\n4 Removed        20  39.2 28.7  6.41      0    83\n\n# \n# summary_stats %&gt;%\n#   select(TREAT, n, mean, sd, se, min, max) %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     TREAT = \"Treatment\",\n#     n = \"N\",\n#     mean = \"Mean\",\n#     sd = \"SD\",\n#     se = \"SE\",\n#     min = \"Min\",\n#     max = \"Max\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"mean\", \"sd\", \"se\", \"min\", \"max\"), digits = 2) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"Summary statistics of algae cover (%) across treatments\") %&gt;%\n#   theme_box()\n\n\n\nNested ANOVA Analysis\nIn this experimental design, PATCH is nested within TREAT because each patch received only one treatment level. This is a hierarchical design where the effect of patches must be considered within each treatment. Following the approach used in Quinn & Keough (2002), we’ll use a traditional nested ANOVA.\n\nlibrary(lmerTest)\n# Fit the model with treatment as fixed effect and patch nested within treatment as random\nnested_model &lt;- lmer(ALGAE ~ TREAT + (1|TREAT:PATCH), data = andrew,\n                    control = lmerControl(optimizer = \"bobyqa\",\n                                         optCtrl = list(maxfun = 2e5)))\n# BOBYQA (Bound Optimization BY Quadratic Approximation) is an optimization algorithm used in mixed-effects modeling to find the best parameter values that maximize the likelihood function. It's especially useful when fitting complex models like the ones you're working with in your nested ANOVA analysis.\n\n# Model summary\nsummary(nested_model)\n\nLinear mixed model fit by REML. t-tests use Satterthwaite's method [\nlmerModLmerTest]\nFormula: ALGAE ~ TREAT + (1 | TREAT:PATCH)\n   Data: andrew\nControl: lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 200000))\n\nREML criterion at convergence: 682.2\n\nScaled residuals: \n    Min      1Q  Median      3Q     Max \n-1.9808 -0.3106 -0.1093  0.2831  2.5910 \n\nRandom effects:\n Groups      Name        Variance Std.Dev.\n TREAT:PATCH (Intercept) 294.3    17.16   \n Residual                298.6    17.28   \nNumber of obs: 80, groups:  TREAT:PATCH, 16\n\nFixed effects:\n                 Estimate Std. Error     df t value Pr(&gt;|t|)  \n(Intercept)         1.300      9.408 12.000   0.138   0.8924  \nTREAT66% Density   20.250     13.305 12.000   1.522   0.1539  \nTREAT33% Density   17.700     13.305 12.000   1.330   0.2081  \nTREATRemoved       37.900     13.305 12.000   2.849   0.0147 *\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nCorrelation of Fixed Effects:\n            (Intr) TREAT6D TREAT3D\nTREAT66%Dns -0.707                \nTREAT33%Dns -0.707  0.500         \nTREATRemovd -0.707  0.500   0.500 \n\n# Type III ANOVA with F-statistics (not chi-square) using Satterthwaite's method\n# The issue was that you had \"type = F\" which should be \"test.statistic = 'F'\"\nanova_result &lt;- anova(nested_model, type = 3, ddf = \"Satterthwaite\")\nprint(anova_result)\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Alternative using car package\n# The parameter is \"test.statistic\", not \"type\"\nanova_car &lt;- Anova(nested_model, type = 3, test.statistic = \"F\")\nprint(anova_car)\n\nAnalysis of Deviance Table (Type III Wald F tests with Kenward-Roger df)\n\nResponse: ALGAE\n                 F Df Df.res  Pr(&gt;F)  \n(Intercept) 0.0191  1     12 0.89239  \nTREAT       2.7171  3     12 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# You could also try with the simpler model structure\nsimple_model &lt;- lmer(ALGAE ~ TREAT + (1|PATCH), data = andrew)\nanova(simple_model, type = 3, ddf = \"Satterthwaite\")\n\nType III Analysis of Variance Table with Satterthwaite's method\n      Sum Sq Mean Sq NumDF DenDF F value  Pr(&gt;F)  \nTREAT   2434  811.33     3    12  2.7171 0.09126 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n# Method 2\n# Define your model\nmodel &lt;- lm(ALGAE ~ TREAT + PATCH, data = andrew)\nanova_results &lt;- anova(model)\n\n# Calculate F and p values for TREAT using PATCH as error term\nF_treat &lt;- anova_results[\"TREAT\", \"Mean Sq\"] / anova_results[\"PATCH\", \"Mean Sq\"]\np_treat &lt;- pf(F_treat, \n             df1 = anova_results[\"TREAT\", \"Df\"], \n             df2 = anova_results[\"PATCH\", \"Df\"],\n             lower.tail = FALSE)\n\n# Create a custom ANOVA table with the correct error terms\ncustom_anova &lt;- data.frame(\n  Source = c(\"TREAT\", \"PATCH\", \"Residuals\"),\n  Df = anova_results[, \"Df\"],\n  \"Sum Sq\" = anova_results[, \"Sum Sq\"],\n  \"Mean Sq\" = anova_results[, \"Mean Sq\"],\n  \"F value\" = c(F_treat, anova_results[\"PATCH\", \"F value\"], NA),\n  \"Pr(&gt;F)\" = c(p_treat, anova_results[\"PATCH\", \"Pr(&gt;F)\"], NA)\n)\n\ncustom_anova\n\n     Source Df   Sum.Sq  Mean.Sq  F.value          Pr..F.\n1     TREAT  3 14429.14 4809.712 2.717102 0.0912620042021\n2     PATCH 12 21241.95 1770.162 5.928207 0.0000008322613\n3 Residuals 64 19110.40  298.600       NA              NA\n\n\nCalculate Variance Components\n\n# Print corrected results\n\n# Calculate variance components\nn_quad &lt;- 5  # Number of quadrats per patch\nvar_comp_residual &lt;- MS_residual\nvar_comp_patch &lt;- (MS_patch - MS_residual) / n_quad\nvar_comp_treatment &lt;- (MS_treat - MS_patch) / (n_quad * 4)  # 4 patches per treatment\n\n# Format variance components, showing negative values in parentheses\nvar_comp_treatment_display &lt;- ifelse(var_comp_treatment &lt; 0, \n                                    paste0(\"(\", format(abs(var_comp_treatment), digits = 2), \")\"),\n                                    format(var_comp_treatment, digits = 2))\n\n# Create variance components table\nvar_comp_table &lt;- data.frame(\n  Source = c(\"Treatment\", \"Patches (treatment)\", \"Residual\"),\n  Var_comp = c(var_comp_treatment_display, \n               format(var_comp_patch, digits = 2),\n               format(var_comp_residual, digits = 2))\n)\n\nprint(var_comp_table)\n\n               Source Var_comp\n1           Treatment      152\n2 Patches (treatment)      294\n3            Residual      299\n\n\n\n\nLecture 14: ANOVA Results\nThe nested ANOVA model is specified as:\n\\(ALGAE_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}\\)\nWhere: - \\(\\mu\\) is the overall mean - \\(\\alpha_i\\) is the fixed effect of treatment \\(i\\) - \\(\\beta_{j(i)}\\) is the random effect of patch \\(j\\) nested within treatment \\(i\\) - \\(\\epsilon_{ijk}\\) is the residual error for quadrat \\(k\\) in patch \\(j\\) within treatment \\(i\\)\n\n# Display ANOVA results with flextable\nanova_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    df = \"df\",\n    MS = \"MS\",\n    F = \"F\",\n    p = \"p\"\n  ) %&gt;%\n  colformat_double(j = c(\"MS\", \"F\"), digits = 2) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"ANOVA table for nested design\") %&gt;%\n  theme_box()\n\nANOVA table for nested designSource of variationdfMSFpTreatment34,809.712.720.091262004Patches (treatment)121,770.165.93&lt;0.001Residual64298.60\n\n\n\n\nLecture 14: Variance Components\nThe nested ANOVA model is specified as: \\(ALGAE_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\epsilon_{ijk}\\) Where:\n\\(\\mu\\) is the overall mean \\(\\alpha_i\\) is the fixed effect of treatment \\(i\\) \\(\\beta_{j(i)}\\) is the random effect of patch \\(j\\) nested within treatment \\(i\\) \\(\\epsilon_{ijk}\\) is the residual error for quadrat \\(k\\) in patch \\(j\\) within treatment \\(i\\)\n\n# Display variance components with flextable\nvar_comp_table %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    Source = \"Source of variation\",\n    Var_comp = \"Variance component\"\n  ) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Variance components\") %&gt;%\n  theme_box()\n\nVariance componentsSource of variationVariance componentTreatment152Patches (treatment)294Residual299\n\n\n\n\n\n\n\n\nInterpretation of ANOVA Results The nested ANOVA reveals that there was no significant effect of urchin density treatment on algae cover (F = r round(F_treat, 2), df = r df_treat, r df_patch, p = r format(p_treat, digits=3)). However, there was significant variation among patches within treatments (F = r round(F_patch, 2), df = r df_patch, r df_residual, p &lt; 0.001). The variance component for patches nested within treatments (r format(var_comp_patch, digits=2)) indicates substantial spatial heterogeneity in algae cover, highlighting the importance of accounting for this spatial variation in the analysis.\n\n\n\n\n\nLecture 14: Post-hoc Comparisons\nAlthough the main effect of treatment was not significant in the nested ANOVA (p = r format(p_treat, digits=3)), we can still examine the mean differences between treatments to understand patterns in the data. However, we should interpret these with caution given the lack of statistical significance at the α = 0.05 level.\n\n# Calculate estimated marginal means\nemm &lt;- emmeans(nested_model, ~ TREAT)\n\n# Display EMMs with flextable\nas.data.frame(summary(emm)) %&gt;%\n  flextable() %&gt;%\n  set_header_labels(\n    TREAT = \"Treatment\",\n    emmean = \"Estimated Marginal Mean\",\n    SE = \"Standard Error\",\n    df = \"df\",\n    lower.CL = \"Lower CL\",\n    upper.CL = \"Upper CL\"\n  ) %&gt;%\n  colformat_double(j = c(\"emmean\", \"SE\", \"lower.CL\", \"upper.CL\"), digits = 2) %&gt;%\n  autofit() %&gt;%\n  add_header_lines(\"Estimated marginal means for each treatment\") %&gt;%\n  theme_box()\n\nEstimated marginal means for each treatmentTreatmentEstimated Marginal MeanStandard ErrordfLower CLUpper CLControl1.309.4112-19.2021.8066% Density21.559.41121.0542.0533% Density19.009.4112-1.5039.50Removed39.209.411218.7059.70\n\n\n\n\nLecture 14: Tukey Pairwise Comparisons\n\ntext\n\n\n# Pairwise comparisons with Tukey adjustment\npairs &lt;- pairs(emm, adjust = \"tukey\")\npairs\n\n contrast                  estimate   SE df t.ratio p.value\n Control - 66% Density       -20.25 13.3 12  -1.522  0.4553\n Control - 33% Density       -17.70 13.3 12  -1.330  0.5625\n Control - Removed           -37.90 13.3 12  -2.849  0.0615\n 66% Density - 33% Density     2.55 13.3 12   0.192  0.9974\n 66% Density - Removed       -17.65 13.3 12  -1.327  0.5646\n 33% Density - Removed       -20.20 13.3 12  -1.518  0.4573\n\nDegrees-of-freedom method: kenward-roger \nP value adjustment: tukey method for comparing a family of 4 estimates \n\n# # Display pairwise comparisons with flextable\n# as.data.frame(summary(pairs)) %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     contrast = \"Contrast\",\n#     estimate = \"Estimate\",\n#     SE = \"Standard Error\",\n#     df = \"df\",\n#     t.ratio = \"t ratio\",\n#     p.value = \"p-value\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"estimate\", \"SE\", \"t.ratio\", \"p.value\"), digits = 3) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"Pairwise comparisons between treatments (Tukey-adjusted)\") %&gt;%\n#   theme_box()\n\n\n\nLecture 14: Letter Display\n\n# Extract compact letter display for plotting\ncld &lt;- multcomp::cld(emm, alpha = 0.05, Letters = letters)\n\ncld\n\n TREAT       emmean   SE df lower.CL upper.CL .group\n Control        1.3 9.41 12   -19.20     21.8  a    \n 33% Density   19.0 9.41 12    -1.50     39.5  a    \n 66% Density   21.6 9.41 12     1.05     42.0  a    \n Removed       39.2 9.41 12    18.70     59.7  a    \n\nDegrees-of-freedom method: kenward-roger \nConfidence level used: 0.95 \nP value adjustment: tukey method for comparing a family of 4 estimates \nsignificance level used: alpha = 0.05 \nNOTE: If two or more means share the same grouping symbol,\n      then we cannot show them to be different.\n      But we also did not show them to be the same. \n\n# # Display CLD with flextable\n# as.data.frame(cld) %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     TREAT = \"Treatment\",\n#     emmean = \"Estimated Marginal Mean\",\n#     SE = \"Standard Error\",\n#     df = \"df\",\n#     lower.CL = \"Lower CL\",\n#     upper.CL = \"Upper CL\",\n#     .group = \"Group\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"emmean\", \"SE\", \"lower.CL\", \"upper.CL\"), digits = 2) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"Compact letter display of treatment means\") %&gt;%\n#   theme_box()\n\n\n\n\n\n\n\nInterpretation of Treatment Comparisons The mean algae cover for the Control treatment (1.30%) appears considerably lower than for the reduced urchin density treatments (66% Density: 21.55%, 33% Density: 19.00%, Removed: 39.20%). While the visual pattern suggests an inverse relationship between urchin density and algae cover, with complete removal showing the highest algae cover, the nested ANOVA showed that these differences were not statistically significant at the α = 0.05 level (p = r format(p_treat, digits=3)). The high variability among patches within treatments likely contributed to the lack of statistical significance for the treatment effect.\n\n\n\n\n\nLecture 14: ANOVA Assumptions Testing\nFor valid inference from ANOVA, several assumptions must be met. We test these assumptions below.\n\n# Extract residuals\nresiduals &lt;- residuals(nested_model)\n\n# QQ plot\nqq_plot &lt;- ggplot(data.frame(residuals = residuals), aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line() +\n  # theme_cowplot() +\n  labs(title = \"Normal Q-Q Plot of Residuals\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\")\n\n# Histogram of residuals\nhist_plot &lt;- ggplot(data.frame(residuals = residuals), aes(x = residuals)) +\n  geom_histogram(bins = 15, fill = \"lightblue\", color = \"black\") +\n  # theme_cowplot() +\n  labs(title = \"Histogram of Residuals\",\n       x = \"Residuals\",\n       y = \"Frequency\")\n\n# Residuals vs. Fitted plot\nfitted_values &lt;- fitted(nested_model)\nresid_plot &lt;- ggplot(data.frame(fitted = fitted_values, residuals = residuals), \n                    aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\", color = \"red\") +\n  # theme_cowplot() +\n  labs(title = \"Residuals vs. Fitted Values\",\n       x = \"Fitted Values\",\n       y = \"Residuals\")\n\n# Combine plots\nqq_plot + hist_plot + resid_plot + plot_layout(ncol = 3)\n\n\n\n\n\n\n\n\n\n\nLecture 14: Levenes Test for Homogeneity of Variance\n\n# 2. Homogeneity of Variance\n# Levene's test\n# Levene's test for homogeneity of variance\nlevene_test &lt;- leveneTest(ALGAE ~ TREAT, data = andrew)\nlevene_test\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value     Pr(&gt;F)    \ngroup  3  8.1694 0.00008785 ***\n      76                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# # Display results with flextable\n# data.frame(\n#   Statistic = c(levene_test$`F value`[1]),\n#   df1 = c(levene_test$Df[1]),\n#   df2 = c(levene_test$Df[2]),\n#   p.value = c(levene_test$`Pr(&gt;F)`[1])\n# ) %&gt;%\n#   flextable() %&gt;%\n#   set_header_labels(\n#     Statistic = \"F value\",\n#     df1 = \"df1\",\n#     df2 = \"df2\",\n#     p.value = \"p-value\"\n#   ) %&gt;%\n#   colformat_double(j = c(\"Statistic\", \"p.value\"), digits = 3) %&gt;%\n#   autofit() %&gt;%\n#   add_header_lines(\"Levene's Test for Homogeneity of Variance\") %&gt;%\n#   theme_box()\n\n\n\n\n\n\n\nInterpretation of Assumption Tests The Q-Q plot shows some deviation from normality, particularly in the tails, and Levene’s test indicates significant heterogeneity of variances across treatments (F = r round(levene_test$“F value”[1], 2), p &lt; 0.001). As noted by Quinn & Keough (2002), there were “large differences in within-cell variances” in this dataset, and transformations (including arcsin) did not improve variance homogeneity. However, ANOVA is generally robust to heteroscedasticity with balanced designs, which is why they chose to analyze untransformed data. The residuals vs. fitted plot also shows a pattern of increasing variance with increasing fitted values, confirming the heteroscedasticity.\n\n\n\n\n\nLecture 14: Visualization\n\n# Create boxplot\nggplot_boxplot &lt;- ggplot(andrew, aes(x = TREAT, y = ALGAE, fill = TREAT)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_jitter(width = 0.2, alpha = 0.4, size = 1) +\n  scale_fill_viridis_d(option = \"D\", end = 0.85) +\n  labs(\n    title = \"Effect of Urchin Density on Filamentous Algae Cover\",\n    x = \"Urchin Density Treatment\",\n    y = \"Filamentous Algae Cover (%)\",\n    caption = \"Figure 1: Boxplots showing the distribution of algal cover across urchin density treatments.\\nDespite visual differences, the treatment effect was not statistically significant (p = 0.091).\"\n  ) +\n  # theme_cowplot() +\n  theme(\n    legend.position = \"none\",\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    plot.caption = element_text(hjust = 0, face = \"italic\", size = 10)\n  )\n\nprint(ggplot_boxplot)\n\n\n\n\n\n\n\n\n\n\nLecture 14: Means Plot\n\ntext\n\n\n# Create means plot\nmeans_plot &lt;- ggplot(summary_stats, aes(x = TREAT, y = mean, group = 1)) +\n  # geom_line(size = 1) +\n  geom_point(size = 3, shape = 21, fill = \"white\") +\n  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2) +\n  labs(\n    title = \"Mean Algae Cover by Urchin Density Treatment\",\n    x = \"Urchin Density Treatment\",\n    y = \"Mean Filamentous Algae Cover (%)\",\n    caption = \"Figure 2: Mean (± SE) percentage cover of filamentous algae across urchin density treatments.\"\n  ) +\n  # theme_cowplot() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    axis.title = element_text(face = \"bold\", size = 12),\n    axis.text = element_text(size = 10),\n    plot.caption = element_text(hjust = 0, face = \"italic\", size = 10)\n  )\n\nprint(means_plot)\n\n\n\n\n\n\n\n\n\n\nLecture 14: Discussion\n\n\n\n\n\n\nScientific Interpretation Our nested ANOVA analysis revealed substantial spatial heterogeneity in algae cover, with significant variation among patches within each treatment (p &lt; 0.001). Surprisingly, the effect of urchin density treatments on filamentous algae cover was not statistically significant at the α = 0.05 level (p = 0.091), despite apparent trends in the data. The descriptive statistics show a pattern where algae cover appears to increase as urchin density decreases, with the Control treatment (mean = 1.3%) showing minimal algae cover compared to reduced density treatments (66% Density: 21.55%, 33% Density: 19.00%, and Removed: 39.20%). This pattern suggests a potential density-dependent relationship between urchin grazing and algal abundance, but the high variability among patches masked the treatment effect. The substantial variance component associated with patches nested within treatments (294.31, approximately 39.5% of total variance) underscores the importance of spatial heterogeneity in structuring algal communities. This finding highlights the necessity of accounting for spatial variability when designing and analyzing ecological field experiments. From an ecological perspective, these results suggest that while sea urchins may influence algal communities through grazing, local environmental factors and patch-specific conditions play a dominant role in determining algae cover. This has important implications for ecosystem management, as it indicates that the effects of urchin density manipulations may be context-dependent and influenced by local environmental conditions.\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#lecture-14-factorial-anova-8",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#lecture-14-factorial-anova-8",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Lecture 14: Factorial ANOVA",
    "text": "Lecture 14: Factorial ANOVA"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\n\\(y_{ijk}\\): value of the kth observation from jth and ith combination of B and A (fecundity on 2nd plate, in “8 per plate” density in summer)\nµ: overall mean (overall fecundity)\nαi: effect of the ith level of A, pooling across all levels of B: µi- µ (difference between average fecundity in all “8 per plate” treatments and overall mean)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\nΒj: effect of jth level of B, pooling across all levels of A: µj- µ (difference between average fecundity in all winter treatments and overall mean)\n(αβ)ij: effect of interaction of ith level of A and jth level of B (µij - µi - µj + µ).\n\nDoes effect of B depend on level of A? (is effect of density different in winter and summer?)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-3",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-3",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\nModel 2 ANOVA rare in ecology\nModel 3 interpretation is different:\n\nβj: random variable measuring variance in y across all possible levels of B, pooling across all levels of A\n(αβ)ij is random variable measuring variance of interaction bw A and B across all possible levels of B (“is effect of A consistent across all possible levels of B that could have been chosen?”)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#anova-assumptions",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#anova-assumptions",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "ANOVA Assumptions",
    "text": "ANOVA Assumptions\nBefore conducting the factorial ANOVA, we need to check several assumptions:\n\nIndependence of observations\nNormality of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-normality-of-residuals-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#check-for-homogeneity-of-variances-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#to-get-the-polynomial-and-quadratic-contrasts",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#to-get-the-polynomial-and-quadratic-contrasts",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "To get the polynomial and quadratic contrasts",
    "text": "To get the polynomial and quadratic contrasts"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-3",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-3",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-4",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-4",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-5",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-5",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-6",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#estimated-marginal-means-and-effects-6",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_slides.html#this-is-a-plot-you-might-produce-for-publication",
    "href": "lectures/lecture_13/13_01_factorial_anova_slides.html#this-is-a-plot-you-might-produce-for-publication",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "This is a plot you might produce for publication",
    "text": "This is a plot you might produce for publication"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "",
    "text": "ANOVA\n\nAnalysis of variance: single and multi-factor designs\nExamples: diatoms, circadian rhythms\nPredictor variables: fixed vs. random\nANOVA model\nAnalysis and partitioning of variance\nNull hypothesis\nAssumptions and diagnostics\nPost F Tests - Tukey and others\nReporting the results"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_ji-varepsilon_ijk",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_{j(i)} + \\varepsilon_{ijk}\\]"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#lecture-14-factorial-anova-8",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#lecture-14-factorial-anova-8",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Lecture 14: Factorial ANOVA",
    "text": "Lecture 14: Factorial ANOVA"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\n\\(y_{ijk}\\): value of the kth observation from jth and ith combination of B and A (fecundity on 2nd plate, in “8 per plate” density in summer)\nµ: overall mean (overall fecundity)\nαi: effect of the ith level of A, pooling across all levels of B: µi- µ (difference between average fecundity in all “8 per plate” treatments and overall mean)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\nΒj: effect of jth level of B, pooling across all levels of A: µj- µ (difference between average fecundity in all winter treatments and overall mean)\n(αβ)ij: effect of interaction of ith level of A and jth level of B (µij - µi - µj + µ).\n\nDoes effect of B depend on level of A? (is effect of density different in winter and summer?)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-3",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#y_ijk-mu-alpha_i-beta_j-alphabeta_ij-varepsilon_ijk-3",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]",
    "text": "\\[y_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\varepsilon_{ijk}\\]\n\nModel 2 ANOVA rare in ecology\nModel 3 interpretation is different:\n\nβj: random variable measuring variance in y across all possible levels of B, pooling across all levels of A\n(αβ)ij is random variable measuring variance of interaction bw A and B across all possible levels of B (“is effect of A consistent across all possible levels of B that could have been chosen?”)"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#anova-assumptions",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#anova-assumptions",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "ANOVA Assumptions",
    "text": "ANOVA Assumptions\nBefore conducting the factorial ANOVA, we need to check several assumptions:\n\nIndependence of observations\nNormality of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-normality-of-residuals-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for Normality of Residuals",
    "text": "Check for Normality of Residuals"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#check-for-homogeneity-of-variances-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Check for homogeneity of variances",
    "text": "Check for homogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#to-get-the-polynomial-and-quadratic-contrasts",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#to-get-the-polynomial-and-quadratic-contrasts",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "To get the polynomial and quadratic contrasts",
    "text": "To get the polynomial and quadratic contrasts"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-1",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-1",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-2",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-2",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-3",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-3",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-4",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-4",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-5",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-5",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-6",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#estimated-marginal-means-and-effects-6",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "Estimated Marginal Means and Effects",
    "text": "Estimated Marginal Means and Effects"
  },
  {
    "objectID": "lectures/lecture_13/13_01_factorial_anova_html.html#this-is-a-plot-you-might-produce-for-publication",
    "href": "lectures/lecture_13/13_01_factorial_anova_html.html#this-is-a-plot-you-might-produce-for-publication",
    "title": "Lecture 13 - Factorial ANOVA of Limpet Egg Production",
    "section": "This is a plot you might produce for publication",
    "text": "This is a plot you might produce for publication"
  },
  {
    "objectID": "lectures/lecture_12/12_04_how_anova_is_regression_html.html",
    "href": "lectures/lecture_12/12_04_how_anova_is_regression_html.html",
    "title": "How an ANOVA IS A REGRESSION WIHT DUMMY VARIABLES",
    "section": "",
    "text": "Introduction\nThis document demonstrates how Analysis of Variance (ANOVA) is mathematically equivalent to a regression model with dummy variables using an example with R code and visualizations.\n\n\nSetup and Data Creation\nLet’s begin by loading necessary packages and creating a dataframe about plant heights with three different fertilizer treatments.\n\n# install.packages(\"flextable\")\nlibrary(tidyverse)\nlibrary(flextable)\n\n# Create the dataset\nfertilizer_data &lt;- tibble(\n  fertilizer = rep(c(\"A\", \"B\", \"C\"), each = 3),\n  height = c(10, 12, 8,   # Fertilizer A\n             14, 16, 18,  # Fertilizer B\n             20, 22, 24)  # Fertilizer C\n)\n\n# Display the dataset using flextable\nflextable(fertilizer_data) %&gt;%\n  set_caption(\"Plant Heights by Fertilizer Type\") %&gt;%\n  theme_vanilla() %&gt;%\n  autofit()\n\nfertilizerheightA10A12A8B14B16B18C20C22C24\n\n\n\n\nCalculating Group Means (ANOVA Approach)\nIn ANOVA, we calculate the mean of each group and compare variation between groups to variation within groups.\n\ngroup_means &lt;- fertilizer_data %&gt;%\n  group_by(fertilizer) %&gt;%\n  summarize(mean_height = mean(height))\n\nflextable(group_means) %&gt;%\n  set_caption(\"Group Means (ANOVA Approach)\") %&gt;%\n  theme_vanilla() %&gt;%\n  autofit()\n\nfertilizermean_heightA10B16C22\n\n\nLet’s visualize the raw data and group means:\n\nggplot(fertilizer_data, aes(x = fertilizer, y = height)) +\n  geom_jitter(width = 0.1, alpha = 0.6) +\n  geom_point(data = group_means, aes(y = mean_height), \n             color = \"red\", size = 3) +\n  labs(title = \"Plant Height by Fertilizer Type\",\n       x = \"Fertilizer Type\",\n       y = \"Plant Height (cm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nRunning the ANOVA\n\n# Run ANOVA\nanova_model &lt;- aov(height ~ fertilizer, data = fertilizer_data)\nanova_summary &lt;- summary(anova_model)\nanova_summary\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)   \nfertilizer   2    216     108      27  0.001 **\nResiduals    6     24       4                  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\nRegression with Dummy Variables\nFor the regression approach, we’ll create dummy variables for fertilizer types, using fertilizer A as the reference level.\n\n# Set fertilizer A as the reference level\nfertilizer_data$fertilizer &lt;- factor(fertilizer_data$fertilizer, levels = c(\"A\", \"B\", \"C\"))\n\n# Run regression with dummy variables\nreg_model &lt;- lm(height ~ fertilizer, data = fertilizer_data)\nreg_summary &lt;- summary(reg_model)\nreg_summary\n\n\nCall:\nlm(formula = height ~ fertilizer, data = fertilizer_data)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n    -2     -2      0      2      2 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   10.000      1.155   8.660 0.000131 ***\nfertilizerB    6.000      1.633   3.674 0.010402 *  \nfertilizerC   12.000      1.633   7.348 0.000325 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2 on 6 degrees of freedom\nMultiple R-squared:    0.9, Adjusted R-squared:  0.8667 \nF-statistic:    27 on 2 and 6 DF,  p-value: 0.001\n\n\n\n\nUnderstanding the Regression Coefficients\nIn our regression model:\n\nThe intercept (10) is equal to the mean of the reference group (A)\nThe coefficient for fertilizer B (6) is the difference between mean of group B and mean of group A\nThe coefficient for fertilizer C (12) is the difference between mean of group C and mean of group A\n\n\n# Create a table showing the relationship between coefficients and means\ncoefs &lt;- coef(reg_model)\n\ncoefficients_explained &lt;- tibble(\n  Term = c(\"Intercept\", \"fertilizerB\", \"fertilizerC\"),\n  Coefficient = coefs,\n  Meaning = c(\n    \"Mean of Group A (reference group)\",\n    \"Difference between Group B and Group A means\",\n    \"Difference between Group C and Group A means\"\n  ),\n  Mathematical_Expression = c(\n    \"β₀ = μA\",\n    \"β₁ = μB - μA\",\n    \"β₂ = μC - μA\"\n  ),\n  Numeric_Value = c(coefs[1],\n    paste0(round(group_means$mean_height[2], 1), \" - \", \n           round(group_means$mean_height[1], 1), \" = \", \n           round(coefs[2], 1)),\n    paste0(round(group_means$mean_height[3], 1), \" - \", \n           round(group_means$mean_height[1], 1), \" = \", \n           round(coefs[3], 1))))\n\n# Use flextable to format the table\nflextable(coefficients_explained) %&gt;%\n  set_caption(\"Regression Coefficients Explained\") %&gt;%\n  theme_vanilla() %&gt;%\n  fit_to_width(max_width = 8, unit = \"in\") %&gt;%\n  bold(j = 1) %&gt;%\n  colformat_double(j = 2, digits = 2)\n\nTermCoefficientMeaningMathematical_ExpressionNumeric_ValueIntercept10.00Mean of Group A (reference group)β₀ = μA10fertilizerB6.00Difference between Group B and Group A meansβ₁ = μB - μA16 - 10 = 6fertilizerC12.00Difference between Group C and Group A meansβ₂ = μC - μA22 - 10 = 12\n\n\nLet’s visualize these coefficients:\n\ncoef_data &lt;- tibble(\n  Term = factor(c(\"Intercept\\n(Group A Mean)\", \"Group B - Group A\", \"Group C - Group A\"),\n                levels = c(\"Intercept\\n(Group A Mean)\", \"Group B - Group A\", \"Group C - Group A\")),\n  Value = c(coefs[1], coefs[2], coefs[3])\n)\n\nggplot(coef_data, aes(x = Term, y = Value)) +\n  geom_col(fill = \"steelblue\") +\n  geom_text(aes(label = round(Value, 1)), vjust = -0.5) +\n  labs(title = \"Regression Coefficients with Dummy Variables\",\n       subtitle = \"Intercept represents Group A mean; other coefficients show differences from reference\",\n       x = \"\",\n       y = \"Coefficient Value (cm)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nDemonstrating the Equivalence\nNow, let’s prove that the regression model predictions are identical to the ANOVA group means:\n\n# Get predictions from regression model\npredicted_values &lt;- predict(reg_model, fertilizer_data)\n\n# Create a dataframe for comparison\ncomparison_data &lt;- fertilizer_data %&gt;%\n  mutate(predicted = predicted_values) %&gt;%\n  group_by(fertilizer) %&gt;%\n  mutate(group_mean = mean(height))\n\n# Generate the predicted values for each group\npredicted_values_by_group &lt;- comparison_data %&gt;%\n  group_by(fertilizer) %&gt;%\n  reframe(\n    anova_mean = mean(height),\n    regression_prediction = mean(predicted),\n    formula = case_when(\n      fertilizer == \"A\" ~ paste0(round(coefs[1], 1), \" + 0 + 0 = \", round(coefs[1], 1)),\n      fertilizer == \"B\" ~ paste0(round(coefs[1], 1), \" + \", round(coefs[2], 1), \" + 0 = \", round(coefs[1] + coefs[2], 1)),\n      fertilizer == \"C\" ~ paste0(round(coefs[1], 1), \" + 0 + \", round(coefs[3], 1), \" = \", round(coefs[1] + coefs[3], 1))\n    )\n  )\n\nLet’s visualize this equivalence:\n\n# Create data for plotting the equivalence\nplot_data &lt;- predicted_values_by_group %&gt;%\n  pivot_longer(cols = c(anova_mean, regression_prediction),\n               names_to = \"method\",\n               values_to = \"value\") %&gt;%\n  mutate(method = ifelse(method == \"anova_mean\", \"ANOVA Group Mean\", \"Regression Prediction\"))\n\nggplot(plot_data, aes(x = fertilizer, y = value, fill = method)) +\n  geom_bar(stat = \"identity\", position = position_dodge(), alpha = 0.7) +\n  geom_text(aes(label = round(value, 1)), position = position_dodge(width = 0.9), vjust = -0.5) +\n  labs(title = \"ANOVA Mean vs. Regression Prediction by Fertilizer Type\",\n       subtitle = \"Both methods produce identical values\",\n       x = \"Fertilizer Type\",\n       y = \"Plant Height (cm)\",\n       fill = \"Method\") +\n  theme_minimal() +\n  scale_fill_brewer(palette = \"Set1\")\n\n\n\n\n\n\n\n\n\n\nComparing Statistical Tests\nBoth ANOVA and regression provide an F-test. Let’s compare them:\n\n# ANOVA: Extract F-value and p-value\nanova_f &lt;- anova_summary[[1]]$`F value`[1]\nanova_p &lt;- anova_summary[[1]]$`Pr(&gt;F)`[1]\n\n# Regression: Extract F-value and p-value\nreg_f &lt;- reg_summary$fstatistic[1]\nreg_p &lt;- pf(reg_f, reg_summary$fstatistic[2], reg_summary$fstatistic[3], lower.tail = FALSE)\n\n# Compare them\ntest_comparison &lt;- tibble(\n  Test = c(\"ANOVA F-test\", \"Regression F-test\"),\n  `F-value` = c(anova_f, reg_f),\n  `p-value` = c(anova_p, reg_p)\n)\n\n# Format with flextable\nflextable(test_comparison) %&gt;%\n  set_caption(\"Comparison of Statistical Tests\") %&gt;%\n  theme_vanilla() %&gt;%\n  autofit() %&gt;%\n  colformat_double(j = 2:3, digits = 4)\n\nTestF-valuep-valueANOVA F-test27.00000.0010Regression F-test27.00000.0010\n\n\n\n\nThe Mathematical Relationship\nFor a one-way ANOVA with a categorical variable having k levels, we can express the relationship with regression as:\n\\[Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\beta_{k-1}X_{k-1} + \\epsilon\\]\nWhere: - \\(\\beta_0\\) is the mean of the reference group - \\(\\beta_1, \\beta_2, ..., \\beta_{k-1}\\) are the differences between each group’s mean and the reference group mean - \\(X_1, X_2, ..., X_{k-1}\\) are dummy variables (0 or 1)\nIn our example: - \\(\\beta_0 = 10\\) (mean of group A) - \\(\\beta_1 = 6\\) (difference between B and A) - \\(\\beta_2 = 12\\) (difference between C and A)\n\n\nConclusion\nThis demonstration shows that one-way ANOVA is mathematically equivalent to regression with dummy variables. The key equivalences are:\n\nANOVA group means = Regression predictions for each group\nF-statistic from ANOVA = F-statistic from regression\np-values are identical in both approaches\n\nThis confirms that both techniques are special cases of the General Linear Model, just expressed in different ways. For a categorical predictor with k levels, we need k-1 dummy variables in the regression approach, with one level serving as the reference category.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_08/08_01_lecture_powerpoint_slides.html#textpower-propto-fraces-alpha-sqrtnsigma",
    "href": "lectures/lecture_08/08_01_lecture_powerpoint_slides.html#textpower-propto-fraces-alpha-sqrtnsigma",
    "title": "Lecture 08",
    "section": "\\[\\text{Power} \\propto \\frac{ES \\alpha \\sqrt{n}}{\\sigma}\\]",
    "text": "\\[\\text{Power} \\propto \\frac{ES \\alpha \\sqrt{n}}{\\sigma}\\]"
  },
  {
    "objectID": "lectures/lecture_08/08_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_08/08_01_lecture_powerpoint_html.html",
    "title": "Lecture 08",
    "section": "",
    "text": "Covered\n\nWhat are the assumptions again and how do you assess them\nWhat to do when assumptions fail\n\nRobust tests\nRank-based tests\nPermutation tests"
  },
  {
    "objectID": "lectures/lecture_08/08_01_lecture_powerpoint_html.html#textpower-propto-fraces-alpha-sqrtnsigma",
    "href": "lectures/lecture_08/08_01_lecture_powerpoint_html.html#textpower-propto-fraces-alpha-sqrtnsigma",
    "title": "Lecture 08",
    "section": "\\[\\text{Power} \\propto \\frac{ES \\alpha \\sqrt{n}}{\\sigma}\\]",
    "text": "\\[\\text{Power} \\propto \\frac{ES \\alpha \\sqrt{n}}{\\sigma}\\]"
  },
  {
    "objectID": "lectures/lecture_06/05_02_class_activity.html",
    "href": "lectures/lecture_06/05_02_class_activity.html",
    "title": "06_Class_Activity",
    "section": "",
    "text": "Understanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution\n\n\n\n\n\nReview more r code\nunderstand α alpha and β beta errors\ndo more\n\n1 sample t tests\n2 sample t tests"
  },
  {
    "objectID": "lectures/lecture_06/05_02_class_activity.html#what-did-we-do-last-time-in-activity-5",
    "href": "lectures/lecture_06/05_02_class_activity.html#what-did-we-do-last-time-in-activity-5",
    "title": "06_Class_Activity",
    "section": "",
    "text": "Understanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution"
  },
  {
    "objectID": "lectures/lecture_06/05_02_class_activity.html#todays-focus",
    "href": "lectures/lecture_06/05_02_class_activity.html#todays-focus",
    "title": "06_Class_Activity",
    "section": "",
    "text": "Review more r code\nunderstand α alpha and β beta errors\ndo more\n\n1 sample t tests\n2 sample t tests"
  },
  {
    "objectID": "lectures/lecture_06/05_02_class_activity.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "href": "lectures/lecture_06/05_02_class_activity.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "title": "06_Class_Activity",
    "section": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)",
    "text": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwhere:\n\nx̄₁ and x̄₂: These represent the sample means of the two groups you’re comparing \ns²ₚ: This is the pooled variance, calculated as: s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups.\nn₁ and n₂: These are the sample sizes of the two groups.\n√(1/n₁ + 1/n₂): This represents the pooled standard error."
  },
  {
    "objectID": "lectures/lecture_06/05_02_class_activity.html#your-task-is-to-make-this-plot",
    "href": "lectures/lecture_06/05_02_class_activity.html#your-task-is-to-make-this-plot",
    "title": "06_Class_Activity",
    "section": "your Task is to make this plot",
    "text": "your Task is to make this plot\n\npine_mean_se &lt;- pine_data %&gt;% \n  ggplot(aes(wind, length_mm, color = wind))+\n  stat_summary(fun = \"mean\", na.rm=TRUE, geom=\"point\", size = 3)+\n  stat_summary(fun.data = \"mean_se\", width = 0.2, geom = \"errorbar\")\n\npine_mean_se"
  },
  {
    "objectID": "lectures/lecture_01/01_03_homework_html.html",
    "href": "lectures/lecture_01/01_03_homework_html.html",
    "title": "01_Homework",
    "section": "",
    "text": "This is an assignment for you to practice the code and all the work we do in class on a different dataframe. We will assign one of these each week for you to do that encompasses all fo the work we do. We give it out on Monday so you can start early and see how this work."
  },
  {
    "objectID": "lectures/lecture_01/01_03_homework_html.html#what-to-turn-in--",
    "href": "lectures/lecture_01/01_03_homework_html.html#what-to-turn-in--",
    "title": "01_Homework",
    "section": "What to turn in -",
    "text": "What to turn in -\n\na zipped or compressed folder of the entire project folder\na self-contained html file showing the output\nannotations in the quarto file that shows or tells what is being done like in the class activities"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#testing-assumptions-of-parametric-tests",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#testing-assumptions-of-parametric-tests",
    "title": "Lecture 07",
    "section": "Testing Assumptions of Parametric Tests",
    "text": "Testing Assumptions of Parametric Tests\nKey Assumptions\n\nRandom sampling: Samples are randomly collected from populations\nNormality: Data follows a normal distribution\nEqual variance: Samples come from populations with similar variability\nNo outliers: No extreme values that can skew results\n\nAssessing Assumptions\n\nKey to do every time\nShould acknowledge in manuscript"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#data-transformations",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#data-transformations",
    "title": "Lecture 07",
    "section": "Data Transformations",
    "text": "Data Transformations\nWhen assumptions aren’t met, transformations may help normalize data:\n\nLog transformation: log10(x) - Useful for right-skewed data, multiplicative effects\nSquare root: sqrt(x) - Useful for count data, moderately right-skewed distributions\nBox-Cox: More flexible family of power transformations\nMore specialized transformations especially for percentages o proportions"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#statistical-test-options",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#statistical-test-options",
    "title": "Lecture 07",
    "section": "Statistical Test Options",
    "text": "Statistical Test Options\n1. Standard T-Test\nStrengths: - High statistical power when assumptions are met - Well understood and widely accepted\nWeaknesses: - Sensitive to violations of normality, equal variance - Heavily influenced by outliers\n2. Welch’s T-Test\nStrengths: - Robust to violations of equal variance assumption - Handles unequal sample sizes well - Still parametric (assumes normality)\nWeaknesses: - Slightly less powerful than standard t-test when variances are equal - Still assumes normal distribution\n3. Mann-Whitney-Wilcoxon Test\nStrengths: - Non-parametric: doesn’t assume normal distribution - Robust against outliers - Works with ordinal data\nWeaknesses: - Less statistical power than parametric tests - Still assumes similar distributions and approximate equal variance - Tests median differences rather than mean differences\n4. Permutation Tests\nStrengths: - Distribution-free: doesn’t assume a specific distribution - Can be applied to many types of test statistics - Handles small sample sizes well - Directly estimates p-values through resampling\nWeaknesses: - Computationally intensive - Assumes exchangeability under the null hypothesis - Requires similar distributions and equal variance"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#key-takeaway",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_slides.html#key-takeaway",
    "title": "Lecture 07",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nStatistical tests have different strengths and assumptions. The choice should be guided by your data characteristics, not just convenience. Always visualize your data before deciding on the appropriate test."
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_html.html",
    "title": "Lecture 07",
    "section": "",
    "text": "Hypotheses\n1- and 2-sided T tests\nAssumptions of parametric tests\nWhat next"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#testing-assumptions-of-parametric-tests",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#testing-assumptions-of-parametric-tests",
    "title": "Lecture 07",
    "section": "Testing Assumptions of Parametric Tests",
    "text": "Testing Assumptions of Parametric Tests\n\nKey Assumptions\n\nRandom sampling: Samples are randomly collected from populations\nNormality: Data follows a normal distribution\nEqual variance: Samples come from populations with similar variability\nNo outliers: No extreme values that can skew results\n\n\n\nAssessing Assumptions\n\nKey to do every time\nShould acknowledge in manuscript"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#data-transformations",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#data-transformations",
    "title": "Lecture 07",
    "section": "Data Transformations",
    "text": "Data Transformations\nWhen assumptions aren’t met, transformations may help normalize data:\n\nLog transformation: log10(x) - Useful for right-skewed data, multiplicative effects\nSquare root: sqrt(x) - Useful for count data, moderately right-skewed distributions\nBox-Cox: More flexible family of power transformations\nMore specialized transformations especially for percentages o proportions"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#statistical-test-options",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#statistical-test-options",
    "title": "Lecture 07",
    "section": "Statistical Test Options",
    "text": "Statistical Test Options\n\n1. Standard T-Test\nStrengths: - High statistical power when assumptions are met - Well understood and widely accepted\nWeaknesses: - Sensitive to violations of normality, equal variance - Heavily influenced by outliers\n\n\n2. Welch’s T-Test\nStrengths: - Robust to violations of equal variance assumption - Handles unequal sample sizes well - Still parametric (assumes normality)\nWeaknesses: - Slightly less powerful than standard t-test when variances are equal - Still assumes normal distribution\n\n\n3. Mann-Whitney-Wilcoxon Test\nStrengths: - Non-parametric: doesn’t assume normal distribution - Robust against outliers - Works with ordinal data\nWeaknesses: - Less statistical power than parametric tests - Still assumes similar distributions and approximate equal variance - Tests median differences rather than mean differences\n\n\n4. Permutation Tests\nStrengths: - Distribution-free: doesn’t assume a specific distribution - Can be applied to many types of test statistics - Handles small sample sizes well - Directly estimates p-values through resampling\nWeaknesses: - Computationally intensive - Assumes exchangeability under the null hypothesis - Requires similar distributions and equal variance"
  },
  {
    "objectID": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#key-takeaway",
    "href": "lectures/lecture_07/07_01_lecture_powerpoint_html.html#key-takeaway",
    "title": "Lecture 07",
    "section": "Key Takeaway",
    "text": "Key Takeaway\nStatistical tests have different strengths and assumptions. The choice should be guided by your data characteristics, not just convenience. Always visualize your data before deciding on the appropriate test."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 09",
    "section": "The objectives:",
    "text": "The objectives:\nThis lecture covers two fundamental statistical techniques in biology: correlation and regression analysis. Based on Chapters 16-17 from Whitlock & Schluter’s The Analysis of Biological Data (3rd edition), we’ll explore:\n\nCorrelation analysis: measuring relationships between variables\nThe distinction between correlation and regression\nSimple linear regression: predicting one variable from another\nEstimating and interpreting regression parameters\nTesting assumptions and handling violations\nAnalysis of variance in regression\nModel selection and comparison"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#whats-the-difference",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#whats-the-difference",
    "title": "Lecture 09",
    "section": "What’s the Difference?",
    "text": "What’s the Difference?\n\n\nCorrelation Analysis:\n\nMeasures the strength and direction of a relationship between two numerical variables\nBoth X and Y are random variables (both measured, neither manipulated)\nVariables are typically on equal footing (either could be X or Y)\nNo cause-effect relationship implied\nQuantifies the degree to which variables are related\nExpressed as a correlation coefficient (r) from -1 to +1\n\nRegression Analysis:\n\nPredicts one variable (Y) from another (X)\nX is often fixed or controlled (manipulated)\nY is the response variable of interest\nOften implies a cause-effect relationship\nProduces an equation for prediction\nEstimates slope and intercept parameters"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#what-is-correlation",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#what-is-correlation",
    "title": "Lecture 09",
    "section": "What Is Correlation?",
    "text": "What Is Correlation?\nCorrelation analysis measures the strength and direction of a relationship between two numerical variables:\n\nRanges from -1 to +1\n+1 indicates perfect positive correlation\n0 indicates no correlation\n-1 indicates perfect negative correlation\n\nThe Pearson correlation coefficient (r) is defined as:\n\\[r = \\frac{\\sum_{i}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i}(X_i - \\bar{X})^2 \\sum_{i}(Y_i - \\bar{Y})^2}}\\]\nThis can be simplified as:\n\\[r = \\frac{\\text{Covariance}(X, Y)}{s_X \\cdot s_Y}\\]\nWhere \\(s_X\\) and \\(s_Y\\) are the standard deviations of X and Y."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-16.1-flipping-the-bird",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-16.1-flipping-the-bird",
    "title": "Lecture 09",
    "section": "Example 16.1: Flipping the Bird",
    "text": "Example 16.1: Flipping the Bird\nNazca boobies (Sula granti) - Do aggressive behaviors as a chick predict future aggressive behavior as an adult?\n\ncorrelation is r = 0.534 - moderate positive relationship\np-value = 0.007 correlation is statistically significant.\n\nFor a Pearson correlation coefficient (r) of 0.53372:\n\nThis is r (not rho as Spearman nonparticipant below), as indicated by “cor” in your output\nTo determine the amount of variation explained, you square this value: r² = 0.53372² = 0.2849 (or approximately 28.49%)\nmeans about 28.49% of the variance in one variable can be explained by the other variable"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#note-texttfracrse_r",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#note-texttfracrse_r",
    "title": "Lecture 09",
    "section": "Note \\(\\text{t}=\\frac{r}{SE_r}\\)",
    "text": "Note \\(\\text{t}=\\frac{r}{SE_r}\\)\n\n\n[1] 0.5337225\n\n\n\n    Pearson's product-moment correlation\n\ndata:  booby_data$visits_as_nestling and booby_data$future_aggression\nt = 2.9603, df = 22, p-value = 0.007229\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1660840 0.7710999\nsample estimates:\n      cor \n0.5337225"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-16.1-flipping-the-bird-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-16.1-flipping-the-bird-1",
    "title": "Lecture 09",
    "section": "Example 16.1: Flipping the Bird",
    "text": "Example 16.1: Flipping the Bird\nInterpretation: The correlation coefficient of r = 0.534 suggests that Nazca boobies who experienced more visits from non-parent adults as nestlings tend to display more aggressive behavior as adults. This supports the hypothesis that early experiences influence adult behavior patterns in this species.\nStandard Error:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#textse_r-sqrtfrac1-r2n-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#textse_r-sqrtfrac1-r2n-2",
    "title": "Lecture 09",
    "section": "\\(\\text{SE}_r = \\sqrt{\\frac{1-r^2}{n-2}}\\)",
    "text": "\\(\\text{SE}_r = \\sqrt{\\frac{1-r^2}{n-2}}\\)"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#se-0.180",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#se-0.180",
    "title": "Lecture 09",
    "section": "SE = 0.180",
    "text": "SE = 0.180\nNeed to be sure relationship is not curved - note below"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-assumptions-for-correlation",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-assumptions-for-correlation",
    "title": "Lecture 09",
    "section": "Testing Assumptions for Correlation",
    "text": "Testing Assumptions for Correlation\nAs described in Section 16.3, correlation analysis has key assumptions:\n\nRandom sampling: Observations should be a random sample from the population\nBivariate normality: Both variables follow a normal distribution, and their joint distribution is bivariate normal\nLinear relationship: The relationship between variables is linear, not curved\n\nLet’s check these assumptions using the lion data from Example 17.1 Lion Noses:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-assumptions-for-correlation-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-assumptions-for-correlation-1",
    "title": "Lecture 09",
    "section": "Testing Assumptions for Correlation",
    "text": "Testing Assumptions for Correlation\nAs described in Section 16.3, correlation analysis has key assumptions:\n\nRandom sampling: Observations should be a random sample from the population\nBivariate normality: Both variables follow a normal distribution, and their joint distribution is bivariate normal\nLinear relationship: The relationship between variables is linear, not curved\n\nLet’s check these assumptions using the lion data from Example 17.1 Lion Noses:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#what-to-do-if-assumptions-are-violated",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#what-to-do-if-assumptions-are-violated",
    "title": "Lecture 09",
    "section": "What to do if assumptions are violated:",
    "text": "What to do if assumptions are violated:\nTransform one or both variables (log, square root, etc.)\nUse non-parametric correlation (Spearman’s rank correlation) or Kendall’s tau 𝛕\nExamine the data for outliers or influential points\nTo understand the amount of variation explained, you can square the Spearman’s rho value.\nFor your value of 0.74485:\nρ² = 0.74485² = 0.5548\nThis means approximately 55.48% of the variance in ranks of one variable can be explained by the ranks of the other variable. This is similar to how R² works in linear regression, but specifically for ranked data.\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lion_data$proportion_black and lion_data$age_years\nS = 1392.1, p-value = 1.013e-06\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7448561"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#correlation-important-considerations",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#correlation-important-considerations",
    "title": "Lecture 09",
    "section": "Correlation: Important Considerations",
    "text": "Correlation: Important Considerations\nThe correlation coefficient depends on the range\n\nRestricting range of values can reduce the correlation coefficient\nComparing correlations between studies requires similar ranges of values\n\nMeasurement error affects correlation\n\nMeasurement error in X or Y tends to weaken observed correlation\nThis bias is called attenuation\nTrue correlation typically stronger than observed correlation\n\nCorrelation vs. Causation\n\nCorrelation does not imply causation\nThree possible explanations for correlation:\n\nX causes Y\nY causes X\nZ (a third variable) causes both X and Y\n\n\nCorrelation significance test\n\nH₀: ρ = 0 (no correlation in population)\nH₁: ρ ≠ 0 (correlation exists in population)\nTest statistic: t = r / SE(r) with df = n-2"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe population regression model \\[Y = \\alpha + \\beta X + \\varepsilon\\]\nWhere:\n\nY is the response variable\nX is the predictor variable\nα (alpha) is the intercept (value of Y when X=0)\nβ (beta) is the slope (change in Y per unit change in X)\nε (epsilon) is the error term (random deviation from the line)\n\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept)\nb is the estimate of β (slope)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-1",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe population regression model is:\n\\[Y = \\alpha + \\beta X + \\varepsilon\\]\nWhere:\n\nY is the response variable\nX is the predictor variable\nα (alpha) is the intercept (value of Y when X=0)\nβ (beta) is the slope (change in Y per unit change in X)\nε (epsilon) is the error term (random deviation from the line)\n\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept)\nb is the estimate of β (slope)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#textage-0.88-10.65-times-textproportion_black",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#textage-0.88-10.65-times-textproportion_black",
    "title": "Lecture 09",
    "section": "\\(\\text{age} = 0.88 + 10.65 \\times \\text{proportion}_{black}\\)",
    "text": "\\(\\text{age} = 0.88 + 10.65 \\times \\text{proportion}_{black}\\)\nThis means: - When a lion has no black on its nose (proportion = 0), its predicted age is 0.88 years - For each 0.1 increase in the proportion of black, age increases by 1.065 years - The slope (10.65) indicates that lions with more black on their noses tend to be older"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-2",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\nmale lions develop more black pigmentation on their noses as they age.\ncan be used to estimate the age of lions in the field.\n\n\n\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-3",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-3",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe calculation for slope (b) is:\n\\[b = \\frac{\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i(X_i - \\bar{X})^2}\\]\nGiven: - \\(\\bar{X} = 0.3222\\) - \\(\\bar{Y} = 4.3094\\) - \\(\\sum_i(X_i - \\bar{X})^2 = 1.2221\\) - \\(\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y}) = 13.0123\\)\nb = 13.0123 / 1.2221 = 10.647\nIntercept (a): \\(a = \\bar{Y} - b\\bar{X} = 4.3094 - 10.647(0.3222) = 0.879\\)\nMaking predictions:\nTo predict the age of a lion with 0.50 proportion of black on its nose:\n\\[\\hat{Y} = 0.88 + 10.65(0.50) = 6.2 \\text{ years}\\]\nConfidence intervals vs. Prediction intervals:\n\nConfidence interval: Range for the mean age of all lions with 0.50 black\nPrediction interval: Range for an individual lion with 0.50 black\n\nBoth intervals are narrowest near \\(\\bar{X}\\) and widen as X moves away from the mean."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-prairie-home-companion",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#example-prairie-home-companion",
    "title": "Lecture 09",
    "section": "Example Prairie Home Companion",
    "text": "Example Prairie Home Companion\n\nDoes biodiversity affect ecosystem stability?\nTilman et al. (2006) investigated using experimental plots varying plant species\n\n\n\n# A tibble: 6 × 2\n  species_number log_stability\n           &lt;dbl&gt;         &lt;dbl&gt;\n1              1         0.763\n2              1         1.45 \n3              1         1.51 \n4              1         0.747\n5              1         0.983\n6              1         1.12 \n\n\n\nCall:\nlm(formula = log_stability ~ species_number, data = prairie_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82774 -0.25344 -0.00426  0.27498  0.75240 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.252629   0.041023  30.535  &lt; 2e-16 ***\nspecies_number 0.025984   0.004926   5.275 4.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3433 on 159 degrees of freedom\nMultiple R-squared:  0.149, Adjusted R-squared:  0.1436 \nF-statistic: 27.83 on 1 and 159 DF,  p-value: 4.276e-07\n\n\n[1] \"rsquared is:  0.148953385305455\"\n\n\nAnalysis of Variance Table\n\nResponse: log_stability\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies_number   1  3.2792  3.2792  27.829 4.276e-07 ***\nResiduals      159 18.7358  0.1178                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#the-test-statistic-is-t-fracb---beta_0se_b",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#the-test-statistic-is-t-fracb---beta_0se_b",
    "title": "Lecture 09",
    "section": "The test statistic is: \\(t = \\frac{b - \\beta_0}{SE_b}\\)",
    "text": "The test statistic is: \\(t = \\frac{b - \\beta_0}{SE_b}\\)\nWith df = n - 2 = 161 - 2 = 159\nInterpretation:\nThe slope estimate is 0.033, indicating that log stability increases by 0.033 units for each additional plant species in the plot.\nThe p-value is very small (2.73e-10), providing strong evidence to reject the null hypothesis that species number has no effect on ecosystem stability.\nR² = 0.222, meaning that approximately 22.2% of the variation in log stability is explained by the number of plant species.\nThis supports the biodiversity-stability hypothesis: more diverse plant communities have more stable biomass production over time."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:\n\nAssume that error 𝞮 is \\(e_i = y_i - \\hat{y}_i\\)\n\nnormally distributed for each xi\nhas the same variance\nhas a mean of 0 at each xi"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-1",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:\n\nAssume that error 𝞮 is - estimated as the residuals: \\(e_i = y_i - \\hat{y}_i\\)\n\nordinary lease square estimates a and b or slope and intercept to minimize the sum of the residuals squared or Mean Squared Error (MSE) as"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#sum_i1n-y_i---haty_i2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#sum_i1n-y_i---haty_i2",
    "title": "Lecture 09",
    "section": "\\(\\sum_{i=1}^{n} = (y_i - \\hat{y}_i)^2\\)",
    "text": "\\(\\sum_{i=1}^{n} = (y_i - \\hat{y}_i)^2\\)"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-2",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-3",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-3",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-4",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#testing-regression-assumptions-4",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-4",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_slides.html#simple-linear-regression-model-4",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nIf assumptions are violated: 1. Transform the data (Section 17.6) 2. Use weighted least squares for heteroscedasticity 3. Consider non-linear models (Section 17.8)"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html",
    "title": "Lecture 09",
    "section": "",
    "text": "Covered\n\nStudy design\nCausality in ecology\nExperimental design:\n\nReplication, controls, randomization, independence\n\nSampling in field studies\nPower analysis: a priori and post hoc\nStudy design and analysis"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 09",
    "section": "The objectives:",
    "text": "The objectives:\nThis lecture covers two fundamental statistical techniques in biology: correlation and regression analysis. Based on Chapters 16-17 from Whitlock & Schluter’s The Analysis of Biological Data (3rd edition), we’ll explore:\n\nCorrelation analysis: measuring relationships between variables\nThe distinction between correlation and regression\nSimple linear regression: predicting one variable from another\nEstimating and interpreting regression parameters\nTesting assumptions and handling violations\nAnalysis of variance in regression\nModel selection and comparison"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#whats-the-difference",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#whats-the-difference",
    "title": "Lecture 09",
    "section": "What’s the Difference?",
    "text": "What’s the Difference?\n\n\nCorrelation Analysis:\n\nMeasures the strength and direction of a relationship between two numerical variables\nBoth X and Y are random variables (both measured, neither manipulated)\nVariables are typically on equal footing (either could be X or Y)\nNo cause-effect relationship implied\nQuantifies the degree to which variables are related\nExpressed as a correlation coefficient (r) from -1 to +1\n\nRegression Analysis:\n\nPredicts one variable (Y) from another (X)\nX is often fixed or controlled (manipulated)\nY is the response variable of interest\nOften implies a cause-effect relationship\nProduces an equation for prediction\nEstimates slope and intercept parameters"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#what-is-correlation",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#what-is-correlation",
    "title": "Lecture 09",
    "section": "What Is Correlation?",
    "text": "What Is Correlation?\nCorrelation analysis measures the strength and direction of a relationship between two numerical variables:\n\nRanges from -1 to +1\n+1 indicates perfect positive correlation\n0 indicates no correlation\n-1 indicates perfect negative correlation\n\nThe Pearson correlation coefficient (r) is defined as:\n\\[r = \\frac{\\sum_{i}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i}(X_i - \\bar{X})^2 \\sum_{i}(Y_i - \\bar{Y})^2}}\\]\nThis can be simplified as:\n\\[r = \\frac{\\text{Covariance}(X, Y)}{s_X \\cdot s_Y}\\]\nWhere \\(s_X\\) and \\(s_Y\\) are the standard deviations of X and Y."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-16.1-flipping-the-bird",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-16.1-flipping-the-bird",
    "title": "Lecture 09",
    "section": "Example 16.1: Flipping the Bird",
    "text": "Example 16.1: Flipping the Bird\nNazca boobies (Sula granti) - Do aggressive behaviors as a chick predict future aggressive behavior as an adult?\n\ncorrelation is r = 0.534 - moderate positive relationship\np-value = 0.007 correlation is statistically significant.\n\nFor a Pearson correlation coefficient (r) of 0.53372:\n\nThis is r (not rho as Spearman nonparticipant below), as indicated by “cor” in your output\nTo determine the amount of variation explained, you square this value: r² = 0.53372² = 0.2849 (or approximately 28.49%)\nmeans about 28.49% of the variance in one variable can be explained by the other variable"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#note-texttfracrse_r",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#note-texttfracrse_r",
    "title": "Lecture 09",
    "section": "Note \\(\\text{t}=\\frac{r}{SE_r}\\)",
    "text": "Note \\(\\text{t}=\\frac{r}{SE_r}\\)\n\n\n[1] 0.5337225\n\n\n\n    Pearson's product-moment correlation\n\ndata:  booby_data$visits_as_nestling and booby_data$future_aggression\nt = 2.9603, df = 22, p-value = 0.007229\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1660840 0.7710999\nsample estimates:\n      cor \n0.5337225"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-16.1-flipping-the-bird-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-16.1-flipping-the-bird-1",
    "title": "Lecture 09",
    "section": "Example 16.1: Flipping the Bird",
    "text": "Example 16.1: Flipping the Bird\nInterpretation: The correlation coefficient of r = 0.534 suggests that Nazca boobies who experienced more visits from non-parent adults as nestlings tend to display more aggressive behavior as adults. This supports the hypothesis that early experiences influence adult behavior patterns in this species.\nStandard Error:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#textse_r-sqrtfrac1-r2n-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#textse_r-sqrtfrac1-r2n-2",
    "title": "Lecture 09",
    "section": "\\(\\text{SE}_r = \\sqrt{\\frac{1-r^2}{n-2}}\\)",
    "text": "\\(\\text{SE}_r = \\sqrt{\\frac{1-r^2}{n-2}}\\)"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#se-0.180",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#se-0.180",
    "title": "Lecture 09",
    "section": "SE = 0.180",
    "text": "SE = 0.180\nNeed to be sure relationship is not curved - note below"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-assumptions-for-correlation",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-assumptions-for-correlation",
    "title": "Lecture 09",
    "section": "Testing Assumptions for Correlation",
    "text": "Testing Assumptions for Correlation\nAs described in Section 16.3, correlation analysis has key assumptions:\n\nRandom sampling: Observations should be a random sample from the population\nBivariate normality: Both variables follow a normal distribution, and their joint distribution is bivariate normal\nLinear relationship: The relationship between variables is linear, not curved\n\nLet’s check these assumptions using the lion data from Example 17.1 Lion Noses:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-assumptions-for-correlation-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-assumptions-for-correlation-1",
    "title": "Lecture 09",
    "section": "Testing Assumptions for Correlation",
    "text": "Testing Assumptions for Correlation\nAs described in Section 16.3, correlation analysis has key assumptions:\n\nRandom sampling: Observations should be a random sample from the population\nBivariate normality: Both variables follow a normal distribution, and their joint distribution is bivariate normal\nLinear relationship: The relationship between variables is linear, not curved\n\nLet’s check these assumptions using the lion data from Example 17.1 Lion Noses:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#what-to-do-if-assumptions-are-violated",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#what-to-do-if-assumptions-are-violated",
    "title": "Lecture 09",
    "section": "What to do if assumptions are violated:",
    "text": "What to do if assumptions are violated:\nTransform one or both variables (log, square root, etc.)\nUse non-parametric correlation (Spearman’s rank correlation) or Kendall’s tau 𝛕\nExamine the data for outliers or influential points\nTo understand the amount of variation explained, you can square the Spearman’s rho value.\nFor your value of 0.74485:\nρ² = 0.74485² = 0.5548\nThis means approximately 55.48% of the variance in ranks of one variable can be explained by the ranks of the other variable. This is similar to how R² works in linear regression, but specifically for ranked data.\n\n\n\n    Spearman's rank correlation rho\n\ndata:  lion_data$proportion_black and lion_data$age_years\nS = 1392.1, p-value = 1.013e-06\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7448561"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#correlation-important-considerations",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#correlation-important-considerations",
    "title": "Lecture 09",
    "section": "Correlation: Important Considerations",
    "text": "Correlation: Important Considerations\nThe correlation coefficient depends on the range\n\nRestricting range of values can reduce the correlation coefficient\nComparing correlations between studies requires similar ranges of values\n\nMeasurement error affects correlation\n\nMeasurement error in X or Y tends to weaken observed correlation\nThis bias is called attenuation\nTrue correlation typically stronger than observed correlation\n\nCorrelation vs. Causation\n\nCorrelation does not imply causation\nThree possible explanations for correlation:\n\nX causes Y\nY causes X\nZ (a third variable) causes both X and Y\n\n\nCorrelation significance test\n\nH₀: ρ = 0 (no correlation in population)\nH₁: ρ ≠ 0 (correlation exists in population)\nTest statistic: t = r / SE(r) with df = n-2"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe population regression model \\[Y = \\alpha + \\beta X + \\varepsilon\\]\nWhere:\n\nY is the response variable\nX is the predictor variable\nα (alpha) is the intercept (value of Y when X=0)\nβ (beta) is the slope (change in Y per unit change in X)\nε (epsilon) is the error term (random deviation from the line)\n\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept)\nb is the estimate of β (slope)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-1",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nSimple linear regression models the relationship between a response variable (Y) and a predictor variable (X).\nThe population regression model is:\n\\[Y = \\alpha + \\beta X + \\varepsilon\\]\nWhere:\n\nY is the response variable\nX is the predictor variable\nα (alpha) is the intercept (value of Y when X=0)\nβ (beta) is the slope (change in Y per unit change in X)\nε (epsilon) is the error term (random deviation from the line)\n\nThe sample regression equation is:\n\\[\\hat{Y} = a + bX\\]\nWhere:\n\n\\(\\hat{Y}\\) is the predicted value of Y\na is the estimate of α (intercept)\nb is the estimate of β (slope)\n\nMethod of Least Squares: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#textage-0.88-10.65-times-textproportion_black",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#textage-0.88-10.65-times-textproportion_black",
    "title": "Lecture 09",
    "section": "\\(\\text{age} = 0.88 + 10.65 \\times \\text{proportion}_{black}\\)",
    "text": "\\(\\text{age} = 0.88 + 10.65 \\times \\text{proportion}_{black}\\)\nThis means: - When a lion has no black on its nose (proportion = 0), its predicted age is 0.88 years - For each 0.1 increase in the proportion of black, age increases by 1.065 years - The slope (10.65) indicates that lions with more black on their noses tend to be older"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-2",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\n\nmale lions develop more black pigmentation on their noses as they age.\ncan be used to estimate the age of lions in the field.\n\n\n\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,    Adjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-3",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-3",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nThe calculation for slope (b) is:\n\\[b = \\frac{\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i(X_i - \\bar{X})^2}\\]\nGiven: - \\(\\bar{X} = 0.3222\\) - \\(\\bar{Y} = 4.3094\\) - \\(\\sum_i(X_i - \\bar{X})^2 = 1.2221\\) - \\(\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y}) = 13.0123\\)\nb = 13.0123 / 1.2221 = 10.647\nIntercept (a): \\(a = \\bar{Y} - b\\bar{X} = 4.3094 - 10.647(0.3222) = 0.879\\)\nMaking predictions:\nTo predict the age of a lion with 0.50 proportion of black on its nose:\n\\[\\hat{Y} = 0.88 + 10.65(0.50) = 6.2 \\text{ years}\\]\nConfidence intervals vs. Prediction intervals:\n\nConfidence interval: Range for the mean age of all lions with 0.50 black\nPrediction interval: Range for an individual lion with 0.50 black\n\nBoth intervals are narrowest near \\(\\bar{X}\\) and widen as X moves away from the mean."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-prairie-home-companion",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#example-prairie-home-companion",
    "title": "Lecture 09",
    "section": "Example Prairie Home Companion",
    "text": "Example Prairie Home Companion\n\nDoes biodiversity affect ecosystem stability?\nTilman et al. (2006) investigated using experimental plots varying plant species\n\n\n\n# A tibble: 6 × 2\n  species_number log_stability\n           &lt;dbl&gt;         &lt;dbl&gt;\n1              1         0.763\n2              1         1.45 \n3              1         1.51 \n4              1         0.747\n5              1         0.983\n6              1         1.12 \n\n\n\nCall:\nlm(formula = log_stability ~ species_number, data = prairie_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82774 -0.25344 -0.00426  0.27498  0.75240 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    1.252629   0.041023  30.535  &lt; 2e-16 ***\nspecies_number 0.025984   0.004926   5.275 4.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3433 on 159 degrees of freedom\nMultiple R-squared:  0.149, Adjusted R-squared:  0.1436 \nF-statistic: 27.83 on 1 and 159 DF,  p-value: 4.276e-07\n\n\n[1] \"rsquared is:  0.148953385305455\"\n\n\nAnalysis of Variance Table\n\nResponse: log_stability\n                Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nspecies_number   1  3.2792  3.2792  27.829 4.276e-07 ***\nResiduals      159 18.7358  0.1178                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#the-test-statistic-is-t-fracb---beta_0se_b",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#the-test-statistic-is-t-fracb---beta_0se_b",
    "title": "Lecture 09",
    "section": "The test statistic is: \\(t = \\frac{b - \\beta_0}{SE_b}\\)",
    "text": "The test statistic is: \\(t = \\frac{b - \\beta_0}{SE_b}\\)\nWith df = n - 2 = 161 - 2 = 159\nInterpretation:\nThe slope estimate is 0.033, indicating that log stability increases by 0.033 units for each additional plant species in the plot.\nThe p-value is very small (2.73e-10), providing strong evidence to reject the null hypothesis that species number has no effect on ecosystem stability.\nR² = 0.222, meaning that approximately 22.2% of the variation in log stability is explained by the number of plant species.\nThis supports the biodiversity-stability hypothesis: more diverse plant communities have more stable biomass production over time."
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:\n\nAssume that error 𝞮 is \\(e_i = y_i - \\hat{y}_i\\)\n\nnormally distributed for each xi\nhas the same variance\nhas a mean of 0 at each xi"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-1",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-1",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:\n\nAssume that error 𝞮 is - estimated as the residuals: \\(e_i = y_i - \\hat{y}_i\\)\n\nordinary lease square estimates a and b or slope and intercept to minimize the sum of the residuals squared or Mean Squared Error (MSE) as"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#sum_i1n-y_i---haty_i2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#sum_i1n-y_i---haty_i2",
    "title": "Lecture 09",
    "section": "\\(\\sum_{i=1}^{n} = (y_i - \\hat{y}_i)^2\\)",
    "text": "\\(\\sum_{i=1}^{n} = (y_i - \\hat{y}_i)^2\\)"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-2",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-2",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-3",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-3",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-4",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#testing-regression-assumptions-4",
    "title": "Lecture 09",
    "section": "Testing Regression Assumptions",
    "text": "Testing Regression Assumptions\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nLet’s check these assumptions for the lion regression model:"
  },
  {
    "objectID": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-4",
    "href": "lectures/lecture_09/09_01_lecture_powerpoint_html.html#simple-linear-regression-model-4",
    "title": "Lecture 09",
    "section": "Simple Linear Regression Model",
    "text": "Simple Linear Regression Model\nlinear regression has four key assumptions:\n\nLinearity: The relationship between X and Y is linear\nIndependence: Observations are independent\nHomoscedasticity: Equal variance across all values of X\nNormality: Residuals are normally distributed\n\nIf assumptions are violated: 1. Transform the data (Section 17.6) 2. Use weighted least squares for heteroscedasticity 3. Consider non-linear models (Section 17.8)"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Overview",
    "text": "Overview\n\nMultivariate data: multiple variables per object\nTypes of multivariate analyses\n\nFunctional vs. structural methods\nR-mode vs. Q-mode analyses\n\nEigenvectors, eigenvalues, and components\nDistance and dissimilarity measures\nData transformations and standardization\nScreening multivariate data\nMANOVA"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#functional-vs.-structural-methods",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#functional-vs.-structural-methods",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Functional vs. Structural Methods",
    "text": "Functional vs. Structural Methods\nFunctional methods: - Clear response and predictor variables - Goal: relate Y’s to X’s - Examples: MANOVA, PERMANOVA\nStructural methods: - Find patterns/structure in data - Often no clear predictors - Examples: PCA, NMDS, Cluster Analysis"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#two-main-approaches",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#two-main-approaches",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Two Main Approaches",
    "text": "Two Main Approaches\nScaling/Ordination Methods: - Reduce dimensions with new derived variables - Summarize patterns in data - Examples: PCA, CCA\nDissimilarity-Based Methods: - Measure dissimilarity between objects - Visualize relationships between objects - Examples: NMDS, Cluster Analysis"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#common-approaches",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#common-approaches",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Common Approaches",
    "text": "Common Approaches\nTransformations: - Log transformation for skewed data - Root transformations for count data - Fourth-root for species abundance data\nStandardization: - Centering: subtract mean (mean = 0) - Standardization: divide by SD (SD = 1) - Crucial for variables with different units - May not be appropriate for species data"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#visual-representation-methods",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#visual-representation-methods",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Visual Representation Methods",
    "text": "Visual Representation Methods\n\nSPLOMS/Scatterplot Matrices: show bivariate relationships\nStar plots: display multiple variables per object\nChernoff faces: represent variables as facial features\nHeatmaps: visualize data matrices with color\nBiplots: show objects and variables together\nOrdination plots: visualize relationships in reduced dimensions"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#key-issues-to-check",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#key-issues-to-check",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Key Issues to Check",
    "text": "Key Issues to Check\nMultivariate Outliers: - Objects with unusual patterns across variables - Detected with Mahalanobis distance (d²) - Test against χ² distribution with p df\nMissing Observations: - Common approaches: - Deletion: remove affected object or variable - Imputation: estimate missing values - Maximum likelihood methods - Multiple imputation"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#key-concepts",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_slides.html#key-concepts",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nMultivariate data requires special techniques to account for correlations between variables\nFunctional methods (MANOVA) test hypotheses about group differences\nStructural methods (PCA, NMDS) find patterns in data\nDistance measures quantify similarities between objects\nData standardization is crucial for variables with different units\nMultivariate graphics help visualize complex relationships"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Overview",
    "text": "Overview\n\nMultivariate data: multiple variables per object\nTypes of multivariate analyses\n\nFunctional vs. structural methods\nR-mode vs. Q-mode analyses\n\nEigenvectors, eigenvalues, and components\nDistance and dissimilarity measures\nData transformations and standardization\nScreening multivariate data\nMANOVA"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#functional-vs.-structural-methods",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#functional-vs.-structural-methods",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Functional vs. Structural Methods",
    "text": "Functional vs. Structural Methods\nFunctional methods: - Clear response and predictor variables - Goal: relate Y’s to X’s - Examples: MANOVA, PERMANOVA\nStructural methods: - Find patterns/structure in data - Often no clear predictors - Examples: PCA, NMDS, Cluster Analysis"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#two-main-approaches",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#two-main-approaches",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Two Main Approaches",
    "text": "Two Main Approaches\nScaling/Ordination Methods: - Reduce dimensions with new derived variables - Summarize patterns in data - Examples: PCA, CCA\nDissimilarity-Based Methods: - Measure dissimilarity between objects - Visualize relationships between objects - Examples: NMDS, Cluster Analysis"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#common-approaches",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#common-approaches",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Common Approaches",
    "text": "Common Approaches\nTransformations: - Log transformation for skewed data - Root transformations for count data - Fourth-root for species abundance data\nStandardization: - Centering: subtract mean (mean = 0) - Standardization: divide by SD (SD = 1) - Crucial for variables with different units - May not be appropriate for species data"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#visual-representation-methods",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#visual-representation-methods",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Visual Representation Methods",
    "text": "Visual Representation Methods\n\nSPLOMS/Scatterplot Matrices: show bivariate relationships\nStar plots: display multiple variables per object\nChernoff faces: represent variables as facial features\nHeatmaps: visualize data matrices with color\nBiplots: show objects and variables together\nOrdination plots: visualize relationships in reduced dimensions"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#key-issues-to-check",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#key-issues-to-check",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Key Issues to Check",
    "text": "Key Issues to Check\nMultivariate Outliers: - Objects with unusual patterns across variables - Detected with Mahalanobis distance (d²) - Test against χ² distribution with p df\nMissing Observations: - Common approaches: - Deletion: remove affected object or variable - Imputation: estimate missing values - Maximum likelihood methods - Multiple imputation"
  },
  {
    "objectID": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#key-concepts",
    "href": "lectures/lecture_17/17_01_lecture_powerpoint_html.html#key-concepts",
    "title": "Lecture 17 - Multivariate STATS",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nMultivariate data requires special techniques to account for correlations between variables\nFunctional methods (MANOVA) test hypotheses about group differences\nStructural methods (PCA, NMDS) find patterns in data\nDistance measures quantify similarities between objects\nData standardization is crucial for variables with different units\nMultivariate graphics help visualize complex relationships"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#review",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#review",
    "title": "Lecture 18 - xxxxxx",
    "section": "Review",
    "text": "Review"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 18 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#key-principles",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#key-principles",
    "title": "Lecture 18 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#assumptions",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_slides.html#assumptions",
    "title": "Lecture 18 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_html.html",
    "title": "Lecture 18 - xxxxxx",
    "section": "",
    "text": "image"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 18 - xxxxxx",
    "section": "Overview",
    "text": "Overview\nAnalysis of variance:\n\nExamples:"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#key-principles",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#key-principles",
    "title": "Lecture 18 - xxxxxx",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\nThe Analysis\n\nasdf\n\nasdf"
  },
  {
    "objectID": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#assumptions",
    "href": "lectures/lecture_18/18_01_lecture_powerpoint_html.html#assumptions",
    "title": "Lecture 18 - xxxxxx",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#review",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#review",
    "title": "Lecture 16 - ANCOVA",
    "section": "Review",
    "text": "Review\n\nGeneral linearized models (GLM"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#overview",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#overview",
    "title": "Lecture 16 - ANCOVA",
    "section": "Overview",
    "text": "Overview\nAnalysis of covariance (ANCOVA):\n\nIntroduction to ANCOVA\nWhen to use ANCOVA\nLinear model for ANCOVA\nAnalysis of variance in ANCOVA\nAssumptions of ANCOVA\n\nHomogeneous slopes\n\nRobust ANCOVA approaches\nSpecific comparisons of means\nExamples and interpretation\nScientific reporting of ANCOVA results"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#what-is-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#what-is-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "What is ANCOVA?",
    "text": "What is ANCOVA?\n\nANCOVA = Analysis of COVAriance\nCombination of regression and ANOVA\nA continuous covariate is measured along with the response variable for each experimental unit\nCommon use: compare means of factor levels (groups), adjusting for variance from continuous covariate\nAnother use: determine whether two or more regression lines differ in slopes and intercepts"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#common-applications-of-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#common-applications-of-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Common Applications of ANCOVA",
    "text": "Common Applications of ANCOVA\n\nIncreasing statistical power\n\nRemoving variation associated with a covariate can reduce residual error\nMore powerful test of treatment effects\n\nAdjusting for confounding variables\n\nWhen treatments might differ in some continuous variable\nNeed to separate treatment effects from covariate effects\n\nTesting equality of regression lines\n\nDo treatments have the same relationship with a continuous variable?\nTests for both slopes and intercepts"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#cricket-chirping-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#cricket-chirping-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Cricket Chirping Example",
    "text": "Cricket Chirping Example\nWant to compare chirping rate of two cricket species:\n\n- Oecanthus exclamationis\n- Oecanthus niveus\n\nBut:\n\n- Measured rates at different temperatures\n- Range of temperatures differed between species\n- Apparent relationship between pulse rate and temperature\n\nANCOVA lets us adjust for temperature effect to get a more powerful test!"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#the-ancova-model",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#the-ancova-model",
    "title": "Lecture 16 - ANCOVA",
    "section": "The ANCOVA Model",
    "text": "The ANCOVA Model\nKey concept in ANCOVA: the difference between “unadjusted” group means and “adjusted” means.\nIn this visualization:\n\nGroup Means (shown as asterisks): raw/unadjusted means for each group - simply the average X value and average Y value for all points in that group. Notice that Group A and Group B have different mean X values (they’re positioned at different points along the X axis).\nAdjusted Means (shown as triangles): These are what ANCOVA actually compares. The adjusted means represent what each group’s mean would be if all groups had the same value of the covariate (in this case, the overall mean X).\n\nThe core purpose of ANCOVA is to make this adjustment. This is important because:\n\nWhen groups differ in their covariate values (as they often do in observational studies or even in experiments with random assignment), comparing raw means can be misleading\nThe adjustment helps “level the playing field” by estimating what each group’s mean would be if they all had the same value of the covariate\n\nIn the diagram:\n\ndistance between adjusted means represents the estimated treatment effect after controlling for the covariate\n\nThe fundamental concept of ANCOVA\n\ncomparing what the group means would be if all groups had the same value of the covariate, thereby removing the influence of the covariate from our comparison of treatment effects."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#mathematical-model-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#mathematical-model-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Mathematical Model for ANCOVA",
    "text": "Mathematical Model for ANCOVA\nFor a single-factor ANCOVA with factor A (p levels, i = 1 to p), a continuous covariate (x), and response variable (y):\n\\(Y_{ij} = \\mu + \\alpha_i + \\beta(X_{ij} - \\bar{X}) + \\varepsilon_{ij}\\)\nWhere: - \\(Y_{ij}\\) = response value for observation j in level i of factor A - \\(\\mu\\) = overall mean - \\(\\alpha_i\\) = effect of level i of factor A - \\(\\beta\\) = common regression slope relating Y to X - \\(X_{ij}\\) = covariate value for observation j in level i of factor A - \\(\\bar{X}\\) = mean value of covariate - \\(\\varepsilon_{ij}\\) = error term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#interpretation-of-parameters",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#interpretation-of-parameters",
    "title": "Lecture 16 - ANCOVA",
    "section": "Interpretation of Parameters",
    "text": "Interpretation of Parameters\n\n\\(\\mu\\) = overall mean response\n\\(\\alpha_i\\) = effect of level i (difference between group mean and overall mean)\n\\(\\beta\\) = pooled within-group regression coefficient\n\\(X_{ij}\\) = covariate value for observation j in group i\n\\(\\bar{X}\\) = overall mean of covariate\n\\(\\varepsilon_{ij}\\) = unexplained error\n\nThis model assumes homogeneous slopes across all treatment groups (we’ll test this later)."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#running-ancova-in-r",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#running-ancova-in-r",
    "title": "Lecture 16 - ANCOVA",
    "section": "Running ANCOVA in R",
    "text": "Running ANCOVA in R\nBasic ANCOVA model: - Response: continuous variable (y) - Predictor: categorical factor (A) - Covariate: continuous variable (x)\nThe simplest ANCOVA model is:\nmodel &lt;- lm(y ~ A + x, data = mydata)\nanova(model)\nAlternative: use aov() function\nmodel &lt;- aov(y ~ A + x, data = mydata)\nsummary(model)\nBoth approaches use Type I SS (sequential). For unbalanced designs, you may want Type III SS using car package."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#partitioning-of-variation-in-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#partitioning-of-variation-in-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Partitioning of Variation in ANCOVA",
    "text": "Partitioning of Variation in ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#anova-table-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#anova-table-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "ANOVA Table for ANCOVA",
    "text": "ANOVA Table for ANCOVA\nThe ANOVA table for a single-factor ANCOVA has these components:\n\n\nSourcedfSum of SquaresMean SquareF-ratioExpected MSFactor A (adjusted)(p-1)SS_A(adj)MS_A(adj) = SS_A(adj)/(p-1)MS_A(adj)/MS_Residualσ² + n∑α²/(p-1)Covariate1SS_CovariateMS_Covariate = SS_Covariate/1MS_Covariate/MS_Residualσ² + β²∑(X-X̄)²Residualn-p-1SS_ResidualMS_Residual = SS_Residual/(n-p-1)σ²Totaln-1SS_Total"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#null-hypotheses-in-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#null-hypotheses-in-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Null Hypotheses in ANCOVA",
    "text": "Null Hypotheses in ANCOVA\n\nTreatment Effect (adjusted for covariate) \\(H_0: \\alpha_1 = \\alpha_2 = ... = \\alpha_p = 0\\)\n\nAre the adjusted group means equal?\nTest with F = MS_A(adj)/MS_Residual\n\nCovariate Effect \\(H_0: \\beta = 0\\)\n\nIs there a relationship between the covariate and the response?\nTest with F = MS_Covariate/MS_Residual\n\nHomogeneity of Slopes (test this first!) \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_p\\)\n\nAre the regression slopes the same for all groups?\nTest by adding group*covariate interaction term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#testing-for-homogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#testing-for-homogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Testing for Homogeneous Slopes",
    "text": "Testing for Homogeneous Slopes\nANCOVA assumes the regression slopes are the same for all groups (parallel regression lines)\nTo test this assumption:\n\nFit model with interaction term:\n\nmodel_int &lt;- lm(y ~ A * x, data = mydata)\n\nTest significance of interaction:\n\nanova(model_int)\n\nIf interaction is significant (p &lt; 0.05):\n\nSlopes are not homogeneous\nStandard ANCOVA inappropriate\nUse alternative approaches"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#parallel-vs.-non-parallel-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#parallel-vs.-non-parallel-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Parallel vs. Non-Parallel Slopes",
    "text": "Parallel vs. Non-Parallel Slopes"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#when-slopes-are-not-homogeneous",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#when-slopes-are-not-homogeneous",
    "title": "Lecture 16 - ANCOVA",
    "section": "When Slopes Are Not Homogeneous",
    "text": "When Slopes Are Not Homogeneous\nIf the interaction term is significant (p &lt; 0.05), the slope-group relationship is not the same across groups.\nOptions:\n\nReport the interaction - this is a biologically interesting result!\nSeparate regressions - analyze each group separately\nJohnson-Neyman procedure - identifies regions of the covariate where groups differ significantly\nAlternative models - consider transformation, polynomial terms, or more complex models"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#ancova-on-longevity-of-male-fruitflies",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#ancova-on-longevity-of-male-fruitflies",
    "title": "Lecture 16 - ANCOVA",
    "section": "ANCOVA on Longevity of Male Fruitflies",
    "text": "ANCOVA on Longevity of Male Fruitflies\n\n# 1. Examine the partridge data structure\n# str(partridge)\n\n# Create better names for treatments\npartridge$treatment &lt;- factor(partridge$TREATMEN,\n                            levels = 1:5,\n                            labels = c(\"No females\", \n                                      \"One virgin female daily\",\n                                      \"Eight virgin females daily\",\n                                      \"One inseminated female daily\",\n                                      \"Eight inseminated females daily\"))\n\n# 2. Create a plot of the data showing relationship\nggplot(partridge, aes(x = THORAX, y = LONGEV, color = treatment)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Thorax Length and Longevity\",\n       x = \"Thorax Length (mm)\",\n       y = \"Longevity (days)\",\n       color = \"Treatment\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#testing-homogeneity-of-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#testing-homogeneity-of-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Testing Homogeneity of Slopes",
    "text": "Testing Homogeneity of Slopes\n\n# Test for homogeneity of slopes\nhomo_slopes_model &lt;- lm(LONGEV ~ THORAX * treatment, data = partridge)\nanova(homo_slopes_model)\n\nAnalysis of Variance Table\n\nResponse: LONGEV\n                  Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nTHORAX             1 15496.6 15496.6 136.0170 &lt; 2.2e-16 ***\ntreatment          4  9611.5  2402.9  21.0905 4.617e-13 ***\nTHORAX:treatment   4    42.5    10.6   0.0933    0.9844    \nResiduals        115 13102.1   113.9                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Extract the p-value for the interaction\np_interaction &lt;- anova(homo_slopes_model)[3, \"Pr(&gt;F)\"]\n\nThe p-value for the interaction term (treatment × THORAX) is 0.984. Since this value is &gt; 0.05, we can assume homogeneous slopes and proceed with the standard ANCOVA."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#full-ancova-analysis",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#full-ancova-analysis",
    "title": "Lecture 16 - ANCOVA",
    "section": "Full ANCOVA Analysis",
    "text": "Full ANCOVA Analysis\n\n# Fit the ANCOVA model (without interaction)\nancova_model &lt;- lm(LONGEV ~ THORAX + treatment, data = partridge)\n\n# View ANOVA table\nanova(ancova_model)\n\nAnalysis of Variance Table\n\nResponse: LONGEV\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nTHORAX      1 15496.6 15496.6 140.293 &lt; 2.2e-16 ***\ntreatment   4  9611.5  2402.9  21.753 1.719e-13 ***\nResiduals 119 13144.7   110.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Get more detailed summary\nsummary(ancova_model)\n\n\nCall:\nlm(formula = LONGEV ~ THORAX + treatment, data = partridge)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.189  -6.599  -0.989   6.408  30.244 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                               -46.055     10.239  -4.498 1.61e-05\nTHORAX                                    135.819     12.439  10.919  &lt; 2e-16\ntreatmentOne virgin female daily           -3.929      2.997  -1.311 0.192347\ntreatmentEight virgin females daily        -1.276      2.983  -0.428 0.669517\ntreatmentOne inseminated female daily     -10.946      2.999  -3.650 0.000391\ntreatmentEight inseminated females daily  -23.879      2.973  -8.031 7.83e-13\n                                            \n(Intercept)                              ***\nTHORAX                                   ***\ntreatmentOne virgin female daily            \ntreatmentEight virgin females daily         \ntreatmentOne inseminated female daily    ***\ntreatmentEight inseminated females daily ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.51 on 119 degrees of freedom\nMultiple R-squared:  0.6564,    Adjusted R-squared:  0.6419 \nF-statistic: 45.46 on 5 and 119 DF,  p-value: &lt; 2.2e-16\n\n# Get adjusted means using emmeans\nadjusted_means &lt;- emmeans(ancova_model, \"treatment\")\nadjusted_means\n\n treatment                       emmean   SE  df lower.CL upper.CL\n No females                        65.4 2.11 119     61.3     69.6\n One virgin female daily           61.5 2.11 119     57.3     65.7\n Eight virgin females daily        64.2 2.10 119     60.0     68.3\n One inseminated female daily      54.5 2.11 119     50.3     58.7\n Eight inseminated females daily   41.6 2.12 119     37.4     45.8\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#pairwise-comparisons-of-adjusted-means",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#pairwise-comparisons-of-adjusted-means",
    "title": "Lecture 16 - ANCOVA",
    "section": "Pairwise Comparisons of Adjusted Means",
    "text": "Pairwise Comparisons of Adjusted Means\n\n# Pairwise comparisons of adjusted means\npairs(adjusted_means, adjust = \"tukey\")\n\n contrast                                                       estimate   SE\n No females - One virgin female daily                               3.93 3.00\n No females - Eight virgin females daily                            1.28 2.98\n No females - One inseminated female daily                         10.95 3.00\n No females - Eight inseminated females daily                      23.88 2.97\n One virgin female daily - Eight virgin females daily              -2.65 2.98\n One virgin female daily - One inseminated female daily             7.02 2.97\n One virgin female daily - Eight inseminated females daily         19.95 3.01\n Eight virgin females daily - One inseminated female daily          9.67 2.98\n Eight virgin females daily - Eight inseminated females daily      22.60 2.99\n One inseminated female daily - Eight inseminated females daily    12.93 3.01\n  df t.ratio p.value\n 119   1.311  0.6849\n 119   0.428  0.9929\n 119   3.650  0.0035\n 119   8.031  &lt;.0001\n 119  -0.891  0.8996\n 119   2.361  0.1336\n 119   6.636  &lt;.0001\n 119   3.249  0.0129\n 119   7.560  &lt;.0001\n 119   4.298  0.0003\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\n# Plot adjusted means with confidence intervals\nplot(adjusted_means, comparisons = TRUE) + \n  labs(title = \"Adjusted Means by Treatment\",\n       x = \"Adjusted Longevity (days)\",\n       y = \"Treatment\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#visualization-options-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#visualization-options-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Visualization Options for ANCOVA",
    "text": "Visualization Options for ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#heterogeneous-slopes-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#heterogeneous-slopes-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Heterogeneous Slopes Example",
    "text": "Heterogeneous Slopes Example\nConstable (1993) studied shrinking in sea urchin test: - Compared suture widths between treatments - Three groups: high food, low food, initial sample - Covariate: body volume (cube root transformed)\nThe analysis showed: - Significant interaction between treatment and covariate - Heterogeneous slopes across treatments - Can’t use standard ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#johnson-neyman-procedure-for-heterogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#johnson-neyman-procedure-for-heterogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Johnson-Neyman Procedure for Heterogeneous Slopes",
    "text": "Johnson-Neyman Procedure for Heterogeneous Slopes\n\n\nAnalysis of Variance Table\n\nResponse: suture_width\n                 Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nvolume            1 0.015724 0.0157238  176.91 &lt; 2.2e-16 ***\ntreatment         2 0.036482 0.0182411  205.23 &lt; 2.2e-16 ***\nvolume:treatment  2 0.006213 0.0031064   34.95 4.453e-11 ***\nResiduals        66 0.005866 0.0000889                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#interpretation-of-heterogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#interpretation-of-heterogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Interpretation of Heterogeneous Slopes",
    "text": "Interpretation of Heterogeneous Slopes\nWhen you have heterogeneous slopes, the Johnson-Neyman procedure identifies regions of the covariate where groups differ:\n\nInitial &gt; Low Food when cube root body volume &gt; 2.95\n\nFor large urchins, the initial sample has wider sutures than low food urchins\n\nHigh Food &gt; Initial when cube root body volume &gt; 1.81\n\nFor most urchins, high food treatment results in wider sutures than initial samples\n\nHigh Food &gt; Low Food when cube root body volume &gt; 2.07\n\nFor most medium to large urchins, high food results in wider sutures than low food\n\n\nThe biological interpretation is that food regime affects suture width differently depending on urchin size. This interaction is biologically meaningful and would be missed if we only looked at adjusted means!"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#key-assumptions",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#key-assumptions",
    "title": "Lecture 16 - ANCOVA",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nIndependence of observations\n\nSamples are random and independent\nNo clustering or repeated measures\n\nNormal distribution of residuals\n\nResiduals follow a normal distribution\nCheck with QQ plots or formal tests\n\nHomogeneity of variances\n\nEqual variances across groups\nCheck with residual plots vs. fitted values\n\nLinearity\n\nLinear relationship between Y and X within each group\nCheck with scatterplots\n\nHomogeneity of regression slopes (critical!)\n\nRegression slopes equal across all groups\nTest with interaction term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#checking-assumptions-in-r",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#checking-assumptions-in-r",
    "title": "Lecture 16 - ANCOVA",
    "section": "Checking Assumptions in R",
    "text": "Checking Assumptions in R\n\n# Fit ANCOVA model for partridge data\nancova_model &lt;- lm(LONGEV ~ THORAX + treatment, data = partridge)\n\n# Create a 2x2 panel of diagnostic plots\npar(mfrow = c(2, 2))\nplot(ancova_model)"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#non-parametric-alternatives",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#non-parametric-alternatives",
    "title": "Lecture 16 - ANCOVA",
    "section": "Non-Parametric Alternatives",
    "text": "Non-Parametric Alternatives\nWhen ANCOVA assumptions are violated, consider:\n\nRank Transformation\n\nRank transform both Y and X variables\nRun standard ANCOVA on ranked data\nSimple but may not handle interactions well\n\nANCOVA on Bootstrapped Data\n\nUse bootstrapping to estimate parameters\nDoesn’t require normality assumption\n\nQuantile Regression\n\nModels relationships at different quantiles\nRobust to outliers and heteroscedasticity\n\nPermutation Tests\n\nRandomization tests of treatment effects\nNo distributional assumptions"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#scientific-writing-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#scientific-writing-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Scientific Writing Example",
    "text": "Scientific Writing Example\nHere’s how you might write up ANCOVA results for publication:\n\n“We analyzed the effects of mating strategy on male fruitfly longevity using analysis of covariance (ANCOVA), with thorax length as a covariate. Before conducting the main analysis, we tested the homogeneity of slopes assumption and found no significant interaction between treatment and thorax length (F₄,₁₁₅ = 1.56, P = 0.19), indicating that the effect of body size on longevity was consistent across treatments.\nThe ANCOVA revealed significant effects of both treatment (F₄,₁₁₉ = 27.97, P &lt; 0.001) and thorax length (F₁,₁₁₉ = 145.44, P &lt; 0.001) on longevity. Thorax length was positively associated with longevity (b = 1.19), with larger males living longer. After adjusting for body size, males with no female partners lived significantly longer (adjusted mean ± SE: 1.81 ± 0.02 log₁₀ days) than males in any other treatment group. Males provided with a single virgin female daily (1.77 ± 0.02) or a single inseminated female daily (1.79 ± 0.02) showed intermediate longevity, while males with eight females per day showed the lowest longevity (1.72 ± 0.02 for inseminated females; 1.59 ± 0.02 for virgin females). Pairwise comparisons using Tukey’s HSD test indicated significant differences between all treatment groups (P &lt; 0.05) except between the two treatments with a single female per day (P = 0.42).”"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#key-principles",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#key-principles",
    "title": "Lecture 16 - ANCOVA",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\n\nANCOVA combines regression and ANOVA approaches\nIncreases power by accounting for continuous covariates\nAllows comparison of adjusted means\n\nThe Analysis\n\nAlways test for homogeneity of slopes first!\nIf slopes are homogeneous, proceed with standard ANCOVA\nIf slopes are heterogeneous, use alternatives (Johnson-Neyman procedure)\n\nInterpretation\n\nFocus on adjusted means (at mean covariate value)\nConsider both statistical and biological significance\nVisualize results clearly with appropriate graphs"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#assumptions",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_slides.html#assumptions",
    "title": "Lecture 16 - ANCOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances\nLinearity of relationships within groups\nHomogeneity of regression slopes\n\n\\\\"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#overview",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#overview",
    "title": "Lecture 16 - ANCOVA",
    "section": "Overview",
    "text": "Overview\nAnalysis of covariance (ANCOVA):\n\nIntroduction to ANCOVA\nWhen to use ANCOVA\nLinear model for ANCOVA\nAnalysis of variance in ANCOVA\nAssumptions of ANCOVA\n\nHomogeneous slopes\n\nRobust ANCOVA approaches\nSpecific comparisons of means\nExamples and interpretation\nScientific reporting of ANCOVA results"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#what-is-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#what-is-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "What is ANCOVA?",
    "text": "What is ANCOVA?\n\nANCOVA = Analysis of COVAriance\nCombination of regression and ANOVA\nA continuous covariate is measured along with the response variable for each experimental unit\nCommon use: compare means of factor levels (groups), adjusting for variance from continuous covariate\nAnother use: determine whether two or more regression lines differ in slopes and intercepts"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#common-applications-of-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#common-applications-of-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Common Applications of ANCOVA",
    "text": "Common Applications of ANCOVA\n\nIncreasing statistical power\n\nRemoving variation associated with a covariate can reduce residual error\nMore powerful test of treatment effects\n\nAdjusting for confounding variables\n\nWhen treatments might differ in some continuous variable\nNeed to separate treatment effects from covariate effects\n\nTesting equality of regression lines\n\nDo treatments have the same relationship with a continuous variable?\nTests for both slopes and intercepts"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#cricket-chirping-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#cricket-chirping-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Cricket Chirping Example",
    "text": "Cricket Chirping Example\nWant to compare chirping rate of two cricket species:\n\n- Oecanthus exclamationis\n- Oecanthus niveus\n\nBut:\n\n- Measured rates at different temperatures\n- Range of temperatures differed between species\n- Apparent relationship between pulse rate and temperature\n\nANCOVA lets us adjust for temperature effect to get a more powerful test!"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#the-ancova-model",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#the-ancova-model",
    "title": "Lecture 16 - ANCOVA",
    "section": "The ANCOVA Model",
    "text": "The ANCOVA Model\nKey concept in ANCOVA: the difference between “unadjusted” group means and “adjusted” means.\nIn this visualization:\n\nGroup Means (shown as asterisks): raw/unadjusted means for each group - simply the average X value and average Y value for all points in that group. Notice that Group A and Group B have different mean X values (they’re positioned at different points along the X axis).\nAdjusted Means (shown as triangles): These are what ANCOVA actually compares. The adjusted means represent what each group’s mean would be if all groups had the same value of the covariate (in this case, the overall mean X).\n\nThe core purpose of ANCOVA is to make this adjustment. This is important because:\n\nWhen groups differ in their covariate values (as they often do in observational studies or even in experiments with random assignment), comparing raw means can be misleading\nThe adjustment helps “level the playing field” by estimating what each group’s mean would be if they all had the same value of the covariate\n\nIn the diagram:\n\ndistance between adjusted means represents the estimated treatment effect after controlling for the covariate\n\nThe fundamental concept of ANCOVA\n\ncomparing what the group means would be if all groups had the same value of the covariate, thereby removing the influence of the covariate from our comparison of treatment effects."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#mathematical-model-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#mathematical-model-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Mathematical Model for ANCOVA",
    "text": "Mathematical Model for ANCOVA\nFor a single-factor ANCOVA with factor A (p levels, i = 1 to p), a continuous covariate (x), and response variable (y):\n\\(Y_{ij} = \\mu + \\alpha_i + \\beta(X_{ij} - \\bar{X}) + \\varepsilon_{ij}\\)\nWhere: - \\(Y_{ij}\\) = response value for observation j in level i of factor A - \\(\\mu\\) = overall mean - \\(\\alpha_i\\) = effect of level i of factor A - \\(\\beta\\) = common regression slope relating Y to X - \\(X_{ij}\\) = covariate value for observation j in level i of factor A - \\(\\bar{X}\\) = mean value of covariate - \\(\\varepsilon_{ij}\\) = error term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#interpretation-of-parameters",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#interpretation-of-parameters",
    "title": "Lecture 16 - ANCOVA",
    "section": "Interpretation of Parameters",
    "text": "Interpretation of Parameters\n\n\\(\\mu\\) = overall mean response\n\\(\\alpha_i\\) = effect of level i (difference between group mean and overall mean)\n\\(\\beta\\) = pooled within-group regression coefficient\n\\(X_{ij}\\) = covariate value for observation j in group i\n\\(\\bar{X}\\) = overall mean of covariate\n\\(\\varepsilon_{ij}\\) = unexplained error\n\nThis model assumes homogeneous slopes across all treatment groups (we’ll test this later)."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#running-ancova-in-r",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#running-ancova-in-r",
    "title": "Lecture 16 - ANCOVA",
    "section": "Running ANCOVA in R",
    "text": "Running ANCOVA in R\nBasic ANCOVA model: - Response: continuous variable (y) - Predictor: categorical factor (A) - Covariate: continuous variable (x)\nThe simplest ANCOVA model is:\nmodel &lt;- lm(y ~ A + x, data = mydata)\nanova(model)\nAlternative: use aov() function\nmodel &lt;- aov(y ~ A + x, data = mydata)\nsummary(model)\nBoth approaches use Type I SS (sequential). For unbalanced designs, you may want Type III SS using car package."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#partitioning-of-variation-in-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#partitioning-of-variation-in-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Partitioning of Variation in ANCOVA",
    "text": "Partitioning of Variation in ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#anova-table-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#anova-table-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "ANOVA Table for ANCOVA",
    "text": "ANOVA Table for ANCOVA\nThe ANOVA table for a single-factor ANCOVA has these components:\n\n\nSourcedfSum of SquaresMean SquareF-ratioExpected MSFactor A (adjusted)(p-1)SS_A(adj)MS_A(adj) = SS_A(adj)/(p-1)MS_A(adj)/MS_Residualσ² + n∑α²/(p-1)Covariate1SS_CovariateMS_Covariate = SS_Covariate/1MS_Covariate/MS_Residualσ² + β²∑(X-X̄)²Residualn-p-1SS_ResidualMS_Residual = SS_Residual/(n-p-1)σ²Totaln-1SS_Total"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#null-hypotheses-in-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#null-hypotheses-in-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Null Hypotheses in ANCOVA",
    "text": "Null Hypotheses in ANCOVA\n\nTreatment Effect (adjusted for covariate) \\(H_0: \\alpha_1 = \\alpha_2 = ... = \\alpha_p = 0\\)\n\nAre the adjusted group means equal?\nTest with F = MS_A(adj)/MS_Residual\n\nCovariate Effect \\(H_0: \\beta = 0\\)\n\nIs there a relationship between the covariate and the response?\nTest with F = MS_Covariate/MS_Residual\n\nHomogeneity of Slopes (test this first!) \\(H_0: \\beta_1 = \\beta_2 = ... = \\beta_p\\)\n\nAre the regression slopes the same for all groups?\nTest by adding group*covariate interaction term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#testing-for-homogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#testing-for-homogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Testing for Homogeneous Slopes",
    "text": "Testing for Homogeneous Slopes\nANCOVA assumes the regression slopes are the same for all groups (parallel regression lines)\nTo test this assumption:\n\nFit model with interaction term:\n\nmodel_int &lt;- lm(y ~ A * x, data = mydata)\n\nTest significance of interaction:\n\nanova(model_int)\n\nIf interaction is significant (p &lt; 0.05):\n\nSlopes are not homogeneous\nStandard ANCOVA inappropriate\nUse alternative approaches"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#parallel-vs.-non-parallel-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#parallel-vs.-non-parallel-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Parallel vs. Non-Parallel Slopes",
    "text": "Parallel vs. Non-Parallel Slopes"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#when-slopes-are-not-homogeneous",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#when-slopes-are-not-homogeneous",
    "title": "Lecture 16 - ANCOVA",
    "section": "When Slopes Are Not Homogeneous",
    "text": "When Slopes Are Not Homogeneous\nIf the interaction term is significant (p &lt; 0.05), the slope-group relationship is not the same across groups.\nOptions:\n\nReport the interaction - this is a biologically interesting result!\nSeparate regressions - analyze each group separately\nJohnson-Neyman procedure - identifies regions of the covariate where groups differ significantly\nAlternative models - consider transformation, polynomial terms, or more complex models"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#ancova-on-longevity-of-male-fruitflies",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#ancova-on-longevity-of-male-fruitflies",
    "title": "Lecture 16 - ANCOVA",
    "section": "ANCOVA on Longevity of Male Fruitflies",
    "text": "ANCOVA on Longevity of Male Fruitflies\n\n# 1. Examine the partridge data structure\n# str(partridge)\n\n# Create better names for treatments\npartridge$treatment &lt;- factor(partridge$TREATMEN,\n                            levels = 1:5,\n                            labels = c(\"No females\", \n                                      \"One virgin female daily\",\n                                      \"Eight virgin females daily\",\n                                      \"One inseminated female daily\",\n                                      \"Eight inseminated females daily\"))\n\n# 2. Create a plot of the data showing relationship\nggplot(partridge, aes(x = THORAX, y = LONGEV, color = treatment)) + \n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Relationship between Thorax Length and Longevity\",\n       x = \"Thorax Length (mm)\",\n       y = \"Longevity (days)\",\n       color = \"Treatment\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#testing-homogeneity-of-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#testing-homogeneity-of-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Testing Homogeneity of Slopes",
    "text": "Testing Homogeneity of Slopes\n\n# Test for homogeneity of slopes\nhomo_slopes_model &lt;- lm(LONGEV ~ THORAX * treatment, data = partridge)\nanova(homo_slopes_model)\n\nAnalysis of Variance Table\n\nResponse: LONGEV\n                  Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nTHORAX             1 15496.6 15496.6 136.0170 &lt; 2.2e-16 ***\ntreatment          4  9611.5  2402.9  21.0905 4.617e-13 ***\nTHORAX:treatment   4    42.5    10.6   0.0933    0.9844    \nResiduals        115 13102.1   113.9                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Extract the p-value for the interaction\np_interaction &lt;- anova(homo_slopes_model)[3, \"Pr(&gt;F)\"]\n\nThe p-value for the interaction term (treatment × THORAX) is 0.984. Since this value is &gt; 0.05, we can assume homogeneous slopes and proceed with the standard ANCOVA."
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#full-ancova-analysis",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#full-ancova-analysis",
    "title": "Lecture 16 - ANCOVA",
    "section": "Full ANCOVA Analysis",
    "text": "Full ANCOVA Analysis\n\n# Fit the ANCOVA model (without interaction)\nancova_model &lt;- lm(LONGEV ~ THORAX + treatment, data = partridge)\n\n# View ANOVA table\nanova(ancova_model)\n\nAnalysis of Variance Table\n\nResponse: LONGEV\n           Df  Sum Sq Mean Sq F value    Pr(&gt;F)    \nTHORAX      1 15496.6 15496.6 140.293 &lt; 2.2e-16 ***\ntreatment   4  9611.5  2402.9  21.753 1.719e-13 ***\nResiduals 119 13144.7   110.5                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n# Get more detailed summary\nsummary(ancova_model)\n\n\nCall:\nlm(formula = LONGEV ~ THORAX + treatment, data = partridge)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-26.189  -6.599  -0.989   6.408  30.244 \n\nCoefficients:\n                                         Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)                               -46.055     10.239  -4.498 1.61e-05\nTHORAX                                    135.819     12.439  10.919  &lt; 2e-16\ntreatmentOne virgin female daily           -3.929      2.997  -1.311 0.192347\ntreatmentEight virgin females daily        -1.276      2.983  -0.428 0.669517\ntreatmentOne inseminated female daily     -10.946      2.999  -3.650 0.000391\ntreatmentEight inseminated females daily  -23.879      2.973  -8.031 7.83e-13\n                                            \n(Intercept)                              ***\nTHORAX                                   ***\ntreatmentOne virgin female daily            \ntreatmentEight virgin females daily         \ntreatmentOne inseminated female daily    ***\ntreatmentEight inseminated females daily ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 10.51 on 119 degrees of freedom\nMultiple R-squared:  0.6564,    Adjusted R-squared:  0.6419 \nF-statistic: 45.46 on 5 and 119 DF,  p-value: &lt; 2.2e-16\n\n# Get adjusted means using emmeans\nadjusted_means &lt;- emmeans(ancova_model, \"treatment\")\nadjusted_means\n\n treatment                       emmean   SE  df lower.CL upper.CL\n No females                        65.4 2.11 119     61.3     69.6\n One virgin female daily           61.5 2.11 119     57.3     65.7\n Eight virgin females daily        64.2 2.10 119     60.0     68.3\n One inseminated female daily      54.5 2.11 119     50.3     58.7\n Eight inseminated females daily   41.6 2.12 119     37.4     45.8\n\nConfidence level used: 0.95"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#pairwise-comparisons-of-adjusted-means",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#pairwise-comparisons-of-adjusted-means",
    "title": "Lecture 16 - ANCOVA",
    "section": "Pairwise Comparisons of Adjusted Means",
    "text": "Pairwise Comparisons of Adjusted Means\n\n# Pairwise comparisons of adjusted means\npairs(adjusted_means, adjust = \"tukey\")\n\n contrast                                                       estimate   SE\n No females - One virgin female daily                               3.93 3.00\n No females - Eight virgin females daily                            1.28 2.98\n No females - One inseminated female daily                         10.95 3.00\n No females - Eight inseminated females daily                      23.88 2.97\n One virgin female daily - Eight virgin females daily              -2.65 2.98\n One virgin female daily - One inseminated female daily             7.02 2.97\n One virgin female daily - Eight inseminated females daily         19.95 3.01\n Eight virgin females daily - One inseminated female daily          9.67 2.98\n Eight virgin females daily - Eight inseminated females daily      22.60 2.99\n One inseminated female daily - Eight inseminated females daily    12.93 3.01\n  df t.ratio p.value\n 119   1.311  0.6849\n 119   0.428  0.9929\n 119   3.650  0.0035\n 119   8.031  &lt;.0001\n 119  -0.891  0.8996\n 119   2.361  0.1336\n 119   6.636  &lt;.0001\n 119   3.249  0.0129\n 119   7.560  &lt;.0001\n 119   4.298  0.0003\n\nP value adjustment: tukey method for comparing a family of 5 estimates \n\n# Plot adjusted means with confidence intervals\nplot(adjusted_means, comparisons = TRUE) + \n  labs(title = \"Adjusted Means by Treatment\",\n       x = \"Adjusted Longevity (days)\",\n       y = \"Treatment\") +\n  theme_minimal()"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#visualization-options-for-ancova",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#visualization-options-for-ancova",
    "title": "Lecture 16 - ANCOVA",
    "section": "Visualization Options for ANCOVA",
    "text": "Visualization Options for ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#heterogeneous-slopes-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#heterogeneous-slopes-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Heterogeneous Slopes Example",
    "text": "Heterogeneous Slopes Example\nConstable (1993) studied shrinking in sea urchin test: - Compared suture widths between treatments - Three groups: high food, low food, initial sample - Covariate: body volume (cube root transformed)\nThe analysis showed: - Significant interaction between treatment and covariate - Heterogeneous slopes across treatments - Can’t use standard ANCOVA"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#johnson-neyman-procedure-for-heterogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#johnson-neyman-procedure-for-heterogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Johnson-Neyman Procedure for Heterogeneous Slopes",
    "text": "Johnson-Neyman Procedure for Heterogeneous Slopes\n\n\nAnalysis of Variance Table\n\nResponse: suture_width\n                 Df   Sum Sq   Mean Sq F value    Pr(&gt;F)    \nvolume            1 0.015724 0.0157238  176.91 &lt; 2.2e-16 ***\ntreatment         2 0.036482 0.0182411  205.23 &lt; 2.2e-16 ***\nvolume:treatment  2 0.006213 0.0031064   34.95 4.453e-11 ***\nResiduals        66 0.005866 0.0000889                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#interpretation-of-heterogeneous-slopes",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#interpretation-of-heterogeneous-slopes",
    "title": "Lecture 16 - ANCOVA",
    "section": "Interpretation of Heterogeneous Slopes",
    "text": "Interpretation of Heterogeneous Slopes\nWhen you have heterogeneous slopes, the Johnson-Neyman procedure identifies regions of the covariate where groups differ:\n\nInitial &gt; Low Food when cube root body volume &gt; 2.95\n\nFor large urchins, the initial sample has wider sutures than low food urchins\n\nHigh Food &gt; Initial when cube root body volume &gt; 1.81\n\nFor most urchins, high food treatment results in wider sutures than initial samples\n\nHigh Food &gt; Low Food when cube root body volume &gt; 2.07\n\nFor most medium to large urchins, high food results in wider sutures than low food\n\n\nThe biological interpretation is that food regime affects suture width differently depending on urchin size. This interaction is biologically meaningful and would be missed if we only looked at adjusted means!"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#key-assumptions",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#key-assumptions",
    "title": "Lecture 16 - ANCOVA",
    "section": "Key Assumptions",
    "text": "Key Assumptions\n\nIndependence of observations\n\nSamples are random and independent\nNo clustering or repeated measures\n\nNormal distribution of residuals\n\nResiduals follow a normal distribution\nCheck with QQ plots or formal tests\n\nHomogeneity of variances\n\nEqual variances across groups\nCheck with residual plots vs. fitted values\n\nLinearity\n\nLinear relationship between Y and X within each group\nCheck with scatterplots\n\nHomogeneity of regression slopes (critical!)\n\nRegression slopes equal across all groups\nTest with interaction term"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#checking-assumptions-in-r",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#checking-assumptions-in-r",
    "title": "Lecture 16 - ANCOVA",
    "section": "Checking Assumptions in R",
    "text": "Checking Assumptions in R\n\n# Fit ANCOVA model for partridge data\nancova_model &lt;- lm(LONGEV ~ THORAX + treatment, data = partridge)\n\n# Create a 2x2 panel of diagnostic plots\npar(mfrow = c(2, 2))\nplot(ancova_model)"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#non-parametric-alternatives",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#non-parametric-alternatives",
    "title": "Lecture 16 - ANCOVA",
    "section": "Non-Parametric Alternatives",
    "text": "Non-Parametric Alternatives\nWhen ANCOVA assumptions are violated, consider:\n\nRank Transformation\n\nRank transform both Y and X variables\nRun standard ANCOVA on ranked data\nSimple but may not handle interactions well\n\nANCOVA on Bootstrapped Data\n\nUse bootstrapping to estimate parameters\nDoesn’t require normality assumption\n\nQuantile Regression\n\nModels relationships at different quantiles\nRobust to outliers and heteroscedasticity\n\nPermutation Tests\n\nRandomization tests of treatment effects\nNo distributional assumptions"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#scientific-writing-example",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#scientific-writing-example",
    "title": "Lecture 16 - ANCOVA",
    "section": "Scientific Writing Example",
    "text": "Scientific Writing Example\nHere’s how you might write up ANCOVA results for publication:\n\n“We analyzed the effects of mating strategy on male fruitfly longevity using analysis of covariance (ANCOVA), with thorax length as a covariate. Before conducting the main analysis, we tested the homogeneity of slopes assumption and found no significant interaction between treatment and thorax length (F₄,₁₁₅ = 1.56, P = 0.19), indicating that the effect of body size on longevity was consistent across treatments.\nThe ANCOVA revealed significant effects of both treatment (F₄,₁₁₉ = 27.97, P &lt; 0.001) and thorax length (F₁,₁₁₉ = 145.44, P &lt; 0.001) on longevity. Thorax length was positively associated with longevity (b = 1.19), with larger males living longer. After adjusting for body size, males with no female partners lived significantly longer (adjusted mean ± SE: 1.81 ± 0.02 log₁₀ days) than males in any other treatment group. Males provided with a single virgin female daily (1.77 ± 0.02) or a single inseminated female daily (1.79 ± 0.02) showed intermediate longevity, while males with eight females per day showed the lowest longevity (1.72 ± 0.02 for inseminated females; 1.59 ± 0.02 for virgin females). Pairwise comparisons using Tukey’s HSD test indicated significant differences between all treatment groups (P &lt; 0.05) except between the two treatments with a single female per day (P = 0.42).”"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#key-principles",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#key-principles",
    "title": "Lecture 16 - ANCOVA",
    "section": "Key Principles",
    "text": "Key Principles\n\nPurpose:\n\nANCOVA combines regression and ANOVA approaches\nIncreases power by accounting for continuous covariates\nAllows comparison of adjusted means\n\nThe Analysis\n\nAlways test for homogeneity of slopes first!\nIf slopes are homogeneous, proceed with standard ANCOVA\nIf slopes are heterogeneous, use alternatives (Johnson-Neyman procedure)\n\nInterpretation\n\nFocus on adjusted means (at mean covariate value)\nConsider both statistical and biological significance\nVisualize results clearly with appropriate graphs"
  },
  {
    "objectID": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#assumptions",
    "href": "lectures/lecture_16/16_01_lecture_powerpoint_html.html#assumptions",
    "title": "Lecture 16 - ANCOVA",
    "section": "Assumptions",
    "text": "Assumptions\n\nIndependence of observations\nNormal distribution of residuals\nHomogeneity of variances\nLinearity of relationships within groups\nHomogeneity of regression slopes\n\n\\\\"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "The objectives:",
    "text": "The objectives:\n\nDesign a well-organized project\nImplement good naming conventions\n\nControlled vocabulary\nIncluding units in names\n\nCreate and use metadata effectively\nBuild tidy, well-structured spreadsheets\nUnderstand data repositories\nCreate effective visualizations with ggplot2"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#why-make-plots",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#why-make-plots",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Why make plots?",
    "text": "Why make plots?"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#get-in-a-group-and-discuss",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#get-in-a-group-and-discuss",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Get in a group and discuss",
    "text": "Get in a group and discuss\n\nWhat is the purpose of a data visualization?\nWhat elements are essential in an effective plot?\nWhat characteristics define a “good” plot?\nWhat common mistakes make plots ineffective?\n\nNapoleon’s Disastrous Invasion of Russia Detailed in an 1869 Data Visualization: It’s Been Called “the Best Statistical Graphic Ever Drawn”"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#common-visualization-problems",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#common-visualization-problems",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Common Visualization Problems",
    "text": "Common Visualization Problems\n\nData distortion:\n\nNon-zero baselines on bar charts\n3D effects that skew perspective\nInappropriate scales\n\nExcessive “chart junk”:\n\nToo many gridlines\nUnnecessary decorative elements\nRedundant information\n\nPoor color choices:\n\nToo many colors\nNon-colorblind-friendly palettes\nColors that don’t print well in grayscale\n\nMisleading representations:\n\nPie charts with too many categories\nDual y-axes with different scales\nTruncated axes without clear indication"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#key-takeaways",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#key-takeaways",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPlan your data management from the beginning\n\nConsistent naming conventions\nGood organization\nRegular backups\n\nMake your data tidy from the start\n\nOne observation per row\nOne variable per column\nOne value per cell\n\nCreate effective visualizations by:\n\nFocusing on data, not decoration\nUsing appropriate plot types\nFollowing good design principles\nCustomizing for clear communication\n\nMaster the grammar of graphics to:\n\nBuild plots layer by layer\nCommunicate patterns clearly\nTell compelling stories with data"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#next-steps",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#next-steps",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Next Steps",
    "text": "Next Steps\n\nPractice creating different types of plots\nLearn to combine multiple plots effectively\nExplore statistical transformations in ggplot2\nDevelop a consistent visualization style"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "",
    "text": "Covered\n\nInductive vs deductive reasoning\nFormulating research questions\nAccuracy vs precision\nData types and classifications\nSetting up R projects\nInstalling and loading libraries\nReading files into R\nCreating basic graphs"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "The objectives:",
    "text": "The objectives:\n\nDesign a well-organized project\nImplement good naming conventions\n\nControlled vocabulary\nIncluding units in names\n\nCreate and use metadata effectively\nBuild tidy, well-structured spreadsheets\nUnderstand data repositories\nCreate effective visualizations with ggplot2"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#why-make-plots",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#why-make-plots",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Why make plots?",
    "text": "Why make plots?"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#get-in-a-group-and-discuss",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#get-in-a-group-and-discuss",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Get in a group and discuss",
    "text": "Get in a group and discuss\n\nWhat is the purpose of a data visualization?\nWhat elements are essential in an effective plot?\nWhat characteristics define a “good” plot?\nWhat common mistakes make plots ineffective?\n\nNapoleon’s Disastrous Invasion of Russia Detailed in an 1869 Data Visualization: It’s Been Called “the Best Statistical Graphic Ever Drawn”"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#common-visualization-problems",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#common-visualization-problems",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Common Visualization Problems",
    "text": "Common Visualization Problems\n\nData distortion:\n\nNon-zero baselines on bar charts\n3D effects that skew perspective\nInappropriate scales\n\nExcessive “chart junk”:\n\nToo many gridlines\nUnnecessary decorative elements\nRedundant information\n\nPoor color choices:\n\nToo many colors\nNon-colorblind-friendly palettes\nColors that don’t print well in grayscale\n\nMisleading representations:\n\nPie charts with too many categories\nDual y-axes with different scales\nTruncated axes without clear indication"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#key-takeaways",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#key-takeaways",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nPlan your data management from the beginning\n\nConsistent naming conventions\nGood organization\nRegular backups\n\nMake your data tidy from the start\n\nOne observation per row\nOne variable per column\nOne value per cell\n\nCreate effective visualizations by:\n\nFocusing on data, not decoration\nUsing appropriate plot types\nFollowing good design principles\nCustomizing for clear communication\n\nMaster the grammar of graphics to:\n\nBuild plots layer by layer\nCommunicate patterns clearly\nTell compelling stories with data"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#next-steps",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#next-steps",
    "title": "Lecture 2: Project Design & Data Visualization",
    "section": "Next Steps",
    "text": "Next Steps\n\nPractice creating different types of plots\nLearn to combine multiple plots effectively\nExplore statistical transformations in ggplot2\nDevelop a consistent visualization style"
  },
  {
    "objectID": "lectures/lecture_template/0X_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_template/0X_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture XX",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_template/0X_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_template/0X_01_lecture_powerpoint_html.html",
    "title": "Lecture XX",
    "section": "",
    "text": "Covered\n\nIntroduction to hypothesis testing\nThe standard normal distribution\nStandard error\nConfidence intervals\nStudent’s t-distribution\nH testing\nOne and Two Sample T Test\np-values"
  },
  {
    "objectID": "lectures/lecture_template/0X_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_template/0X_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture XX",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html",
    "href": "lectures/lecture_05/05_02_class_activity.html",
    "title": "05_Class_Activity",
    "section": "",
    "text": "In our previous activity, we:\n\nCreated and interpreted frequency distributions (histograms)\nCompared data between groups using side-by-side histograms\nExplored how sample size affects our understanding of populations\nCreated density plots and calculated probabilities\n\n\n\n\nToday we’ll focus on:\n\nStudent’s t-distribution and when to use it\nCalculating and interpreting standard error\nCreating confidence intervals\nConducting one-sample and two-sample t-tests\nUnderstanding statistical assumptions and their importance"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#what-did-we-do-last-time",
    "href": "lectures/lecture_05/05_02_class_activity.html#what-did-we-do-last-time",
    "title": "05_Class_Activity",
    "section": "",
    "text": "In our previous activity, we:\n\nCreated and interpreted frequency distributions (histograms)\nCompared data between groups using side-by-side histograms\nExplored how sample size affects our understanding of populations\nCreated density plots and calculated probabilities"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#todays-focus",
    "href": "lectures/lecture_05/05_02_class_activity.html#todays-focus",
    "title": "05_Class_Activity",
    "section": "",
    "text": "Today we’ll focus on:\n\nStudent’s t-distribution and when to use it\nCalculating and interpreting standard error\nCreating confidence intervals\nConducting one-sample and two-sample t-tests\nUnderstanding statistical assumptions and their importance"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#now-lets-calculate-summary-statistics-for-each-lake",
    "href": "lectures/lecture_05/05_02_class_activity.html#now-lets-calculate-summary-statistics-for-each-lake",
    "title": "05_Class_Activity",
    "section": "Now, let’s calculate summary statistics for each lake:",
    "text": "Now, let’s calculate summary statistics for each lake:\n\n# Calculate summary statistics for both lakes\ngrayling_summary &lt;- grayling_df %&gt;% \n  group_by(lake) %&gt;%\n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n = sum(!is.na(length_mm)),\n    se_length = sd_length / sqrt(n),\n    .groups = \"drop\"\n  )\n\n# Display the summary statistics\ngrayling_summary\n\n# A tibble: 2 × 5\n  lake  mean_length sd_length     n se_length\n  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1 I3           266.      28.3    66      3.48\n2 I8           363.      52.3   102      5.18"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#look-a-the-plot-of-pine-needles",
    "href": "lectures/lecture_05/05_02_class_activity.html#look-a-the-plot-of-pine-needles",
    "title": "05_Class_Activity",
    "section": "Look a the plot of pine needles",
    "text": "Look a the plot of pine needles\n\n# Create a boxplot to visualize the data\npine_df %&gt;%\n  ggplot(aes(x = wind, y = length_mm, fill = wind)) +\n  geom_boxplot() +\n  labs(title = \"Pine Needle Lengths by Wind Exposure\",\n       x = \"Position\",\n       y = \"Length (mm)\",\n       fill = \"Wind Position\") +\n  scale_fill_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\"))\n\n\n\n\n\n\n\n\nBefore conducting the t-test, we should check the assumptions:\n\n\n\n\n\n\nPractice Exercise 5: Check Assumptions for Two-Sample t-Test\n\n\n\n\n# Separate data by groups\nwindward_data &lt;- pine_df %&gt;% filter(wind == \"wind\")\nleeward_data &lt;- pine_df %&gt;% filter(wind == \"lee\")\n\n# 1. Check for normality in each group using QQ plots\n\nqqPlot(windward_data$length_mm, main = \"QQ Plot: Windward Needles\")\n\n\n\n\n\n\n\n\n[1] 21 22\n\n\n\nqqPlot(leeward_data$length_mm, main = \"QQ Plot: Leeward Needles\")\n\n\n\n\n\n\n\n\n[1]  4 16\n\n\n\n# 2. Check for equal variances using Levene's test\n# H0: Variances are equal\n# H1: Variances are not equal\nlevene_result &lt;- leveneTest(length_mm ~ wind, data = pine_df)\nprint(\"Levene's Test for Homogeneity of Variance:\")\n\n[1] \"Levene's Test for Homogeneity of Variance:\"\n\nprint(levene_result)\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(&gt;F)\ngroup  1  1.2004 0.2789\n      46               \n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nInterpreting the assumption checks:\n\nQQ plots: Do points approximately follow the line for both groups?\nLevene’s test: If p &gt; 0.05, we don’t reject the assumption of equal variances"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#now-lets-conduct-the-two-sample-t-test",
    "href": "lectures/lecture_05/05_02_class_activity.html#now-lets-conduct-the-two-sample-t-test",
    "title": "05_Class_Activity",
    "section": "Now let’s conduct the two-sample t-test:",
    "text": "Now let’s conduct the two-sample t-test:\n\n\n\n\n\n\nPractice Exercise 6: Two-Sample t-Test\n\n\n\n\n# Perform a two-sample t-test\n# H0: μ1 = μ2 (The mean needle lengths are equal)\n# H1: μ1 ≠ μ2 (The mean needle lengths are different)\n\n# var.equal=TRUE uses the standard t-test (pooled variance)\n# var.equal=FALSE uses Welch's t-test (for unequal variances)\nt_test_result &lt;- t.test(length_mm ~ wind, data = pine_df, var.equal = TRUE)\n\n# Display the test results\nprint(t_test_result)\n\n\n    Two Sample t-test\n\ndata:  length_mm by wind\nt = 8.6792, df = 46, p-value = 3.01e-11\nalternative hypothesis: true difference in means between group lee and group wind is not equal to 0\n95 percent confidence interval:\n 4.224437 6.775563\nsample estimates:\n mean in group lee mean in group wind \n          20.41667           14.91667 \n\n\n\n# Calculate the mean difference\nmean_diff &lt;- pine_summary$mean_length[pine_summary$wind == \"lee\"] - \n             pine_summary$mean_length[pine_summary$wind == \"wind\"]\ncat(\"Mean difference (lee - wind):\", round(mean_diff, 2), \"mm\\n\")\n\nMean difference (lee - wind): 5.5 mm\n\n\n\n# Visualize the results with a mean and error bar plot\nggplot(pine_summary, aes(x = wind, y = mean_length, fill = wind)) +\n  geom_bar(stat = \"identity\", alpha = 0.7) +\n  geom_errorbar(aes(ymin = mean_length - se_length, \n                   ymax = mean_length + se_length),\n               width = 0.2) +\n  scale_fill_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\")) +\n  labs(title = \"Pine Needle Lengths by Wind Exposure\",\n       subtitle = paste(\"t =\", round(t_test_result$statistic, 2), \n                      \", p =\", format.pval(t_test_result$p.value, digits = 3)),\n       x = \"Position\",\n       y = \"Mean Length (mm)\",\n       fill = \"Wind Position\")\n\n\n\n\n\n\n\n\nInterpret the results:\n\nWhat was the null hypothesis?\nWhat was the alternative hypothesis?\nWhat does the p-value tell us?\nShould we reject or fail to reject the null hypothesis?\nWhat is the practical interpretation for botanists?"
  },
  {
    "objectID": "lectures/lecture_05/05_03_homework_html.html",
    "href": "lectures/lecture_05/05_03_homework_html.html",
    "title": "05_Homework",
    "section": "",
    "text": "This is an assignment for you to practice coding and redo the work we do in class with a few twists on a new dataframe practicing to create new projects and writing new code. I realize you could copy the code from lecture and although you will get the code right, I urge you to retype it form scratch as it will be learned so much faster. This is a new language for you and if you dont “type” == “speak” the language you would remember it…. really and try breaking things. Dont be afraid you can download a new version or fix it… that is how we learn."
  },
  {
    "objectID": "lectures/lecture_05/05_03_homework_html.html#setup",
    "href": "lectures/lecture_05/05_03_homework_html.html#setup",
    "title": "05_Homework",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataset:\n\n# Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\n# Read in the data file\nw5_df &lt;- read_csv(\"data/mice_weights.csv\")\n\nRows: 62 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): sampling_site, location\ndbl (2): date_year, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Look at the first few rows\nhead(w5_df)\n\n# A tibble: 6 × 4\n  sampling_site location date_year mass_g\n  &lt;chr&gt;         &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Sidney Island island        2021   26  \n2 Sidney Island island        2021   24  \n3 Sidney Island island        2021   21.5\n4 Sidney Island island        2021   23  \n5 Sidney Island island        2021   22  \n6 Sidney Island island        2021   21  \n\n\nLet’s calculate some basic statistics for mice\n\n# Calculate basic statistics \nw5_stats &lt;- w5_df %&gt;% \n  group_by(sampling_site) %&gt;% \n  summarize(\n    mean_mass = mean(mass_g, na.rm = TRUE),\n    sd_mass = sd(mass_g, na.rm = TRUE),\n    n = sum(!is.na(mass_g)),\n    se_mass = sd_mass / sqrt(n)\n  )\n\n# Display the statistics\nw5_stats\n\n# A tibble: 2 × 5\n  sampling_site mean_mass sd_mass     n se_mass\n  &lt;chr&gt;             &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 Sidney Island      23.4    2.80    33   0.487\n2 Vancouver          20.2    1.71    28   0.324"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html",
    "href": "lectures/lecture_04/04_02_class_activity.html",
    "title": "03_Class_Activity",
    "section": "",
    "text": "Setting up a project and variable names and code names\nHow to use the pipe command %&gt;%\nHow to create descriptive statistics of a sample\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n_length = sum(!is.na(length_mm)))\n\nMore graphs…\nggplot(data = p_df, aes(x=length_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))\n\nWhat questions do you have and what is unclear - what did not work so far when you started the homework?"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "href": "lectures/lecture_04/04_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "title": "03_Class_Activity",
    "section": "",
    "text": "Setting up a project and variable names and code names\nHow to use the pipe command %&gt;%\nHow to create descriptive statistics of a sample\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    n_length = sum(!is.na(length_mm)))\n\nMore graphs…\nggplot(data = p_df, aes(x=length_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))\n\nWhat questions do you have and what is unclear - what did not work so far when you started the homework?"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#setup",
    "href": "lectures/lecture_04/04_02_class_activity.html#setup",
    "title": "03_Class_Activity",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataset:\n\n# # Install the patchwork package if needed\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nlibrary(tidyverse)\n\n\n# Read in the data file\nsculpin_df &lt;- read_csv(\"data/sculpin.csv\")\n\n# Look at the first few rows\nhead(sculpin_df)\n\n# A tibble: 6 × 5\n   site lake  species       length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;             &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin        53   1.25\n2   146 E 01  slimy sculpin        61   1.9 \n3   146 E 01  slimy sculpin        53   1.75\n4   146 E 01  slimy sculpin        77   4.25\n5   146 E 01  slimy sculpin        45   0.9 \n6   146 E 01  slimy sculpin        48   0.9"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#basic-data-summary",
    "href": "lectures/lecture_04/04_02_class_activity.html#basic-data-summary",
    "title": "03_Class_Activity",
    "section": "Basic Data Summary",
    "text": "Basic Data Summary\nLet’s first check what lakes are in our dataframe:\n\n# Get a list of unique lakes\nunique(sculpin_df$lake)\n\n[1] \"E 01\"   \"E 05\"   \"NE 12\"  \"NE 14\"  \"S 06\"   \"S 07\"   \"Toolik\"\n\n\nHow many fish do we have from each lake?\n\n# Count observations by lake\nsculpin_df %&gt;%\n  group_by(lake) %&gt;% \n  summarize(sculpin_n = n())\n\n# A tibble: 7 × 2\n  lake   sculpin_n\n  &lt;chr&gt;      &lt;int&gt;\n1 E 01         268\n2 E 05          75\n3 NE 12        180\n4 NE 14         37\n5 S 06         132\n6 S 07          73\n7 Toolik       287\n\n\n\n# Count observations by lake\nsculpin_df %&gt;%\n  group_by(lake) %&gt;% \n  summarize(sculpin_n = sum(!is.na(length_mm)))\n\n# A tibble: 7 × 2\n  lake   sculpin_n\n  &lt;chr&gt;      &lt;int&gt;\n1 E 01          79\n2 E 05          14\n3 NE 12        180\n4 NE 14         37\n5 S 06         132\n6 S 07          73\n7 Toolik       208"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#basic-histograms",
    "href": "lectures/lecture_04/04_02_class_activity.html#basic-histograms",
    "title": "03_Class_Activity",
    "section": "Basic Histograms",
    "text": "Basic Histograms\nA histogram shows how many observations fall into certain ranges (or “bins”).\nLet’s create a simple histogram of fish lengths from Lake E 01 :\n\n# Filter for Toolik Lake and create a histogram\nsculpin_df %&gt;%\n  filter(lake == \"E 01\") %&gt;%\n  ggplot(aes(x = length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Fish Lengths in Lake E 01\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nTry changing the binwidth parameter to 5 and then to 1. How does the appearance of the histogram change?\n\n# Try it here"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#comparing-lakes",
    "href": "lectures/lecture_04/04_02_class_activity.html#comparing-lakes",
    "title": "03_Class_Activity",
    "section": "Comparing Lakes",
    "text": "Comparing Lakes\nNow let’s compare two lakes\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, \n                 position = \"identity\") +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, \n                 position = position_dodge2(width=1)) +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nNow let’s compare two lakes side by side:\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, position = \"identity\") +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\") +\n  # facet_wrap(~lake, ncol = 1) +\n  facet_grid(lake~.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 2\n\n\n\nChoose two new lakes to compare. What differences do you notice in their distributions?\nAdd code here\n\n# enter code here"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#small-vs.-large-samples",
    "href": "lectures/lecture_04/04_02_class_activity.html#small-vs.-large-samples",
    "title": "03_Class_Activity",
    "section": "Small vs. Large Samples",
    "text": "Small vs. Large Samples\nWe’ll randomly select different sample sizes from Toolik Lake:\n\n# Set a seed for reproducibility\nset.seed(123)\n\n# Create small sample (15 fish)\nsmall_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(15)\n\n# Create larger sample (50 fish)\nlarger_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(100)\n\n# Plot both samples\np1 &lt;- small_sample %&gt;%\n  ggplot(aes(x = length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"red\", alpha = 0.7) +\n  # coord_cartesian(xlim = c(20,80)) +\n  labs(title = \"Small Sample (n=15)\",\n       x = \"Length (mm)\",\n       y = \"Count\") +\n  coord_cartesian(xlim = c(20,80))\n\np2 &lt;- larger_sample %&gt;%\n  ggplot(aes(x = length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  # coord_cartesian(xlim = c(20,80)) +\n  labs(title = \"Larger Sample (n=50)\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n\n# Display the plots side by side\np1 + p2 +\n  plot_layout(ncol = 1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 3\n\n\n\nTry changing the sample sizes. What happens when you use very small samples (n=5)? What about larger samples (n=150)?\nadd code here\n\n# enter code here"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#woah---what-happened-there---there-are-na-values-in-the-data",
    "href": "lectures/lecture_04/04_02_class_activity.html#woah---what-happened-there---there-are-na-values-in-the-data",
    "title": "03_Class_Activity",
    "section": "WOAH - what happened there - there are NA values in the data",
    "text": "WOAH - what happened there - there are NA values in the data\nyou need to either remove missing values or you can do that in the formulas\nWhat is the advantage to manually removing or doing it in formulas?\n\n# Calculate mean, standard deviation, and sample size by lake\nsculpin_stats_df &lt;- sculpin_df %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    se_length = sd(length_mm, na.rm = TRUE)/ sum(!is.na(length_mm))^.5,\n    count = sum(!is.na(length_mm)),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\nsculpin_stats_df\n\n# A tibble: 7 × 5\n  lake   mean_length sd_length se_length count\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Toolik        51.7      12.0     0.834   208\n2 NE 12         49.8      15.2     1.13    180\n3 S 06          54.0      10.9     0.949   132\n4 E 01          58.2      15.3     1.72     79\n5 S 07          55.6      12.7     1.48     73\n6 NE 14         47.3      10.5     1.72     37\n7 E 05          47.1      10.8     2.88     14\n\n\nNow let’s visualize these statistics:\n\n# Create a bar plot of mean lengths with error bars\nsculpin_df %&gt;%  \n  ggplot(aes(lake, length_mm)) +\n  stat_summary(\n    fun = mean, na.rm = TRUE, \n    geom = \"bar\",\n    fill = \"skyblue\"\n    ) +\n  stat_summary(\n    fun.data = mean_se, na.rm = TRUE, \n    geom = \"errorbar\", \n    width = 0.2) +\n  labs(title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\nWe could also do this from the dataframe we just made\n\n# Create a bar plot of mean lengths with error bars\nsculpin_stats_df %&gt;%  \n  ggplot(aes(x = reorder(lake, mean_length), y = mean_length)) +\n  geom_bar(stat = \"identity\", \n           fill = \"skyblue\") +\n  geom_errorbar(aes(\n    ymin = mean_length - se_length, \n    ymax = mean_length + se_length),\n    width = 0.2\n    ) +\n  labs(\n    title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\nThe power of the pipe command is you can do this without hving to make a new dataframe\n\n# Create a bar plot of mean lengths with error bars\nsculpin_df %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    se_length = sd_length / sqrt(n()),\n    count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(count &gt;= 10) %&gt;%  # Only include lakes with sufficient sample size\n  ggplot(aes(x = reorder(lake, mean_length), y = mean_length)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_errorbar(aes(ymin = mean_length - se_length, \n                    ymax = mean_length + se_length),\n                width = 0.2) +\n  labs(title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 5\n\n\n\nBased on the mean plot and what you’ve seen in the distributions, what can you say about fish sizes in different lakes? Are there lakes with particularly large or small fish?\nWe will start to ask how different are they and is it by chance?\nThis is the inductive phase of doing research."
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 03",
    "section": "The objectives:",
    "text": "The objectives:\n\nUnderstand why statistics is vital in biology\nDistinguish between different types of biological variables\nLearn about accuracy, precision, and bias in measurements\nCalculate and interpret measures of central tendency (mean, median, geometric mean)\nCalculate and interpret measures of spread (standard deviation, variance, IQR)\nUnderstand data transformations for skewed distributions\nVisualize descriptive statistics for our data\nLearn how to handle uncertainty in our data\n\nWe’ll use a dataset on grayling fish from two different lakes to explore these concepts.."
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#bary-fracsum_i1n-y_in",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#bary-fracsum_i1n-y_in",
    "title": "Lecture 03",
    "section": "\\[\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n}\\]",
    "text": "\\[\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n}\\]\nWhere:\n\n\\(Y_i\\) represents each individual measurement\n\\(n\\) is the total number of observations"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s-fracsum_i1n-y_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s-fracsum_i1n-y_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\[s = {\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]",
    "text": "\\[s = {\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]\nThe standard deviation is the square root of variance\n\nmeasures how far observations typically are from the mean and are in the units of the mean:"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s-sqrtfracsum_i1n-y_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s-sqrtfracsum_i1n-y_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]",
    "text": "\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#cv-fracsbary-times-100",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#cv-fracsbary-times-100",
    "title": "Lecture 03",
    "section": "\\[CV = \\frac{s}{\\bar{Y}} \\times 100\\%\\]",
    "text": "\\[CV = \\frac{s}{\\bar{Y}} \\times 100\\%\\]\nThis is useful for comparing the variability of measurements with different units or vastly different scales."
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 03",
    "section": "The objectives:",
    "text": "The objectives:\n\nUnderstand why statistics is vital in biology\nDistinguish between different types of biological variables\nLearn about accuracy, precision, and bias in measurements\nCalculate and interpret measures of central tendency (mean, median, geometric mean)\nCalculate and interpret measures of spread (standard deviation, variance, IQR)\nUnderstand data transformations for skewed distributions\nVisualize descriptive statistics for our data\nLearn how to handle uncertainty in our data\n\nWe’ll use a dataset on grayling fish from two different lakes to explore these concepts.."
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#bary-fracsum_i1n-y_in",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#bary-fracsum_i1n-y_in",
    "title": "Lecture 03",
    "section": "\\[\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n}\\]",
    "text": "\\[\\bar{Y} = \\frac{\\sum_{i=1}^{n} Y_i}{n}\\]\nWhere:\n\n\\(Y_i\\) represents each individual measurement\n\\(n\\) is the total number of observations"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s-fracsum_i1n-y_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s-fracsum_i1n-y_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\[s = {\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]",
    "text": "\\[s = {\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]\nThe standard deviation is the square root of variance\n\nmeasures how far observations typically are from the mean and are in the units of the mean:"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s-sqrtfracsum_i1n-y_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s-sqrtfracsum_i1n-y_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]",
    "text": "\\[s = \\sqrt{\\frac{\\sum_{i=1}^{n} (Y_i - \\bar{Y})^2}{n-1}}\\]"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#cv-fracsbary-times-100",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#cv-fracsbary-times-100",
    "title": "Lecture 03",
    "section": "\\[CV = \\frac{s}{\\bar{Y}} \\times 100\\%\\]",
    "text": "\\[CV = \\frac{s}{\\bar{Y}} \\times 100\\%\\]\nThis is useful for comparing the variability of measurements with different units or vastly different scales."
  }
]