[
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html",
    "title": "Lecture 03",
    "section": "",
    "text": "We covered:\n\ndata wrangling and types of variable names\nmeta data\nproject design\nsummary statistics\ngraphing the mean and standard error graphs\npipes and %&gt;% or |&gt; and how to group_by\n\n\n\nOur last graph"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#introduction-to-probability-distributions",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#introduction-to-probability-distributions",
    "title": "Lecture 03",
    "section": "Introduction to probability distributions",
    "text": "Introduction to probability distributions\n\nWhat is a frequency distribution?\nWhat is a probability distribution?\nDistributions for variables and for statistics"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#estimation",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#estimation",
    "title": "Lecture 03",
    "section": "Estimation",
    "text": "Estimation\n\nPopulations and samples\nParameters and statistics\n\nwe are going to use some sculpin data that is real!"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#fy-frac1sqrt2pisigma2-e-fracy---mu22sigma2",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#fy-frac1sqrt2pisigma2-e-fracy---mu22sigma2",
    "title": "Lecture 03",
    "section": "\\(f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}\\)",
    "text": "\\(f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#binomial-multinomial",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#binomial-multinomial",
    "title": "Lecture 03",
    "section": "Binomial (multinomial):",
    "text": "Binomial (multinomial):\n\nprobability of event that have two outcomes (heads/ tails, dead/alive)\nDefined in terms of “successes” out of set number of trials\n\n\nIn large number of trials: approximately normal distribution"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#poisson-occurrences-of-rare-event-in-timespace",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#poisson-occurrences-of-rare-event-in-timespace",
    "title": "Lecture 03",
    "section": "Poisson: occurrences of (rare) event in time/space",
    "text": "Poisson: occurrences of (rare) event in time/space\n\nE.g., number of\n\nTaraxacum officinale - common dandelion in quadrat\ncopepod eaten per minute\ncells in field of view\n\nMeasures Probability(y= certain integer value)\n\ndefined in terms of μ or mean\nRight-skewed at small μ\nmore symmetrical at higher μ"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#also-have-distributions-of-test-statistics",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#also-have-distributions-of-test-statistics",
    "title": "Lecture 03",
    "section": "Also have distributions of test statistics",
    "text": "Also have distributions of test statistics\n\nTest statistics:\n\nsummary values calculated from data used to test hypotheses\nis your result due to chance?\n\n\n\nDifferent test statistics:\n\ndifferent, well-defined distributions\nallows estimation of probabilities associated with results\nExamples:\n\nz-distribution, student’s t-distribution, χ2-distribution, F-distribution"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#mu-fracsumlimits_i1n-y_in",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#mu-fracsumlimits_i1n-y_in",
    "title": "Lecture 03",
    "section": "\\(\\mu = \\frac{\\sum\\limits_{i=1}^{n} Y_i}{n}\\)",
    "text": "\\(\\mu = \\frac{\\sum\\limits_{i=1}^{n} Y_i}{n}\\)\nFormula for n odd"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textmedian-y_n12",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textmedian-y_n12",
    "title": "Lecture 03",
    "section": "\\(\\text{median = } Y\\_{(n+1)/2}\\)",
    "text": "\\(\\text{median = } Y\\_{(n+1)/2}\\)\nFormula for n even"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textmedian-fracy_n2-y_n21-2",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textmedian-fracy_n2-y_n21-2",
    "title": "Lecture 03",
    "section": "\\(\\text{median = }\\frac{Y_{n/2} + Y_{(n/2)+1}}{  2}\\)",
    "text": "\\(\\text{median = }\\frac{Y_{n/2} + Y_{(n/2)+1}}{  2}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#spread",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#spread",
    "title": "Lecture 03",
    "section": "Spread",
    "text": "Spread\n\nRange: from highest and lowest observation\nVariance (σ2, s2): sum of squared differences of observations from mean, divided by n-1\n\nE.g., fish lengths = 20, 30, 35, 24, 36 g"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s2-sum_i1n-fracy_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s2-sum_i1n-fracy_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\(s^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}\\)",
    "text": "\\(s^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#spread-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#spread-1",
    "title": "Lecture 03",
    "section": "Spread",
    "text": "Spread\n\nStandard Deviation(σ, s): square root of variance.\n\nIn same units as observations\nIn example: √48 = 6.9 mm\n\nCoefficient of variation: SD as % of mean.\n\nUseful for comparing spread in samples with different means\nIn example: (6.9/29)*100= 23.8 %"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s2-sqrtsum_i1n-fracy_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#s2-sqrtsum_i1n-fracy_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\(s^2 = \\sqrt{\\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}}\\)",
    "text": "\\(s^2 = \\sqrt{\\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textcoefficient-of-variation-fracsbary-times-100",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_html.html#textcoefficient-of-variation-fracsbary-times-100",
    "title": "Lecture 03",
    "section": "\\(\\text{Coefficient of variation} = \\frac{S}{\\bar{Y}} \\times 100\\)",
    "text": "\\(\\text{Coefficient of variation} = \\frac{S}{\\bar{Y}} \\times 100\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#introduction-to-probability-distributions",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#introduction-to-probability-distributions",
    "title": "Lecture 03",
    "section": "Introduction to probability distributions",
    "text": "Introduction to probability distributions\n\nWhat is a frequency distribution?\nWhat is a probability distribution?\nDistributions for variables and for statistics"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#estimation",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#estimation",
    "title": "Lecture 03",
    "section": "Estimation",
    "text": "Estimation\n\nPopulations and samples\nParameters and statistics\n\nwe are going to use some sculpin data that is real!"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#fy-frac1sqrt2pisigma2-e-fracy---mu22sigma2",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#fy-frac1sqrt2pisigma2-e-fracy---mu22sigma2",
    "title": "Lecture 03",
    "section": "\\(f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}\\)",
    "text": "\\(f(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(y - \\mu)^2}{2\\sigma^2}}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#binomial-multinomial",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#binomial-multinomial",
    "title": "Lecture 03",
    "section": "Binomial (multinomial):",
    "text": "Binomial (multinomial):\n\nprobability of event that have two outcomes (heads/ tails, dead/alive)\nDefined in terms of “successes” out of set number of trials\n\nIn large number of trials: approximately normal distribution"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#poisson-occurrences-of-rare-event-in-timespace",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#poisson-occurrences-of-rare-event-in-timespace",
    "title": "Lecture 03",
    "section": "Poisson: occurrences of (rare) event in time/space",
    "text": "Poisson: occurrences of (rare) event in time/space\n\nE.g., number of\n\nTaraxacum officinale - common dandelion in quadrat\ncopepod eaten per minute\ncells in field of view\n\nMeasures Probability(y= certain integer value)\n\ndefined in terms of μ or mean\nRight-skewed at small μ\nmore symmetrical at higher μ"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#also-have-distributions-of-test-statistics",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#also-have-distributions-of-test-statistics",
    "title": "Lecture 03",
    "section": "Also have distributions of test statistics",
    "text": "Also have distributions of test statistics\nTest statistics:\n\nsummary values calculated from data used to test hypotheses\nis your result due to chance?\n\nDifferent test statistics:\n\ndifferent, well-defined distributions\nallows estimation of probabilities associated with results\nExamples:\n\nz-distribution, student’s t-distribution, χ2-distribution, F-distribution"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#mu-fracsumlimits_i1n-y_in",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#mu-fracsumlimits_i1n-y_in",
    "title": "Lecture 03",
    "section": "\\(\\mu = \\frac{\\sum\\limits_{i=1}^{n} Y_i}{n}\\)",
    "text": "\\(\\mu = \\frac{\\sum\\limits_{i=1}^{n} Y_i}{n}\\)\nFormula for n odd"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textmedian-y_n12",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textmedian-y_n12",
    "title": "Lecture 03",
    "section": "\\(\\text{median = } Y\\_{(n+1)/2}\\)",
    "text": "\\(\\text{median = } Y\\_{(n+1)/2}\\)\nFormula for n even"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textmedian-fracy_n2-y_n21-2",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textmedian-fracy_n2-y_n21-2",
    "title": "Lecture 03",
    "section": "\\(\\text{median = }\\frac{Y_{n/2} + Y_{(n/2)+1}}{  2}\\)",
    "text": "\\(\\text{median = }\\frac{Y_{n/2} + Y_{(n/2)+1}}{  2}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#spread",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#spread",
    "title": "Lecture 03",
    "section": "Spread",
    "text": "Spread\n\nRange: from highest and lowest observation\nVariance (σ2, s2): sum of squared differences of observations from mean, divided by n-1\n\nE.g., fish lengths = 20, 30, 35, 24, 36 g"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s2-sum_i1n-fracy_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s2-sum_i1n-fracy_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\(s^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}\\)",
    "text": "\\(s^2 = \\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#spread-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#spread-1",
    "title": "Lecture 03",
    "section": "Spread",
    "text": "Spread\n\nStandard Deviation(σ, s): square root of variance.\n\nIn same units as observations\nIn example: √48 = 6.9 mm\n\nCoefficient of variation: SD as % of mean.\n\nUseful for comparing spread in samples with different means\nIn example: (6.9/29)*100= 23.8 %"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s2-sqrtsum_i1n-fracy_i---bary2n-1",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#s2-sqrtsum_i1n-fracy_i---bary2n-1",
    "title": "Lecture 03",
    "section": "\\(s^2 = \\sqrt{\\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}}\\)",
    "text": "\\(s^2 = \\sqrt{\\sum_{i=1}^{n} \\frac{(y_i - \\bar{y})^2}{n-1}}\\)"
  },
  {
    "objectID": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textcoefficient-of-variation-fracsbary-times-100",
    "href": "lectures/lecture_03/03_01_lecture_powerpoint_slides.html#textcoefficient-of-variation-fracsbary-times-100",
    "title": "Lecture 03",
    "section": "\\(\\text{Coefficient of variation} = \\frac{S}{\\bar{Y}} \\times 100\\)",
    "text": "\\(\\text{Coefficient of variation} = \\frac{S}{\\bar{Y}} \\times 100\\)"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html",
    "href": "lectures/lecture_04/04_02_class_activity.html",
    "title": "04_Class_Activity",
    "section": "",
    "text": "Created and interpreted frequency distributions with histograms\nCompared lakes using side-by-side histograms\nExplored how sample size affects our view of a population\nCreated density plots and calculated probabilities\n\n\n\n\n\nUnderstanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "href": "lectures/lecture_04/04_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "title": "04_Class_Activity",
    "section": "",
    "text": "Created and interpreted frequency distributions with histograms\nCompared lakes using side-by-side histograms\nExplored how sample size affects our view of a population\nCreated density plots and calculated probabilities"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#todays-focus",
    "href": "lectures/lecture_04/04_02_class_activity.html#todays-focus",
    "title": "04_Class_Activity",
    "section": "",
    "text": "Understanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#setup",
    "href": "lectures/lecture_04/04_02_class_activity.html#setup",
    "title": "04_Class_Activity",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataset:\n\n# Load required packages\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in the data file\nsculpin_df &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Look at the first few rows\nhead(sculpin_df)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9 \n\n\nLet’s calculate some basic statistics for Toolik Lake fish:\n\n# Calculate basic statistics for Toolik Lake\ntoolik_stats &lt;- sculpin_df %&gt;% \n  filter(lake == \"Toolik\") %&gt;% \n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n)\n  )\n\n# Display the statistics\ntoolik_stats\n\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1        51.7      12.0   208     0.834"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#converting-to-z-scores",
    "href": "lectures/lecture_04/04_02_class_activity.html#converting-to-z-scores",
    "title": "04_Class_Activity",
    "section": "Converting to Z-scores",
    "text": "Converting to Z-scores\nZ-scores tell us how many standard deviations a data point is from the mean. The formula is:\n\\[ Z = \\frac{X - \\mu}{\\sigma} \\]\nLet’s calculate z-scores for the Toolik Lake fish:\n\n# Add a z-score column to our Toolik Lake data\ntoolik_z &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  mutate(z_score = (total_length_mm - toolik_stats$mean_length) / toolik_stats$sd_length)\n\n# Display the first few rows with z-scores\nhead(toolik_z)\n\n# A tibble: 6 × 6\n   site lake   species       total_length_mm mass_g z_score\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1   100 Toolik slimy sculpin            12.5 0.0152  -3.26 \n2   100 Toolik slimy sculpin            13   0.0104  -3.22 \n3   100 Toolik slimy sculpin            74   2.3      1.85 \n4   100 Toolik slimy sculpin            42   0.5     -0.806\n5   100 Toolik slimy sculpin            39   0.4     -1.06 \n6   100 Toolik slimy sculpin            71   3        1.61 \n\n\nNow, let’s create a histogram of the original data and the z-scores side by side:\n\n# Create histogram of original data\np1 &lt;- toolik_z %&gt;%\n  ggplot(aes(total_length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Original Scale\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n# Create histogram of z-scores\np2 &lt;- toolik_z %&gt;%\n  ggplot(aes(z_score)) +\n  geom_histogram(binwidth = 0.2, fill = \"red\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linewidth = 1) +\n  geom_vline(xintercept = c(-1, 1), color = \"black\", linetype = \"dashed\") +\n  labs(title = \"Z-Score Scale\",\n       x = \"Standard deviations from mean (z-score)\",\n       y = \"Count\")\n\n# Combine plots\np1 / p2\n\nWarning: Removed 79 rows containing non-finite outside the scale range (`stat_bin()`).\nRemoved 79 rows containing non-finite outside the scale range (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nCalculate the z-score for a fish with a length of 30mm from Toolik Lake. Is this fish length common or unusual?\n\n# Calculate the z-score for a 30mm fish\nfish_length &lt;- 30\nz_score_30mm &lt;- (fish_length - toolik_stats$mean_length) / toolik_stats$sd_length\n\n# Print the result\nz_score_30mm\n\n[1] -1.804193\n\n# Determine if this is common or unusual\n# A z-score beyond -2 or 2 is generally considered unusual"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#understanding-areas-under-the-standard-normal-curve",
    "href": "lectures/lecture_04/04_02_class_activity.html#understanding-areas-under-the-standard-normal-curve",
    "title": "04_Class_Activity",
    "section": "Understanding Areas Under the Standard Normal Curve",
    "text": "Understanding Areas Under the Standard Normal Curve\nThe standard normal distribution has known probabilities associated with z-scores. We can calculate these probabilities in R:\n\n# Calculate probability of z-score less than 1\npnorm(1)  # This gives us the area under the standard normal curve to the left of z=1\n\n[1] 0.8413447\n\n# Calculate probability of z-score between -1 and 1\npnorm(1) - pnorm(-1)  # This is approximately 68% (the 68-95-99.7 rule)\n\n[1] 0.6826895\n\n\n\n\n\n\n\n\nActivity 2\n\n\n\nUsing the pnorm() function, answer: 1. What proportion of fish in Toolik Lake are shorter than 40mm?\n\n# First, convert 40mm to a z-score\nz_score_40mm &lt;- (40 - toolik_stats$mean_length) / toolik_stats$sd_length\nz_score_40mm\n\n[1] -0.9725651\n\n# Then, find the proportion of fish shorter than 40mm\nproportion_less_than_40mm &lt;- pnorm(z_score_40mm)\nproportion_less_than_40mm\n\n[1] 0.1653847"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#taking-multiple-samples",
    "href": "lectures/lecture_04/04_02_class_activity.html#taking-multiple-samples",
    "title": "04_Class_Activity",
    "section": "Taking Multiple Samples",
    "text": "Taking Multiple Samples\nWhen we take multiple samples from a population, each sample gives a slightly different estimate of the population mean. Let’s see this in action:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Take a small sample\nsmall_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(10)\n\n# Calculate the mean of this sample\nsmall_sample_mean &lt;- mean(small_sample$total_length_mm, na.rm = TRUE)\n\n# Print the result\ncat(\"Sample mean (n=10):\", small_sample_mean, \"mm\\n\")\n\nSample mean (n=10): 50.42857 mm\n\ncat(\"Population mean:\", toolik_stats$mean_length, \"mm\\n\")\n\nPopulation mean: 51.69471 mm\n\ncat(\"Difference:\", abs(small_sample_mean - toolik_stats$mean_length), \"mm\\n\")\n\nDifference: 1.26614 mm\n\n\nLet’s take multiple small samples and see how their means vary:\n\n# Function to take a sample and calculate its mean\nget_sample_mean &lt;- function() {\n  sculpin_df %&gt;%\n    filter(lake == \"Toolik\") %&gt;%\n    sample_n(10) %&gt;%\n    summarize(mean_length = mean(total_length_mm, na.rm = TRUE)) %&gt;%\n    pull(mean_length)\n}\n\n# Take 20 samples\nsample_means &lt;- replicate(20, get_sample_mean())\n\n# Create a histogram of the sample means\ntibble(sample_means = sample_means) %&gt;%\n  ggplot(aes(sample_means)) +\n  geom_histogram(bins = 10, fill = \"green\", alpha = 0.7) +\n  geom_vline(xintercept = toolik_stats$mean_length, color = \"red\", linewidth = 1) +\n  labs(title = \"Distribution of Sample Means (n = 10)\",\n       subtitle = paste(\"Red line = population mean =\", round(toolik_stats$mean_length, 1), \"mm\"),\n       x = \"Sample Mean (mm)\",\n       y = \"Frequency\")"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#standard-error",
    "href": "lectures/lecture_04/04_02_class_activity.html#standard-error",
    "title": "04_Class_Activity",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error of the mean tells us how much we expect sample means to vary:\n\\[ SE = \\frac{s}{\\sqrt{n}} \\]\n\n# Calculate the standard error from our samples\nsample_se &lt;- sd(sample_means)\n\n# Calculate the theoretical standard error\ntheoretical_se &lt;- toolik_stats$sd_length / sqrt(10)\n\n# Display both values\ncat(\"Standard Error from samples:\", round(sample_se, 2), \"mm\\n\")\n\nStandard Error from samples: 3.55 mm\n\ncat(\"Standard Error from formula:\", round(theoretical_se, 2), \"mm\\n\")\n\nStandard Error from formula: 3.8 mm\n\n\n\n\n\n\n\n\nActivity 3\n\n\n\nLet’s see how sample size affects the standard error. Run the code below to take samples of size 5 and size 20.\n\n# Function to take multiple samples of a given size\ntake_samples &lt;- function(sample_size, n_samples = 20) {\n  replicate(n_samples, {\n    sculpin_df %&gt;%\n      filter(lake == \"Toolik\") %&gt;%\n      sample_n(sample_size) %&gt;%\n      summarize(mean_length = mean(total_length_mm, na.rm = TRUE)) %&gt;%\n      pull(mean_length)\n  })\n}\n\n# Take samples of size 5 and calculate SE\nsample_means_5 &lt;- take_samples(5)\nse_5 &lt;- sd(sample_means_5)\n\n# Take samples of size 20 and calculate SE\nsample_means_20 &lt;- take_samples(20)\nse_20 &lt;- sd(sample_means_20)\n\n# Print results\ncat(\"SE with n=5:\", round(se_5, 2), \"mm\\n\")\n\nSE with n=5: 5.07 mm\n\ncat(\"SE with n=20:\", round(se_20, 2), \"mm\\n\")\n\nSE with n=20: 3.21 mm\n\n# How does SE change as sample size increases?"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#calculating-confidence-intervals",
    "href": "lectures/lecture_04/04_02_class_activity.html#calculating-confidence-intervals",
    "title": "04_Class_Activity",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\nA confidence interval gives us a range of values that likely contains the true population mean. For a 95% confidence interval:\n\\[ 95\\% \\text{ CI} = \\bar{x} \\pm 1.96 \\times SE \\]\nLet’s calculate the 95% confidence interval for the mean length of Toolik Lake fish:\n\n# Calculate 95% confidence interval using z (normal distribution)\nci_lower &lt;- toolik_stats$mean_length - 1.96 * toolik_stats$se_length\nci_upper &lt;- toolik_stats$mean_length + 1.96 * toolik_stats$se_length\n\n# Display the confidence interval\ncat(\"95% Confidence Interval:\", \n    round(ci_lower, 1), \"to\", round(ci_upper, 1), \"mm\\n\")\n\n95% Confidence Interval: 50.1 to 53.3 mm"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#visualizing-confidence-intervals",
    "href": "lectures/lecture_04/04_02_class_activity.html#visualizing-confidence-intervals",
    "title": "04_Class_Activity",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals\n\n# Create a data frame with our statistics\ntoolik_ci &lt;- data.frame(\n  lake = \"Toolik\",\n  mean = toolik_stats$mean_length,\n  lower = ci_lower,\n  upper = ci_upper\n)\n\n# Plot the mean with error bars showing the 95% CI\nggplot(toolik_ci, aes(x = lake, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  labs(title = \"Mean Fish Length with 95% Confidence Interval\",\n       x = NULL,\n       y = \"Mean Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 4\n\n\n\nCalculate and visualize the 95% confidence interval for another lake of your choice.\n\n# Choose another lake\nother_lake &lt;- \"E 01\"  # You can change this to any lake in the dataset\n\n# Calculate statistics for your chosen lake\nother_lake_stats &lt;- sculpin_df %&gt;%\n  filter(lake == other_lake) %&gt;%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n)\n  )\n\n# Display the statistics\nother_lake_stats\n\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1        58.2      15.3    79      1.72\n\n# Calculate 95% confidence interval\nother_ci_lower &lt;- other_lake_stats$mean_length - 1.96 * other_lake_stats$se_length\nother_ci_upper &lt;- other_lake_stats$mean_length + 1.96 * other_lake_stats$se_length\n\n# Display the confidence interval\ncat(\"95% Confidence Interval for\", other_lake, \":\", \n    round(other_ci_lower, 1), \"to\", round(other_ci_upper, 1), \"mm\\n\")\n\n95% Confidence Interval for E 01 : 54.8 to 61.6 mm\n\n# Create a visualization\n# Add your code here to create a similar errorbar plot"
  },
  {
    "objectID": "lectures/lecture_04/04_02_class_activity.html#two-sample-t-test",
    "href": "lectures/lecture_04/04_02_class_activity.html#two-sample-t-test",
    "title": "04_Class_Activity",
    "section": "Two-Sample t-test",
    "text": "Two-Sample t-test\nNow let’s compare fish lengths between two lakes:\n\n# Get data for Toolik and E 01 lakes\ntoolik_fish &lt;- sculpin_df %&gt;% \n  filter(lake == \"Toolik\", !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\ne01_fish &lt;- sculpin_df %&gt;% \n  filter(lake == \"E 01\", !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\n# Perform a two-sample t-test\nt_test_result &lt;- t.test(toolik_fish, e01_fish)\n\n# Display the result\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  toolik_fish and e01_fish\nt = -3.4051, df = 116.36, p-value = 0.0009082\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -10.313036  -2.727921\nsample estimates:\nmean of x mean of y \n 51.69471  58.21519 \n\n# Create a boxplot comparison\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\"), !is.na(total_length_mm)) %&gt;%\n  ggplot(aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(title = \"Comparing Fish Lengths Between Lakes\",\n       subtitle = paste(\"t-test p-value =\", round(t_test_result$p.value, 4)),\n       x = \"Lake\",\n       y = \"Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 6\n\n\n\nRun a t-test to compare fish lengths between two different lakes of your choice.\n\n# Choose two lakes to compare\nlake1 &lt;- \"S 06\"  # You can change this\nlake2 &lt;- \"Toolik\"  # You can change this\n\n# Get the data for both lakes\nlake1_fish &lt;- sculpin_df %&gt;% \n  filter(lake == lake1, !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\nlake2_fish &lt;- sculpin_df %&gt;% \n  filter(lake == lake2, !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\n# Run the t-test\nmy_t_test &lt;- t.test(lake1_fish, lake2_fish)\n\n# Display the result\nmy_t_test\n\n\n    Welch Two Sample t-test\n\ndata:  lake1_fish and lake2_fish\nt = 1.8374, df = 298.76, p-value = 0.06714\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1648305  4.8057105\nsample estimates:\nmean of x mean of y \n 54.01515  51.69471 \n\n# Interpret your results:\n# - What is the p-value?\n# - Is there a significant difference between the lakes at α = 0.05?\n# - Which lake has longer fish on average?"
  },
  {
    "objectID": "lectures/lecture_05/06_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_05/06_01_lecture_powerpoint_html.html",
    "title": "Lecture 06 Homework",
    "section": "",
    "text": "Homework take-up\n\n\n\nPerform 2-sample t-test:\n\npop A: 5.3, 5.6, 4.3, 4.9, 5.3, 4.1, 5.2, 5.0 cm\npop B: 6.1, 4.7, 5.9, 4.7, 6.2, 6.0, 5.4, 4.9 cm\n\nȳpopA - ȳpopB = -0.53\nsȳopA-ȳopB = 0.29\nt = -1.80\ndf = 14\np (estimated from t-table) = 0.05 &lt; p &lt; 0.1\nWriteup: “a 2-tailed, independent 2-sampe t-test showed no significant difference bw beak length of pop A (4.96 cm ± 0.52 SD) and pop B (5.49 ± 0.64) at á=0.05: t(14) =-1.80, p = 0.094”\n\n\n\n\n\n\n\nHomework take-up\n\n\n\nLook through the ecological literature and find an example of a published manuscript that uses either a t-test of one of the tests mentioned in Q2. Provide the following information:\n\nReference for paper\nScientific question being addressed\nSpecific hypothesis tested (in mathematical notation)\nThe results of the t test (t, df, p) and the author’s conclusions\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html",
    "href": "lectures/lecture_05/05_02_class_activity.html",
    "title": "05_Class_Activity",
    "section": "",
    "text": "Created and interpreted frequency distributions with histograms\nCompared lakes using side-by-side histograms\nExplored how sample size affects our view of a population\nCreated density plots and calculated probabilities\n\n\n\n\n\nUnderstanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "href": "lectures/lecture_05/05_02_class_activity.html#what-did-we-do-last-time-in-activity-3",
    "title": "05_Class_Activity",
    "section": "",
    "text": "Created and interpreted frequency distributions with histograms\nCompared lakes using side-by-side histograms\nExplored how sample size affects our view of a population\nCreated density plots and calculated probabilities"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#todays-focus",
    "href": "lectures/lecture_05/05_02_class_activity.html#todays-focus",
    "title": "05_Class_Activity",
    "section": "",
    "text": "Understanding standard normal distributions and z-scores\nCalculating and interpreting standard error\nCreating confidence intervals\nWorking with the Student’s t-distribution"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#setup",
    "href": "lectures/lecture_05/05_02_class_activity.html#setup",
    "title": "05_Class_Activity",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataset:\n\n# Load required packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(patchwork)\n\n# Read in the data file\nsculpin_df &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Look at the first few rows\nhead(sculpin_df)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9 \n\n\nLet’s calculate some basic statistics for Toolik Lake fish:\n\n# Calculate basic statistics for Toolik Lake\ntoolik_stats &lt;- sculpin_df %&gt;% \n  filter(lake == \"Toolik\") %&gt;% \n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n)\n  )\n\n# Display the statistics\ntoolik_stats\n\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1        51.7      12.0   208     0.834"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#converting-to-z-scores",
    "href": "lectures/lecture_05/05_02_class_activity.html#converting-to-z-scores",
    "title": "05_Class_Activity",
    "section": "Converting to Z-scores",
    "text": "Converting to Z-scores\nZ-scores tell us how many standard deviations a data point is from the mean. The formula is:\n\\[ Z = \\frac{X - \\mu}{\\sigma} \\]\nLet’s calculate z-scores for the Toolik Lake fish:\n\n# Add a z-score column to our Toolik Lake data\ntoolik_z &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  mutate(z_score = (total_length_mm - toolik_stats$mean_length) / toolik_stats$sd_length)\n\n# Display the first few rows with z-scores\nhead(toolik_z)\n\n# A tibble: 6 × 6\n   site lake   species       total_length_mm mass_g z_score\n  &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1   100 Toolik slimy sculpin            12.5 0.0152  -3.26 \n2   100 Toolik slimy sculpin            13   0.0104  -3.22 \n3   100 Toolik slimy sculpin            74   2.3      1.85 \n4   100 Toolik slimy sculpin            42   0.5     -0.806\n5   100 Toolik slimy sculpin            39   0.4     -1.06 \n6   100 Toolik slimy sculpin            71   3        1.61 \n\n\nNow, let’s create a histogram of the original data and the z-scores side by side:\n\n# Create histogram of original data\np1 &lt;- toolik_z %&gt;%\n  ggplot(aes(total_length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Original Scale\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n# Create histogram of z-scores\np2 &lt;- toolik_z %&gt;%\n  ggplot(aes(z_score)) +\n  geom_histogram(binwidth = 0.2, fill = \"red\", alpha = 0.7) +\n  geom_vline(xintercept = 0, linewidth = 1) +\n  geom_vline(xintercept = c(-1, 1), color = \"black\", linetype = \"dashed\") +\n  labs(title = \"Z-Score Scale\",\n       x = \"Standard deviations from mean (z-score)\",\n       y = \"Count\")\n\n# Combine plots\np1 / p2\n\nWarning: Removed 79 rows containing non-finite outside the scale range (`stat_bin()`).\nRemoved 79 rows containing non-finite outside the scale range (`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nCalculate the z-score for a fish with a length of 30mm from Toolik Lake. Is this fish length common or unusual?\n\n# Calculate the z-score for a 30mm fish\nfish_length &lt;- 30\nz_score_30mm &lt;- (fish_length - toolik_stats$mean_length) / toolik_stats$sd_length\n\n# Print the result\nz_score_30mm\n\n[1] -1.804193\n\n# Determine if this is common or unusual\n# A z-score beyond -2 or 2 is generally considered unusual"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#understanding-areas-under-the-standard-normal-curve",
    "href": "lectures/lecture_05/05_02_class_activity.html#understanding-areas-under-the-standard-normal-curve",
    "title": "05_Class_Activity",
    "section": "Understanding Areas Under the Standard Normal Curve",
    "text": "Understanding Areas Under the Standard Normal Curve\nThe standard normal distribution has known probabilities associated with z-scores. We can calculate these probabilities in R:\n\n# Calculate probability of z-score less than 1\npnorm(1)  # This gives us the area under the standard normal curve to the left of z=1\n\n[1] 0.8413447\n\n# Calculate probability of z-score between -1 and 1\npnorm(1) - pnorm(-1)  # This is approximately 68% (the 68-95-99.7 rule)\n\n[1] 0.6826895\n\n\n\n\n\n\n\n\nActivity 2\n\n\n\nUsing the pnorm() function, answer: 1. What proportion of fish in Toolik Lake are shorter than 40mm?\n\n# First, convert 40mm to a z-score\nz_score_40mm &lt;- (40 - toolik_stats$mean_length) / toolik_stats$sd_length\nz_score_40mm\n\n[1] -0.9725651\n\n# Then, find the proportion of fish shorter than 40mm\nproportion_less_than_40mm &lt;- pnorm(z_score_40mm)\nproportion_less_than_40mm\n\n[1] 0.1653847"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#taking-multiple-samples",
    "href": "lectures/lecture_05/05_02_class_activity.html#taking-multiple-samples",
    "title": "05_Class_Activity",
    "section": "Taking Multiple Samples",
    "text": "Taking Multiple Samples\nWhen we take multiple samples from a population, each sample gives a slightly different estimate of the population mean. Let’s see this in action:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Take a small sample\nsmall_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(10)\n\n# Calculate the mean of this sample\nsmall_sample_mean &lt;- mean(small_sample$total_length_mm, na.rm = TRUE)\n\n# Print the result\ncat(\"Sample mean (n=10):\", small_sample_mean, \"mm\\n\")\n\nSample mean (n=10): 50.42857 mm\n\ncat(\"Population mean:\", toolik_stats$mean_length, \"mm\\n\")\n\nPopulation mean: 51.69471 mm\n\ncat(\"Difference:\", abs(small_sample_mean - toolik_stats$mean_length), \"mm\\n\")\n\nDifference: 1.26614 mm\n\n\nLet’s take multiple small samples and see how their means vary:\n\n# Function to take a sample and calculate its mean\nget_sample_mean &lt;- function() {\n  sculpin_df %&gt;%\n    filter(lake == \"Toolik\") %&gt;%\n    sample_n(10) %&gt;%\n    summarize(mean_length = mean(total_length_mm, na.rm = TRUE)) %&gt;%\n    pull(mean_length)\n}\n\n# Take 20 samples\nsample_means &lt;- replicate(20, get_sample_mean())\n\n# Create a histogram of the sample means\ntibble(sample_means = sample_means) %&gt;%\n  ggplot(aes(sample_means)) +\n  geom_histogram(bins = 10, fill = \"green\", alpha = 0.7) +\n  geom_vline(xintercept = toolik_stats$mean_length, color = \"red\", linewidth = 1) +\n  labs(title = \"Distribution of Sample Means (n = 10)\",\n       subtitle = paste(\"Red line = population mean =\", round(toolik_stats$mean_length, 1), \"mm\"),\n       x = \"Sample Mean (mm)\",\n       y = \"Frequency\")"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#standard-error",
    "href": "lectures/lecture_05/05_02_class_activity.html#standard-error",
    "title": "05_Class_Activity",
    "section": "Standard Error",
    "text": "Standard Error\nThe standard error of the mean tells us how much we expect sample means to vary:\n\\[ SE = \\frac{s}{\\sqrt{n}} \\]\n\n# Calculate the standard error from our samples\nsample_se &lt;- sd(sample_means)\n\n# Calculate the theoretical standard error\ntheoretical_se &lt;- toolik_stats$sd_length / sqrt(10)\n\n# Display both values\ncat(\"Standard Error from samples:\", round(sample_se, 2), \"mm\\n\")\n\nStandard Error from samples: 3.55 mm\n\ncat(\"Standard Error from formula:\", round(theoretical_se, 2), \"mm\\n\")\n\nStandard Error from formula: 3.8 mm\n\n\n\n\n\n\n\n\nActivity 3\n\n\n\nLet’s see how sample size affects the standard error. Run the code below to take samples of size 5 and size 20.\n\n# Function to take multiple samples of a given size\ntake_samples &lt;- function(sample_size, n_samples = 20) {\n  replicate(n_samples, {\n    sculpin_df %&gt;%\n      filter(lake == \"Toolik\") %&gt;%\n      sample_n(sample_size) %&gt;%\n      summarize(mean_length = mean(total_length_mm, na.rm = TRUE)) %&gt;%\n      pull(mean_length)\n  })\n}\n\n# Take samples of size 5 and calculate SE\nsample_means_5 &lt;- take_samples(5)\nse_5 &lt;- sd(sample_means_5)\n\n# Take samples of size 20 and calculate SE\nsample_means_20 &lt;- take_samples(20)\nse_20 &lt;- sd(sample_means_20)\n\n# Print results\ncat(\"SE with n=5:\", round(se_5, 2), \"mm\\n\")\n\nSE with n=5: 5.07 mm\n\ncat(\"SE with n=20:\", round(se_20, 2), \"mm\\n\")\n\nSE with n=20: 3.21 mm\n\n# How does SE change as sample size increases?"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#calculating-confidence-intervals",
    "href": "lectures/lecture_05/05_02_class_activity.html#calculating-confidence-intervals",
    "title": "05_Class_Activity",
    "section": "Calculating Confidence Intervals",
    "text": "Calculating Confidence Intervals\nA confidence interval gives us a range of values that likely contains the true population mean. For a 95% confidence interval:\n\\[ 95\\% \\text{ CI} = \\bar{x} \\pm 1.96 \\times SE \\]\nLet’s calculate the 95% confidence interval for the mean length of Toolik Lake fish:\n\n# Calculate 95% confidence interval using z (normal distribution)\nci_lower &lt;- toolik_stats$mean_length - 1.96 * toolik_stats$se_length\nci_upper &lt;- toolik_stats$mean_length + 1.96 * toolik_stats$se_length\n\n# Display the confidence interval\ncat(\"95% Confidence Interval:\", \n    round(ci_lower, 1), \"to\", round(ci_upper, 1), \"mm\\n\")\n\n95% Confidence Interval: 50.1 to 53.3 mm"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#visualizing-confidence-intervals",
    "href": "lectures/lecture_05/05_02_class_activity.html#visualizing-confidence-intervals",
    "title": "05_Class_Activity",
    "section": "Visualizing Confidence Intervals",
    "text": "Visualizing Confidence Intervals\n\n# Create a data frame with our statistics\ntoolik_ci &lt;- data.frame(\n  lake = \"Toolik\",\n  mean = toolik_stats$mean_length,\n  lower = ci_lower,\n  upper = ci_upper\n)\n\n# Plot the mean with error bars showing the 95% CI\nggplot(toolik_ci, aes(x = lake, y = mean)) +\n  geom_point(size = 3) +\n  geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +\n  labs(title = \"Mean Fish Length with 95% Confidence Interval\",\n       x = NULL,\n       y = \"Mean Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 4\n\n\n\nCalculate and visualize the 95% confidence interval for another lake of your choice.\n\n# Choose another lake\nother_lake &lt;- \"E 01\"  # You can change this to any lake in the dataset\n\n# Calculate statistics for your chosen lake\nother_lake_stats &lt;- sculpin_df %&gt;%\n  filter(lake == other_lake) %&gt;%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n)\n  )\n\n# Display the statistics\nother_lake_stats\n\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;     &lt;dbl&gt;\n1        58.2      15.3    79      1.72\n\n# Calculate 95% confidence interval\nother_ci_lower &lt;- other_lake_stats$mean_length - 1.96 * other_lake_stats$se_length\nother_ci_upper &lt;- other_lake_stats$mean_length + 1.96 * other_lake_stats$se_length\n\n# Display the confidence interval\ncat(\"95% Confidence Interval for\", other_lake, \":\", \n    round(other_ci_lower, 1), \"to\", round(other_ci_upper, 1), \"mm\\n\")\n\n95% Confidence Interval for E 01 : 54.8 to 61.6 mm\n\n# Create a visualization\n# Add your code here to create a similar errorbar plot"
  },
  {
    "objectID": "lectures/lecture_05/05_02_class_activity.html#two-sample-t-test",
    "href": "lectures/lecture_05/05_02_class_activity.html#two-sample-t-test",
    "title": "05_Class_Activity",
    "section": "Two-Sample t-test",
    "text": "Two-Sample t-test\nNow let’s compare fish lengths between two lakes:\n\n# Get data for Toolik and E 01 lakes\ntoolik_fish &lt;- sculpin_df %&gt;% \n  filter(lake == \"Toolik\", !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\ne01_fish &lt;- sculpin_df %&gt;% \n  filter(lake == \"E 01\", !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\n# Perform a two-sample t-test\nt_test_result &lt;- t.test(toolik_fish, e01_fish)\n\n# Display the result\nt_test_result\n\n\n    Welch Two Sample t-test\n\ndata:  toolik_fish and e01_fish\nt = -3.4051, df = 116.36, p-value = 0.0009082\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -10.313036  -2.727921\nsample estimates:\nmean of x mean of y \n 51.69471  58.21519 \n\n# Create a boxplot comparison\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\"), !is.na(total_length_mm)) %&gt;%\n  ggplot(aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(title = \"Comparing Fish Lengths Between Lakes\",\n       subtitle = paste(\"t-test p-value =\", round(t_test_result$p.value, 4)),\n       x = \"Lake\",\n       y = \"Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 6\n\n\n\nRun a t-test to compare fish lengths between two different lakes of your choice.\n\n# Choose two lakes to compare\nlake1 &lt;- \"S 06\"  # You can change this\nlake2 &lt;- \"Toolik\"  # You can change this\n\n# Get the data for both lakes\nlake1_fish &lt;- sculpin_df %&gt;% \n  filter(lake == lake1, !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\nlake2_fish &lt;- sculpin_df %&gt;% \n  filter(lake == lake2, !is.na(total_length_mm)) %&gt;%\n  pull(total_length_mm)\n\n# Run the t-test\nmy_t_test &lt;- t.test(lake1_fish, lake2_fish)\n\n# Display the result\nmy_t_test\n\n\n    Welch Two Sample t-test\n\ndata:  lake1_fish and lake2_fish\nt = 1.8374, df = 298.76, p-value = 0.06714\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.1648305  4.8057105\nsample estimates:\nmean of x mean of y \n 54.01515  51.69471 \n\n# Interpret your results:\n# - What is the p-value?\n# - Is there a significant difference between the lakes at α = 0.05?\n# - Which lake has longer fish on average?"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html",
    "title": "Lecture 02",
    "section": "",
    "text": "We covered inductive vs deductive reasoning\nHow to begin to ask questions\nAccuracy and precision\nWhat are general types of data\nHow to set up an R project in Rstudio\nHow to install and load libraries\nHow to read a file into R\nHow to make a graph\n\n\nOur first graph"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#be-consistent",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#be-consistent",
    "title": "Lecture 02",
    "section": "Be consistent!",
    "text": "Be consistent!\n\nVariable names\n\nCodes for categorical variables\nVariable names\n\nuse snake case and lower case - nitrate_n_mgl\nalways use the same name\n\nCodes for missing values - NA or 9999 or a space - I know but I do it\nDate formats -\n\nYYYY-MM-DD HH:MM:SS\nTime begins in 1970-01-01\n\nnames of objects\n\ndataframes after import data_df\nplots - len_wt_plot\nmodels - anova_wt_model\n\nFile names\n\nuse separators - 2025_02_01_lake_x_inflow.csv\n\n\nNote format Requires considerable foresight and organization"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#variable-and-file-names-can-be-a-problem",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#variable-and-file-names-can-be-a-problem",
    "title": "Lecture 02",
    "section": "Variable and file names can be a problem",
    "text": "Variable and file names can be a problem\n\nAvoid spaces but use underscore _\nAvoid special characters @#$%^@#\nBe sure to also use a variety of separators so you can separate later\n\nor use the same number of characters across a variable name\n2025_03_04_file-site"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#excel-will-drive-you-mad",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#excel-will-drive-you-mad",
    "title": "Lecture 02",
    "section": "Excel will drive you mad",
    "text": "Excel will drive you mad\n\nit will mess up your dates\nstore data in separate columns - year - month - day\nor use a string 20250401\nalways use unambiguous format of larges to smallest - why?\n\nis 01 04 2025 the same as 04 01 2025\nwhat are the dates in english?\nor European"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#never-do-calculations-in-excel",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#never-do-calculations-in-excel",
    "title": "Lecture 02",
    "section": "Never do Calculations in Excel",
    "text": "Never do Calculations in Excel\n\nalways do calculations in R - reproducible\nnever merge cells\ncan use highlighting but it will disappear\na nice rectangular dataframe will make you happy\n\ntears will flow if not"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#meta-data",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_html.html#meta-data",
    "title": "Lecture 02",
    "section": "Meta Data",
    "text": "Meta Data\n\nThis data will love beyond you\nSomeone will need to interpret it - what do they need\n\nWhat is data about\nWho collected it\nWhen\nWhere\nFunding agency\nMethods used to collect\nVariable names\n\ndescription\nunits\nabbreviations\n\nCALCULATIONS AND WHY?"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#be-consistent",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#be-consistent",
    "title": "Lecture 02",
    "section": "Be consistent!",
    "text": "Be consistent!\n\nVariable names\n\nCodes for categorical variables\nVariable names\n\nuse snake case and lower case - nitrate_n_mgl\nalways use the same name\n\nCodes for missing values - NA or 9999 or a space - I know but I do it\nDate formats -\n\nYYYY-MM-DD HH:MM:SS\nTime begins in 1970-01-01\n\nnames of objects\n\ndataframes after import data_df\nplots - len_wt_plot\nmodels - anova_wt_model\n\nFile names\n\nuse separators - 2025_02_01_lake_x_inflow.csv\n\n\nNote format Requires considerable foresight and organization"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#variable-and-file-names-can-be-a-problem",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#variable-and-file-names-can-be-a-problem",
    "title": "Lecture 02",
    "section": "Variable and file names can be a problem",
    "text": "Variable and file names can be a problem\n\nAvoid spaces but use underscore _\nAvoid special characters @#$%^@#\nBe sure to also use a variety of separators so you can separate later\n\nor use the same number of characters across a variable name\n2025_03_04_file-site"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#excel-will-drive-you-mad",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#excel-will-drive-you-mad",
    "title": "Lecture 02",
    "section": "Excel will drive you mad",
    "text": "Excel will drive you mad\n\nit will mess up your dates\nstore data in separate columns - year - month - day\nor use a string 20250401\nalways use unambiguous format of larges to smallest - why?\n\nis 01 04 2025 the same as 04 01 2025\nwhat are the dates in english?\nor European"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#never-do-calculations-in-excel",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#never-do-calculations-in-excel",
    "title": "Lecture 02",
    "section": "Never do Calculations in Excel",
    "text": "Never do Calculations in Excel\n\nalways do calculations in R - reproducible\nnever merge cells\ncan use highlighting but it will disappear\na nice rectangular dataframe will make you happy\n\ntears will flow if not"
  },
  {
    "objectID": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#meta-data",
    "href": "lectures/lecture_02/02_01_lecture_powerpoint_slides.html#meta-data",
    "title": "Lecture 02",
    "section": "Meta Data",
    "text": "Meta Data\n\nThis data will love beyond you\nSomeone will need to interpret it - what do they need\n\nWhat is data about\nWho collected it\nWhen\nWhere\nFunding agency\nMethods used to collect\nVariable names\n\ndescription\nunits\nabbreviations\n\nCALCULATIONS AND WHY?"
  },
  {
    "objectID": "lectures/lecture_01/01_1_lecture_powerpoint_html.html",
    "href": "lectures/lecture_01/01_1_lecture_powerpoint_html.html",
    "title": "01_Lecture",
    "section": "",
    "text": "Lecture 1: Syllabus\n\nPlease look over the syllabus as it has all the details of the class and how it will run.\n\n\n\n\nLecture 1: Who am I?\n\n\n\nBill Perry\nOffice is in XXXX\nPhone is XXXX\nEmail is wlperry@d.umn.edu\n\n\n\n\n\n\n\n\nLecture 1: My goals\n\nHow do we make observations and hypotheses?\nHow do we design an experiment\nHow do we collect data?\nHow do we organize, clean, summarize, and view the data?\nHow do we use statistics to test our hypotheses\n\nwhat tests to use\nwhat are the assumptions\nwhat are the interpretations\n\n\n\n\nLecture 1: My expectations\n\nCommunication\nPractice\nFailure\nLearn to correct and troubleshoot\n\n\n\nLecture 1: Science\n\nWay to acquire knowledge, organize it and apply it back to the real world\nMake predictions and testing these predictions using a falsifiable approach - statistics\nExplanations that cannot be falsified are not science\n\n\n\nWhat is Statistics?\n\n\nZar (1999) - “analysis and interpretation of data with view towards objective evaluation of conclusions based on the data”\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nInductive Reasoning (Specific → General)\nInductive reasoning involves observing specific cases and using them to form a general conclusion.\nExample:\n\nMeasure 10 pine needles from a tree - average length is 75 mm.\nMeasure 10 more needles from the same tree and gets similar results.\nMeasures needles from second tree - average length is 120 mm .\nYou generalize pine needles from different trees vary in length, but each tree tends to have a characteristic range.\n\nConclusion (Induction): “Pine needle length varies by tree, but each tree seems to have a typical range of lengths.\nPotential Issue: Conclusion is not guaranteed to be true - based on patterns observed in a sample, and there could be exceptions.\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nDeductive Reasoning (General → Specific)\nDeductive reasoning starts with a general principle and applies it to a specific case.\nExample:\n\nGeneral Principle: Pine needles from a species of pine tree have a predictable length range (e.g., 70–80 mm).\nSpecific Case: Collect sample of pine needles and measure them.\nPrediction: Since its the species the needle lengths should fall within 70–80 mm.\nMeasurement: Check the data and confirm needles fall within this expected range.\n\nConclusion (Deduction): “This tree belongs the species with a needle length range of 70–80 mm, we expect its needle lengths to fall in this range.”\nStronger than induction because it’s based on a general rule—but if the assumption (length range) is incorrect, conclusion could still be wrong.\n\n\n\n\n\n\n\n\n\n\n\nLecture 1: Inductive vs deductive reasoning\n\n\n\nIn reality we are doing both of these processes\n\n\n\n\n\n\n\n\n\n\n\nHow do we test hypotheses\n\n\n\nStatistics\n\nDesign good experiments\nDesign good tests\nSummarize patterns/data\nUse to make probabilistic determinations to see if differences are “real”\n\n\n\n\n\n\n\n\nData Types\n\n\n\nContinuous\n\nnumeric\n\ndiscrete\n\ninteger or numerical\n\ncategorical\n\nnominal – up, down, right, left…\nordinal – order - a, b, c, d or morning, afternoon, evening\n\n\n\n\n\n\n\n\nMeasurements\n\n\nData is obtained through measurement\nThe world is a messy place and how you measure matters\nOur measures depend on\n\naccuracy - how close we are to the real value\nprecision - how close all our measurements are but may not be precise\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "lectures/lecture_06/06_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_06/06_01_lecture_powerpoint_html.html",
    "title": "Lecture 06 H testing and simple tests II",
    "section": "",
    "text": "Brief review\n\n\n\nH test for a single population\n1- and 2-sided tests\nH test for two populations\nAssumptions of parametric tests\n\n\n\n\n\n\n\nLecture 5 overview\n\n\n\nAssumptions of parametric tests\nStatistical vs. biological significance\nRobust tests\nRank-based tests\nPermutation tests\nAssignment 1\n\n\nLake Trout\n\nsource\n\nGrayling\n\n\n\n\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nT-tests are parametric tests\nParametric tests: specify/assume probability distribution from which parameters came\nNon-parametric tests: no assumption about probability distribution\nMukasa et al 2021 DOI: 10.4236/ojbm.2021.93081\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nIf assumptions of parametric test violated, test becomes unreliable\nThis is because test statistic may no longer follow distribution\nMost parametric tests robust to mild/moderate violations of below assumptions\n\n\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nBasic assumptions of parametric t-tests:\nNormality, equal variance, random sampling, no outliers\nNormality: Samples from normally distributed population\n\nGraphical tests: histograms, dotplots, boxplots, qq-plots\n“Formal” tests: Shapiro-Wilk test\n\n\n\n&lt;\n\n\n\n\nAssumptions of parametric tests\n\n\n\nEqual variance: samples are from populations with similar degree of variability\n\nGraphical tests: boxplots\n“Formal” tests: F-ratio test\n\nParametric tests most robust to violations of normality and equal var. assumptions when samples sizes equal\n\n\n\n\n\n\n\nAssumptions of parametric tests\n\n\n\nNormality, equal variance, random sampling, no outliers\nRandom sampling: samples are randomly collected from populations; part of experimental design\nNecessary for sample -&gt; population inference\n\n\n&lt;&gt;\n\n\n\n\nAssumptions of parametric tests\n\n\n\nNormality, equal variance, random sampling, no outliers\nNo outliers: no “extreme” values that are very different from rest of sample\nGraphical tests: boxplots, histograms\n“Formal tests”: Grubb’s test\nNote: outliers also problem for non-parametric tests\n\n\n&lt;&gt;\n\n\n\n\nStatistical vs. biological significance\n\n\n\nStatistical significance: difference unlikely due to chance\nSays nothing about biological significance of difference!\nWith large sample size can detect very small differences between populations\nE.g.: consider 2 snail populations,\n\nA and B:\n\nHo: µ~size A~ = µ~size B~\nHa: µ~size A~ ≠ µ~size B~\n\n\n\n\n&lt;!–  –&gt;\n\n\n\n\nStatistical vs. biological significance\n\n\n\nSize of A: 5.05 (± 2.00 SD)mm, size of B: 5.00 (± 2.00 SD)mm\nSample 50, 200, 30,000 individuals from each pop:\n\nn = 50: t = 0.32, df = 98, p-value = 0.75\nn = 200: t = 0.058, df = 398, p-value = 0.95\nn = 30,000: t = -4.47, df = 59998, p-value = 7.996*10-6\n\n\n\n\n\n\n\n\nStatistical vs. biological significance\n\n\n\nFinally, statistically significant difference…\nMeaningful? Ecologically significant? Statistics can’t answer this question\nIMPORTANT to report info that can assess biological significance\n“A two-tailed, two-sample independent t-test showed significant difference in size between pop. A (4.99 mm ± 1.99 SD) and pop. B (5.06 mm ± 1.99 SD) at á=0.05 (t = -4.47, df = 59998, p-value &lt; 0.0001).”\n\n\n&lt;!– –&gt;\n\n\n\n\nAssumptions of parametric tests\n\n\n\nBasic assumptions of parametric t-tests:\nNormality, equal variance, random sampling, no outliers\nWhat to do if assumptions are violated?\n\n\n&lt;&gt;\n\n\n\n\nHomework take-up\n\n\n\nt-tests have several assumptions. Alternative tests, with more relaxed assumptions, are available to statisticians. In which case would you use the following tests?\n\nWelch’s t-test: when distribution normal but variance unequal\nPermutation test for two samples: when distribution not normal (but both groups should still have similar distributions and ~equal variance)\nMann-Whitney-Wilcoxon test: when distribution not normal and/or outliers are present (but both groups should still have similar distributions and ~equal variance)\n\n\n\n&lt; –&gt;\n\n\n\n\nAssumptions of parametric tests\n\n\n\nQQ-plots: tool for assessing normality\nOn x- theoretical quantiles from SND\nOn y- ordered sample values\nDeviation from normal can be detected as deviation from straight line\n\n\n&lt;!– –&gt;\n\n\n\n\nAssumptions of parametric tests\n\n\n\nIn some cases, data can be mathematically “transformed” to meet assumptions of parametric tests\n\n\n\n\n\n\n\nRobust tests\n\n\n\nWelch’s t-test: common “robust” test for means of two populations\nRobust to violation of equal variance assumption, deals better with unequal sample size\nParametric test (assumes normal distribution)\nCalculates a t statistic but recalculates df based on samples sizes and s\n\n\n&lt;!– –&gt;\n\n\n\n\nRobust tests\n\n\n\nIn R:\nt.test(y1, y2, var.equal = FALSE, paired = FALSE)\nwill use the Welch approach\nT-test\nAvB df= 38 t= -3.62 p= 0.0009\nAvC df= 38 t= -2.91 p= 0.005\n\nWelch’s\n\nAvB df= 37.9 t= -3.62 p= 0.0009\nAvC df= 26.1 t= -2.91 p= 0.007\n\n\n&lt;!–  –&gt;\n\n\n\n\nRank based tests\n\n\n\nRank-based tests: no assumptions about distribution (non-parametric)\nRanks of data: observations assigned ranks, sums (and signs for paired tests) of ranks for groups compared\nMann-Whitney U test common alternative to independent samples t-test\nWilcoxon signed-rank test is alternative to paired t-test\n\n\n\n\n\n\n\nRank based tests\n\n\n\nAssumptions: similar distributions for groups, equal variance\nLess power than parametric tests\nBest when normality assumption can not be met by transformation (weird distribution) or large outliers\n\nA: n= 15, y= 8, s= 4 B : n= 15, y= 10, s= 5\nApproach A vs. B\nT-test df= 28 t= -3.53 p= 0.0014 M-W U (Wilcoxon’s) W= 41 p= 0.002\n\n\n\n\n\n\nPermutation tests\n\n\n\nPermutation tests based on resampling: reshuffling of original data\nResampling allows parameter estimation when distribution unknown, including SEs and CIs of statistics (means, medians)\nCommon approach is bootstrap: resample sample with replacement many times, recalculate sample stats\n\n\n\n\n\n\n\nPermutation tests\n\n\n\nSample A: n = 40, ȳ= 1.72, s = 4.17\nSample B: n = 35, ȳ= 4.50, s = 4.83\nHo: µA = µB, Ha: µA ≠µB\nCalculate ∆ in means between two groups (2.78)\n\n\n&lt;!– –&gt;\n\n\n\n\nPermutation tests\n\n\n\nRandomly reshuffle observations between groups (keeping nA=40 and nB=35), calculate ∆\nRepeat &gt;1,000 times\nRecord proportion of the ∆means is ≥2.94 µmol\nThis is equivalent to p-value and can be used in “traditional” H test framework\nFor a graphical explanation:\n\nGraphical Explanation\n\n\n\n\n\n\n\n\nPermutation tests\n\n\n\nIn R (using ‘perm’ package):\npermTS(y1, y2, alternative = “two.sided”, method = “exact.mc”, control = permControl(nmc = 10000))\nAssumptions: both groups have similar distribution; equal variance\n\n\n\n\n\n\n\nR practice\n\n\n\nGet practice doing basic t-tests\nAlternatives in next lecture\nDataset (squirrel_data.csv) and lab instructions on Canvas\nAnswer questions in bold\nDue end of Thursday\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Helpful Links",
    "section": "",
    "text": "These are the really helpful links I use a lot\nStackoverflow - a site that has a lot of good resources\nR Companion - this is a great stats site with a lot of well done tests\nR4DataScience - this is the book by Grolemund and Whickham that is really good\nA fun site is BlueSky and use #rstats\nStill adding\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "This course teaches the fundamentals of statistics and data analysis for biological sciences. Using R and the tidyverse, you’ll learn how to analyze and visualize data effectively to answer scientific questions.\n\n\nMaterials for this course are organized into lectures, in-class activities, and homework assignments. All materials are available through this website and may be on canvas website at school.\n\n\n\nBelow is a summary of the course schedule.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndownloadable class activity code\n\n\n\n01 Introduction\nR4DataScience Intro\nR4DataScience Visualizaiton\nLecture 1 PowerPoint\nLecture 1 Activity\nWeek 1 Homework\n\n\n\n\n02 Data wrangling\nR4DataScience Tidy Data\nLecture 2 PowerPoint\nLecture 2 Activity\n\n\n\n\n\n03 Probability Distributions and estimation\n\nLecture 3 PowerPoint\nLecture 3 Activity\nWeek 3 Homework\n\n\n\n\n04 Standard normal distribution + SE + 95% CI\n\nLecture 4 ppt\nLecture 4 activity\n\n\n\n\n\n05 Statistical hypothesis testing\n\nLecture 5 ppt\nLecture 5 activity\nWeek 4 Homework\nLecture_05_downloadable\n\n\n\n06 xxx\n\nLecture 6 ppt\n\n\n\n\n\n\n07 xxx\n\nLecture 7 ppt\nLecture 7 activity\nWeek 5 Homework\n\n\n\n\n08 xxx\n\nLecture 8 ppt\nLecture 8 activity\n\n\n\n\n\n\n\nOrder of topics is subject to change depending on the progress of the class. Changes to the class schedule will be announced in class.\n\n\n\n\n\nR for Data Science by Hadley Wickham\nWhitlock and Schluter The Analysis of Biological Data - 3rd edition\n\n\n\n\nI have put together a page of helpful links that might be of great help as you move through the class\n\n\n\n\n\n\n\n\n\nIf you have questions about the course or the website, please contact me at wlperry@d.umn.edu"
  },
  {
    "objectID": "index.html#course-materials",
    "href": "index.html#course-materials",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "Materials for this course are organized into lectures, in-class activities, and homework assignments. All materials are available through this website and may be on canvas website at school."
  },
  {
    "objectID": "index.html#course-schedule",
    "href": "index.html#course-schedule",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "Below is a summary of the course schedule.\n\n\n\n\n\n\n\n\n\n\n\n\n\ndownloadable class activity code\n\n\n\n01 Introduction\nR4DataScience Intro\nR4DataScience Visualizaiton\nLecture 1 PowerPoint\nLecture 1 Activity\nWeek 1 Homework\n\n\n\n\n02 Data wrangling\nR4DataScience Tidy Data\nLecture 2 PowerPoint\nLecture 2 Activity\n\n\n\n\n\n03 Probability Distributions and estimation\n\nLecture 3 PowerPoint\nLecture 3 Activity\nWeek 3 Homework\n\n\n\n\n04 Standard normal distribution + SE + 95% CI\n\nLecture 4 ppt\nLecture 4 activity\n\n\n\n\n\n05 Statistical hypothesis testing\n\nLecture 5 ppt\nLecture 5 activity\nWeek 4 Homework\nLecture_05_downloadable\n\n\n\n06 xxx\n\nLecture 6 ppt\n\n\n\n\n\n\n07 xxx\n\nLecture 7 ppt\nLecture 7 activity\nWeek 5 Homework\n\n\n\n\n08 xxx\n\nLecture 8 ppt\nLecture 8 activity\n\n\n\n\n\n\n\nOrder of topics is subject to change depending on the progress of the class. Changes to the class schedule will be announced in class."
  },
  {
    "objectID": "index.html#required-readings",
    "href": "index.html#required-readings",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "R for Data Science by Hadley Wickham\nWhitlock and Schluter The Analysis of Biological Data - 3rd edition"
  },
  {
    "objectID": "index.html#helpful-links-for-resources",
    "href": "index.html#helpful-links-for-resources",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "I have put together a page of helpful links that might be of great help as you move through the class"
  },
  {
    "objectID": "index.html#questions-or-issues",
    "href": "index.html#questions-or-issues",
    "title": "UMD Biostatistics",
    "section": "",
    "text": "If you have questions about the course or the website, please contact me at wlperry@d.umn.edu"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "Welch’s t-test (also known as Welch’s unequal variances t-test) is an adaptation of the standard two-sample t-test that is designed to provide a valid test when the two groups have unequal variances. This is particularly important because the assumption of equal variances is often violated in real-world data.\nWhile the standard two-sample t-test makes the following comparison:\n\\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1 \\neq \\mu_2\\)\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group\n\n\n\nThe formula for Welch’s t-test is:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_1^2\\) is the sample variance of the first group - \\(s_2^2\\) is the sample variance of the second group - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe key difference from the standard t-test is that Welch’s t-test does not use a pooled variance estimate, making it more robust when the variances differ between groups.\nThe degrees of freedom for Welch’s t-test are calculated using the Welch-Satterthwaite equation:\n\\(df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\)\nThis often results in a non-integer value for degrees of freedom, which is why you’ll typically see it rounded in reports."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#background-and-theory",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#background-and-theory",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "Welch’s t-test (also known as Welch’s unequal variances t-test) is an adaptation of the standard two-sample t-test that is designed to provide a valid test when the two groups have unequal variances. This is particularly important because the assumption of equal variances is often violated in real-world data.\nWhile the standard two-sample t-test makes the following comparison:\n\\(H_0: \\mu_1 = \\mu_2\\) \\(H_A: \\mu_1 \\neq \\mu_2\\)\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#formula-for-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#formula-for-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "",
    "text": "The formula for Welch’s t-test is:\n\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\)\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_1^2\\) is the sample variance of the first group - \\(s_2^2\\) is the sample variance of the second group - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe key difference from the standard t-test is that Welch’s t-test does not use a pooled variance estimate, making it more robust when the variances differ between groups.\nThe degrees of freedom for Welch’s t-test are calculated using the Welch-Satterthwaite equation:\n\\(df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\)\nThis often results in a non-integer value for degrees of freedom, which is why you’ll typically see it rounded in reports."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#loading-libraries-and-data",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#loading-libraries-and-data",
    "title": "Welchs Two Sample T-Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\nlibrary(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\n\nLoading required package: survival\n\nlibrary(rcompanion)  # For plotNormalHistogramlibrary(tidyverse)\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#data-overview",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#data-overview",
    "title": "Welchs Two Sample T-Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site           : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake           : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species        : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ total_length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g         : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   total_length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species          total_length_mm \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n           site            lake         species total_length_mm          mass_g \n             79               0               0             329               0"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#data-preparation",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#data-preparation",
    "title": "Welchs Two Sample T-Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(total_length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(total_length_mm),\n    sd = sd(total_length_mm),\n    se = sd / sqrt(n),\n    var = var(total_length_mm)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 6\n  lake      n  mean    sd    se   var\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72  110.\n2 S 07     73  55.6  12.7  1.48  160."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#mean-and-standard-error-plot",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#mean-and-standard-error-plot",
    "title": "Welchs Two Sample T-Test",
    "section": "Mean and Standard Error Plot",
    "text": "Mean and Standard Error Plot\nLet’s also create a plot showing the mean and standard error for each lake, with # Data Visualization\nLet’s visualize our data to better understand the distributions and differences between the two lakes:"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#box-plot-with-individual-data-points",
    "title": "Welchs Two Sample T-Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points show the actual measurements, helping us visualize the full distribution of the data. in the background:\n\n# Calculate means and standard errors\nlake_means &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean = mean(total_length_mm),\n    se = sd(total_length_mm) / sqrt(n())\n  )\n\n# Create mean and standard error plot with data points\nggplot() +\n  # Add individual data points in the background\n  geom_point(data = sculpin_filtered, \n             aes(x = lake, y = total_length_mm, color = lake),\n             position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  geom_point(data = lake_means, \n             aes(x = lake, y = mean, color = lake),\n             size = 4) +\n  geom_errorbar(data = lake_means, \n                aes(x = lake, ymin = mean - se, ymax = mean + se, color = lake),\n                width = 0.2, size = 1) +\n  # Add annotations for the means\n  geom_text(data = lake_means,\n            aes(x = lake, y = mean + se + 3, \n                label = paste0(round(mean, 1), \" ± \", round(se, 1), \" mm\")),\n            size = 3.5) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\nMean and Standard Error Plot\nNow, let’s create a plot showing the mean and standard error for each lake, with individual data points in the background:\n\n# Create mean and standard error plot with data points\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, color = lake)) +\n  # Add individual data points in the background\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#assumptions-of-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#assumptions-of-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "Assumptions of Welch’s t-Test",
    "text": "Assumptions of Welch’s t-Test\n\nIndependence: The observations within each group are independent, and the two groups are independent of each other.\nNormality: The data in each group follow approximately normal distributions (though Welch’s t-test is more robust to violations of normality than the standard t-test).\n\nUnlike the standard t-test, Welch’s t-test does not assume that the variances of the two groups are equal. This makes it more appropriate for many real-world datasets.\nLet’s test the assumptions we do need to meet:\n\n1. Independence Assumption\nIndependence is a design issue and can’t be tested statistically. We assume our sampling design ensures independence between and within groups.\n\n\n2. Normality Assumption\nWe’ll check normality using: - Visual methods: Histograms and Q-Q plots - Formal test: Shapiro-Wilk test\n\nHistograms\n\n# Create histograms for both lakes\npar(mfrow = c(1, 2))\n\n# note that this is base r plotting\n\n# Lake S 07\nhist(s07_data$total_length_mm, \n     main = \"Histogram of Total Length for Lake S 07\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# Lake NE 14\nhist(ne14_data$total_length_mm, \n     main = \"Histogram of Total Length for Lake NE 14\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n\n\n\n\n\n\n\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n\nQQ Plots\n\n# QQ plot for Lake S 07\nggplot(data = s07_data, aes(sample = total_length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake S 07\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# QQ plot for Lake NE 14\nggplot(data = ne14_data, aes(sample = total_length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake NE 14\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n# Shapiro-Wilk test for Lake S 07\nshapiro_s07 &lt;- shapiro.test(s07_data$total_length_mm)\nprint(\"Shapiro-Wilk test for Lake S 07:\")\n\n[1] \"Shapiro-Wilk test for Lake S 07:\"\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$total_length_mm\nW = 0.98035, p-value = 0.3125\n\n# Shapiro-Wilk test for Lake NE 14\nshapiro_ne14 &lt;- shapiro.test(ne14_data$total_length_mm)\nprint(\"Shapiro-Wilk test for Lake NE 14:\")\n\n[1] \"Shapiro-Wilk test for Lake NE 14:\"\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$total_length_mm\nW = 0.9479, p-value = 0.08258\n\n# Summary table\nshapiro_results &lt;- data.frame(\n  Lake = c(\"S 07\", \"NE 14\"),\n  W_statistic = c(shapiro_s07$statistic, shapiro_ne14$statistic),\n  p_value = c(shapiro_s07$p.value, shapiro_ne14$p.value),\n  is_normal = c(shapiro_s07$p.value &gt; 0.05, shapiro_ne14$p.value &gt; 0.05)\n)\n\nknitr::kable(shapiro_results, caption = \"Shapiro-Wilk Test Results\")\n\n\nShapiro-Wilk Test Results\n\n\nLake\nW_statistic\np_value\nis_normal\n\n\n\n\nS 07\n0.9803526\n0.3125255\nTRUE\n\n\nNE 14\n0.9479006\n0.0825839\nTRUE\n\n\n\n\n\n\n\n\n3. Homogeneity of Variances\nWe’ll check for homogeneity of variances using: - Visual inspection of boxplots (already done above) - Levene’s test\n\n# Calculate variances for each group\ns07_variance &lt;- var(s07_data$total_length_mm)\nne14_variance &lt;- var(ne14_data$total_length_mm)\n\n# Print variances\ncat(\"Variance for Lake S 07:\", s07_variance, \"\\n\")\n\nVariance for Lake S 07: 160.0552 \n\ncat(\"Variance for Lake NE 14:\", ne14_variance, \"\\n\")\n\nVariance for Lake NE 14: 109.9805 \n\n# Calculate variance ratio\nvariance_ratio &lt;- max(s07_variance, ne14_variance) / min(s07_variance, ne14_variance)\ncat(\"Variance ratio (larger/smaller):\", variance_ratio, \"\\n\")\n\nVariance ratio (larger/smaller): 1.455305 \n\n# Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(total_length_mm ~ lake, data = sculpin_filtered)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_test)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1   2.029 0.1572\n      108"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-assumption-tests",
    "title": "Welchs Two Sample T-Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests:\n\nIndependence: We assume this is met based on the data collection process, as samples from each lake were collected independently of one another.\nNormality:\n\nThe Q-Q plots show that the data points largely follow the theoretical normal distribution line for both lakes, with some minor deviations at the extremes.\nThe Shapiro-Wilk test results will help us formally assess normality. If the p-value is greater than 0.05, we fail to reject the null hypothesis that the data is normally distributed.\nFor samples larger than 30, the Central Limit Theorem suggests that the sampling distribution of means will be approximately normal regardless of the underlying distribution.\n\nHomogeneity of Variances:\n\nLevene’s test evaluates whether the variances between groups are equal.\nA p-value greater than 0.05 indicates that we cannot reject the null hypothesis of equal variances.\nAs a rule of thumb, if the variance ratio is less than 4:1, the t-test is reasonably robust to violations of this assumption.\nIf this assumption is violated, we should consider using Welch’s t-test instead, which does not assume equal variances."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#line-by-line-interpretation-of-welchs-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#line-by-line-interpretation-of-welchs-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Line-by-Line Interpretation of Welch’s t-Test Results",
    "text": "Line-by-Line Interpretation of Welch’s t-Test Results\nLet’s break down the output from the Welch’s t-test:\n\nTest Type: “Welch Two Sample t-test” indicates we’re using the Welch’s version of the t-test, which does not assume equal variances.\nFormula: total_length_mm ~ lake means we’re testing if total length differs by lake.\nData: Our filtered sculpin dataset.\nt-value: The calculated t-statistic. This is the ratio of the difference between group means to the standard error of that difference.\nDegrees of Freedom (df): For Welch’s t-test, this is calculated using the Welch-Satterthwaite equation and is typically not a whole number. This adjustment accounts for the different variances.\np-value: The probability of observing a t-statistic as extreme as (or more extreme than) the one we calculated, assuming the null hypothesis is true. A p-value less than our significance level (typically 0.05) leads us to reject the null hypothesis.\nAlternative Hypothesis: States that the difference in means is not equal to 0, which corresponds to our two-sided test.\n95% Confidence Interval: The estimated range for the true difference in means. If this interval does not contain 0, it supports rejecting the null hypothesis.\nSample Estimates: The means of each group being compared."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#visual-representation-of-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#visual-representation-of-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Visual Representation of t-Test Results",
    "text": "Visual Representation of t-Test Results\n\n# Create a plot with the t-test results\np_value_text &lt;- ifelse(p_value &lt; 0.001, \"p &lt; 0.001\", paste(\"p =\", round(p_value, 3)))\n\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  annotate(\"text\", x = 1.5, y = max(sculpin_filtered$total_length_mm) + 5,\n           label = paste0(\"Welch's t-test: t(\", round(df, 1), \") = \", t_statistic, \", \", p_value_text),\n           size = 4) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    subtitle = \"With Welch's t-test Results\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-welchs-t-test-results",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#interpretation-of-welchs-t-test-results",
    "title": "Welchs Two Sample T-Test",
    "section": "Interpretation of Welch’s t-Test Results",
    "text": "Interpretation of Welch’s t-Test Results\nBased on our analysis, we can conclude:\nThe total length of slimy sculpin fish differs significantly between Lake S 07 and Lake NE 14 (Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001). Fish from Lake S 07 were on average 8.29 mm longer than those from Lake NE 14 (mean ± SE: 55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm).\nWelch’s t-test was appropriate for this analysis because:\n\nOur data from both lakes appeared to be approximately normally distributed (as seen in the QQ plots and confirmed by the Shapiro-Wilk test).\nOur samples were independent, with fish collected randomly from each lake.\nThe variances between the two groups were somewhat different (variance ratio of 1.46), making Welch’s t-test preferable to the standard t-test.\n\nThe significant p-value (p &lt; 0.001) indicates that the observed difference in fish length between lakes is very unlikely to have occurred by chance alone if there were truly no difference in the population means. The 95% confidence interval for the mean difference does not include zero, which further supports rejecting the null hypothesis."
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "title": "Welchs Two Sample T-Test",
    "section": "How to Report These Results in a Scientific Publication",
    "text": "How to Report These Results in a Scientific Publication\nWhen reporting these results in a scientific publication, follow this format:\n“Slimy sculpin (Cottus cognatus) from Lake S 07 were significantly larger than those from Lake NE 14 (55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm, respectively; Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001). This represents an approximately 17.5% difference in total length between the two populations.”\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption for the mean/SE plot would read:\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (Welch’s t-test: t(85.4) = -3.65, p &lt; 0.001).”"
  },
  {
    "objectID": "test_overviews/02_welches_two_smple_ttest_html.html#advantages-of-using-welchs-t-test",
    "href": "test_overviews/02_welches_two_smple_ttest_html.html#advantages-of-using-welchs-t-test",
    "title": "Welchs Two Sample T-Test",
    "section": "Advantages of Using Welch’s t-Test",
    "text": "Advantages of Using Welch’s t-Test\nWelch’s t-test offers several advantages over the standard t-test:\n\nRobustness to unequal variances: Welch’s t-test does not assume equal variances between groups, making it more appropriate for real-world data where this assumption is often violated.\nMinimal loss of power: When variances are equal, Welch’s t-test performs nearly as well as the standard t-test.\nReduced Type I error rate: When variances are unequal, the standard t-test can have an inflated Type I error rate (false positives), which Welch’s t-test corrects.\nFlexibility: It can be used regardless of whether the variances are equal or not, making it a more versatile statistical test.\n\nFor these reasons, many statisticians recommend using Welch’s t-test as the default approach for comparing means between two independent groups, even when the homogeneity of variance assumption appears to be met.\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption would read:\nagain adding in the mean +/- SE for each lake would be better\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (two-sample t-test: t(108) = 3.46, p &lt; 0.001).”"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html",
    "href": "test_overviews/two_sample_ttest_html.html",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The two-sample t-test (also known as independent samples t-test) is used to determine whether there is a statistically significant difference between the means of two independent groups. In this analysis, we will examine whether there are significant differences in the total length of slimy sculpin fish between two different lakes.\nThe two-sample t-test makes the following comparison:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group\n\n\n\nThe formula for the two-sample t-test with equal variances (pooled variance) is:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_p\\) is the pooled standard deviation - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe pooled standard deviation is calculated as:\n\\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\]\nWhere: - \\(s_1^2\\) is the variance of the first group - \\(s_2^2\\) is the variance of the second group\nThe degrees of freedom (df) for this test is \\(n_1 + n_2 - 2\\).\nFor unequal variances (Welch’s t-test), the formula is slightly different:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWith degrees of freedom approximated using the Welch-Satterthwaite equation:\n\\[df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\]"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#background-and-theory",
    "href": "test_overviews/two_sample_ttest_html.html#background-and-theory",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The two-sample t-test (also known as independent samples t-test) is used to determine whether there is a statistically significant difference between the means of two independent groups. In this analysis, we will examine whether there are significant differences in the total length of slimy sculpin fish between two different lakes.\nThe two-sample t-test makes the following comparison:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\nWhere: - \\(H_0\\) is the null hypothesis stating that the population means are equal - \\(H_A\\) is the alternative hypothesis stating that the population means are different - \\(\\mu_1\\) is the population mean of the first group - \\(\\mu_2\\) is the population mean of the second group"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#formula",
    "href": "test_overviews/two_sample_ttest_html.html#formula",
    "title": "Two Sample T-Test",
    "section": "",
    "text": "The formula for the two-sample t-test with equal variances (pooled variance) is:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{s_p \\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\]\nWhere: - \\(\\bar{x}_1\\) is the sample mean of the first group - \\(\\bar{x}_2\\) is the sample mean of the second group - \\(s_p\\) is the pooled standard deviation - \\(n_1\\) is the sample size of the first group - \\(n_2\\) is the sample size of the second group\nThe pooled standard deviation is calculated as:\n\\[s_p = \\sqrt{\\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1 + n_2 - 2}}\\]\nWhere: - \\(s_1^2\\) is the variance of the first group - \\(s_2^2\\) is the variance of the second group\nThe degrees of freedom (df) for this test is \\(n_1 + n_2 - 2\\).\nFor unequal variances (Welch’s t-test), the formula is slightly different:\n\\[t = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\\]\nWith degrees of freedom approximated using the Welch-Satterthwaite equation:\n\\[df = \\frac{(\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2})^2}{\\frac{(s_1^2/n_1)^2}{n_1-1} + \\frac{(s_2^2/n_2)^2}{n_2-1}}\\]"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#loading-libraries-and-data",
    "href": "test_overviews/two_sample_ttest_html.html#loading-libraries-and-data",
    "title": "Two Sample T-Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\nlibrary(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\n\nLoading required package: survival\n\nlibrary(rcompanion)  # For plotNormalHistogram\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-overview",
    "href": "test_overviews/two_sample_ttest_html.html#data-overview",
    "title": "Two Sample T-Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site           : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake           : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species        : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ total_length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g         : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   total_length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species          total_length_mm \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n           site            lake         species total_length_mm          mass_g \n             79               0               0             329               0"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-preparation",
    "href": "test_overviews/two_sample_ttest_html.html#data-preparation",
    "title": "Two Sample T-Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(total_length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake\nsculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    count = n(),\n    mean_length = mean(total_length_mm),\n    sd_length = sd(total_length_mm),\n    se_length = sd_length / sqrt(count)\n  )\n\n# A tibble: 2 × 5\n  lake  count mean_length sd_length se_length\n  &lt;chr&gt; &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 NE 14    37        47.3      10.5      1.72\n2 S 07     73        55.6      12.7      1.48"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#data-visualization",
    "href": "test_overviews/two_sample_ttest_html.html#data-visualization",
    "title": "Two Sample T-Test",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nBox Plot with Individual Data Points\nLet’s create a box plot with individual data points to visualize the distribution of total length in the two lakes:\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\n\n\nMean and Standard Error Plot\nNow, let’s create a plot showing the mean and standard error for each lake, with individual data points in the background:\n\n# Create mean and standard error plot with data points\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, color = lake)) +\n  # Add individual data points in the background\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 1.5) +\n  # Add mean and standard error\n  stat_summary(fun = mean, geom = \"point\", size = 4) +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.1) +\n  labs(\n    title = \"Mean Total Length (± SE) of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    color = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_color_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#assumptions-of-the-two-sample-t-test",
    "href": "test_overviews/two_sample_ttest_html.html#assumptions-of-the-two-sample-t-test",
    "title": "Two Sample T-Test",
    "section": "Assumptions of the Two-Sample t-Test",
    "text": "Assumptions of the Two-Sample t-Test\n\nIndependence: The observations within each group are independent, and the two groups are independent of each other.\nNormality: The data in each group follow a normal distribution.\nHomogeneity of Variances: The variances of the two groups are approximately equal (for the standard t-test).\n\nLet’s test each of these assumptions:\n\n1. Independence Assumption\nIndependence is a design issue and can’t be tested statistically. We assume our sampling design ensures independence between and within groups.\n\n\n2. Normality Assumption\nWe’ll check normality using: - Histograms - Q-Q plots - Shapiro-Wilk test\n\nHistograms\n\n# Create histograms for both lakes\npar(mfrow = c(1, 2))\n\n# Lake S 07\nhist(s07_data$total_length_mm, \n     main = \"Histogram of Total Length for Lake S 07\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightblue\",\n     breaks = 10)\n\n# Lake NE 14\nhist(ne14_data$total_length_mm, \n     main = \"Histogram of Total Length for Lake NE 14\",\n     xlab = \"Total Length (mm)\",\n     col = \"lightgreen\",\n     breaks = 8)\n\n\n\n\n\n\n\n\n\n\n\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n\n\nQQ Plots\nLet’s create individual QQ plots for each lake:\n\n# QQ plot for Lake S 07\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nggplot(data = s07_data, aes(sample = total_length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake S 07\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n# QQ plot for Lake NE 14\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\nggplot(data = ne14_data, aes(sample = total_length_mm)) +\n  stat_qq() +\n  stat_qq_line() +\n  labs(\n    title = \"Q-Q Plot for Lake NE 14\",\n    x = \"Theoretical Quantiles\",\n    y = \"Sample Quantiles\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nShapiro-Wilk Test\n\n# Shapiro-Wilk test for Lake S 07\nshapiro_s07 &lt;- shapiro.test(s07_data$total_length_mm)\nprint(\"Shapiro-Wilk test for Lake S 07:\")\n\n[1] \"Shapiro-Wilk test for Lake S 07:\"\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$total_length_mm\nW = 0.98035, p-value = 0.3125\n\n# Shapiro-Wilk test for Lake NE 14\nshapiro_ne14 &lt;- shapiro.test(ne14_data$total_length_mm)\nprint(\"Shapiro-Wilk test for Lake NE 14:\")\n\n[1] \"Shapiro-Wilk test for Lake NE 14:\"\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$total_length_mm\nW = 0.9479, p-value = 0.08258\n\n# Summary table\nshapiro_results &lt;- data.frame(\n  Lake = c(\"S 07\", \"NE 14\"),\n  W_statistic = c(shapiro_s07$statistic, shapiro_ne14$statistic),\n  p_value = c(shapiro_s07$p.value, shapiro_ne14$p.value),\n  is_normal = c(shapiro_s07$p.value &gt; 0.05, shapiro_ne14$p.value &gt; 0.05)\n)\n\nknitr::kable(shapiro_results, caption = \"Shapiro-Wilk Test Results\")\n\n\nShapiro-Wilk Test Results\n\n\nLake\nW_statistic\np_value\nis_normal\n\n\n\n\nS 07\n0.9803526\n0.3125255\nTRUE\n\n\nNE 14\n0.9479006\n0.0825839\nTRUE\n\n\n\n\n\n\n\n3. Homogeneity of Variances\nWe’ll check for homogeneity of variances using: - Visual inspection of boxplots (already done above) - Levene’s test\n\n# Calculate variances for each group\ns07_variance &lt;- var(s07_data$total_length_mm)\nne14_variance &lt;- var(ne14_data$total_length_mm)\n\n# Print variances\ncat(\"Variance for Lake S 07:\", s07_variance, \"\\n\")\n\nVariance for Lake S 07: 160.0552 \n\ncat(\"Variance for Lake NE 14:\", ne14_variance, \"\\n\")\n\nVariance for Lake NE 14: 109.9805 \n\n# Calculate variance ratio\nvariance_ratio &lt;- max(s07_variance, ne14_variance) / min(s07_variance, ne14_variance)\ncat(\"Variance ratio (larger/smaller):\", variance_ratio, \"\\n\")\n\nVariance ratio (larger/smaller): 1.455305 \n\n# Levene's test for homogeneity of variances\nlevene_test &lt;- leveneTest(total_length_mm ~ lake, data = sculpin_filtered)\n\nWarning in leveneTest.default(y = y, group = group, ...): group coerced to\nfactor.\n\nprint(levene_test)\n\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value Pr(&gt;F)\ngroup   1   2.029 0.1572\n      108"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/two_sample_ttest_html.html#interpretation-of-assumption-tests",
    "title": "Two Sample T-Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests:\n\nIndependence: We assume this is met based on the data collection process, as samples from each lake were collected independently of one another.\nNormality:\n\nThe Q-Q plots show that the data points largely follow the theoretical normal distribution line for both lakes, with some minor deviations at the extremes.\nThe Shapiro-Wilk test results will help us formally assess normality. If the p-value is greater than 0.05, we fail to reject the null hypothesis that the data is normally distributed.\nFor samples larger than 30, the Central Limit Theorem suggests that the sampling distribution of means will be approximately normal regardless of the underlying distribution.\n\nHomogeneity of Variances:\n\nLevene’s test evaluates whether the variances between groups are equal.\nA p-value greater than 0.05 indicates that we cannot reject the null hypothesis of equal variances.\nAs a rule of thumb, if the variance ratio is less than 4:1, the t-test is reasonably robust to violations of this assumption.\nIf this assumption is violated, we should consider using Welch’s t-test instead, which does not assume equal variances."
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#line-by-line-interpretation-of-t-test-results",
    "href": "test_overviews/two_sample_ttest_html.html#line-by-line-interpretation-of-t-test-results",
    "title": "Two Sample T-Test",
    "section": "Line-by-Line Interpretation of t-Test Results",
    "text": "Line-by-Line Interpretation of t-Test Results\nLet’s break down the t-test output:\n\nTest Type: Two Sample t-test\nFormula: total_length_mm ~ lake means we’re testing if total length differs by lake\nData: Our filtered sculpin dataset\nt-value: The calculated t-statistic\nDegrees of Freedom (df): n₁ + n₂ - 2\np-value: The probability of observing this data (or more extreme) if the null hypothesis is true\nAlternative Hypothesis: The means are different\n95% Confidence Interval: The estimated range for the true difference in means\nSample Estimates: The means of each group"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#visual-representation-of-t-test-results",
    "href": "test_overviews/two_sample_ttest_html.html#visual-representation-of-t-test-results",
    "title": "Two Sample T-Test",
    "section": "Visual Representation of t-Test Results",
    "text": "Visual Representation of t-Test Results\n\n# Create a plot with the t-test results\np_value_text &lt;- ifelse(p_value &lt; 0.001, \"p &lt; 0.001\", paste(\"p =\", round(p_value, 3)))\n\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  annotate(\"text\", x = 1.5, y = max(sculpin_filtered$total_length_mm) + 5,\n           label = paste0(\"t(\", round(df), \") = \", t_statistic, \", \", p_value_text),\n           size = 4) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake with t-test Results\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\",\n    caption = paste0(\"n(S 07) = \", nrow(s07_data), \", n(NE 14) = \", nrow(ne14_data))\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")"
  },
  {
    "objectID": "test_overviews/two_sample_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "href": "test_overviews/two_sample_ttest_html.html#how-to-report-these-results-in-a-scientific-publication",
    "title": "Two Sample T-Test",
    "section": "How to Report These Results in a Scientific Publication",
    "text": "How to Report These Results in a Scientific Publication\nWhen reporting these results in a scientific publication, follow this format:\n“Slimy sculpin (Cottus cognatus) from Lake S 07 were significantly larger than those from Lake NE 14\n(55.56 ± 1.48 mm vs. 47.27 ± 1.72 mm, respectively; two-sample t-test: t(108) = -3.43, p &lt; 0.001). This represents an approximately 17.5% difference in total length between the two populations.”\nFor figures, include:\n\nA boxplot or mean/SE plot showing the difference\nClear labels and scales\nSample sizes\nStatistical test information in the figure caption\n\nA typical caption would read:\nNote I would also add the mean and SE of each lake\n“Figure X. Total length (mean ± SE) of slimy sculpin fish from two Arctic lakes. Fish from Lake S 07 (n = 73) were significantly larger than those from Lake NE 14 (n = 37) (two-sample t-test: t(108) = 3.46, p &lt; 0.001).”"
  },
  {
    "objectID": "test_details.html",
    "href": "test_details.html",
    "title": "Statistical Tests",
    "section": "",
    "text": "This is a breif list of the statistical tests we have done.\nI will be adding tests as the course continues\nMy hope is that this will be a resource you can download and use in the future.\n\n\n\nStatistical Test\nwebpage\ncode\n\n\n\n\nTwo Sample Test\n\n\n\n\nParametric Two Sample TTest\nweb\ncode\n\n\nNonParametric Welches Two Sample Test\nweb\ncode\n\n\nNonParametric Permutation Two Sample Test\nweb\ncode\n\n\nNonParametric Mann Whitney U Two Sample Test\nweb\ncode\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "Biostatistics course\nBill Perry\nThis course will serve as an introduction to reproducible data analysis using R. Specifically the goal is to introduce students to all facets of managing a research project with an emphasis on:\n\nDeveloping questions form observations, hypotheses, and predictions for testing statistically\nDesigning data workflows with data entry, curation, QA/QC, and cleaning\nUsing a controlled vocabulary and organized project structure and documenting the metadata for the project\nImporting data into R and doing calculations and transformations\nVisualizing data using ggplot\nUnderstanding how to decide on statistical tests that are appropriate - T-Tests - parametric and nonparametric\n\n-   Correlations\n-   Linear models\n    -   linear regression\n    -   multiple linear regression\n    -   ANOVA\n    -   ANCOVA\n    -   GLM and logistic regression\n-   Analyzing Frequencies\n-   Multivariate Statistics\n    -   Principal component analysis\n    -   NMDS\n    -   Cluster analysis\nIn the main webpage I have provided links to all the information you will need:\n\nlinks to readings that should be read prior to class\npowerpoint lectures that should be reviewed prior to class\nin-class activities that will actively cover the materials in the powerpoints\nweekly homework to do the in class activities on your own using different data\nassignments that form the crux of the grade and there 4 of them\n\n\n\n\n Back to top"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html",
    "href": "test_overviews/03_permutation_two_sample_test_html.html",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "Permutation tests (also known as randomization tests) are non-parametric methods used to test hypotheses without making assumptions about the underlying distribution of the data. This makes them particularly valuable when:\n\nSample sizes are small\nData violate normality assumptions\nData have unequal variances\nOutliers are present\n\nFor comparing two independent groups, a permutation test assesses whether the observed difference between groups is likely to occur by chance if there were no true difference between them.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The two samples come from the same distribution}\\] \\[H_A: \\text{The two samples come from different distributions}\\]\nMore specifically, for comparing means:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]\n\n\n\nThe permutation test follows these steps:\n\nCalculate the observed test statistic (e.g., difference in means) between the two groups.\nRandomly reassign the observations to the two groups, maintaining the original group sizes.\nCalculate the test statistic for this random permutation.\nRepeat steps 2-3 many times (typically 1,000 to 10,000 times) to build a distribution of test statistics under the null hypothesis.\nCalculate the p-value as the proportion of permuted test statistics that are as extreme as or more extreme than the observed test statistic.\n\nThe key insight is that if the null hypothesis is true (no difference between groups), then the group labels are essentially arbitrary, and any permutation of the labels is equally likely."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#background-and-theory",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#background-and-theory",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "Permutation tests (also known as randomization tests) are non-parametric methods used to test hypotheses without making assumptions about the underlying distribution of the data. This makes them particularly valuable when:\n\nSample sizes are small\nData violate normality assumptions\nData have unequal variances\nOutliers are present\n\nFor comparing two independent groups, a permutation test assesses whether the observed difference between groups is likely to occur by chance if there were no true difference between them.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The two samples come from the same distribution}\\] \\[H_A: \\text{The two samples come from different distributions}\\]\nMore specifically, for comparing means:\n\\[H_0: \\mu_1 = \\mu_2\\] \\[H_A: \\mu_1 \\neq \\mu_2\\]"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#how-permutation-tests-work",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#how-permutation-tests-work",
    "title": "Two Sample Permutation Test",
    "section": "",
    "text": "The permutation test follows these steps:\n\nCalculate the observed test statistic (e.g., difference in means) between the two groups.\nRandomly reassign the observations to the two groups, maintaining the original group sizes.\nCalculate the test statistic for this random permutation.\nRepeat steps 2-3 many times (typically 1,000 to 10,000 times) to build a distribution of test statistics under the null hypothesis.\nCalculate the p-value as the proportion of permuted test statistics that are as extreme as or more extreme than the observed test statistic.\n\nThe key insight is that if the null hypothesis is true (no difference between groups), then the group labels are essentially arbitrary, and any permutation of the labels is equally likely."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#loading-libraries-and-data",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#loading-libraries-and-data",
    "title": "Two Sample Permutation Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogram\n\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#data-overview",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#data-overview",
    "title": "Two Sample Permutation Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site           : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake           : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species        : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ total_length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g         : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   total_length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species          total_length_mm \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n           site            lake         species total_length_mm          mass_g \n             79               0               0             329               0"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#data-preparation",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#data-preparation",
    "title": "Two Sample Permutation Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(total_length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(total_length_mm),\n    sd = sd(total_length_mm),\n    se = sd / sqrt(n),\n    var = var(total_length_mm),\n    median = median(total_length_mm),\n    min = min(total_length_mm),\n    max = max(total_length_mm)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 9\n  lake      n  mean    sd    se   var median   min   max\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72  110.     48    25    67\n2 S 07     73  55.6  12.7  1.48  160.     57    31    87"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#box-plot-with-individual-data-points",
    "title": "Two Sample Permutation Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points help us visualize the full distribution of the data."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#density-plots",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#density-plots",
    "title": "Two Sample Permutation Test",
    "section": "Density Plots",
    "text": "Density Plots\nLet’s also create density plots to better visualize the distribution shapes:\n\n# Create density plots\nggplot(sculpin_filtered, aes(x = total_length_mm, fill = lake)) +\n  geom_density(alpha = 0.6) +\n  labs(\n    title = \"Density Distribution of Slimy Sculpin Total Length by Lake\",\n    x = \"Total Length (mm)\",\n    y = \"Density\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe density plots show the continuous distribution of lengths for each lake, helping us visualize the overall shape, central tendency, and spread of the data."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#assessing-normality",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#assessing-normality",
    "title": "Two Sample Permutation Test",
    "section": "Assessing Normality",
    "text": "Assessing Normality\n\n# Create normal quantile plots for each lake with a normal histogram\npar(mfrow = c(1, 2))\n\n# Lake S 07\nplotNormalHistogram(s07_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake S 07\",\n                    xlab = \"Total Length (mm)\")\n\n# Lake NE 14\nplotNormalHistogram(ne14_data$total_length_mm,\n                    main = \"Distribution of Total Length for Lake NE 14\",\n                    xlab = \"Total Length (mm)\")\n\n\n\n\n\n\n\n\n\n# Shapiro-Wilk test for normality\nshapiro_s07 &lt;- shapiro.test(s07_data$total_length_mm)\nshapiro_ne14 &lt;- shapiro.test(ne14_data$total_length_mm)\n\ncat(\"Shapiro-Wilk test for Lake S 07:\\n\")\n\nShapiro-Wilk test for Lake S 07:\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$total_length_mm\nW = 0.98035, p-value = 0.3125\n\ncat(\"\\nShapiro-Wilk test for Lake NE 14:\\n\")\n\n\nShapiro-Wilk test for Lake NE 14:\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$total_length_mm\nW = 0.9479, p-value = 0.08258"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#assessing-variance-equality",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#assessing-variance-equality",
    "title": "Two Sample Permutation Test",
    "section": "Assessing Variance Equality",
    "text": "Assessing Variance Equality\n\n# F-test for equality of variances\nvar_test &lt;- var.test(total_length_mm ~ lake, data = sculpin_filtered)\nprint(var_test)\n\n\n    F test to compare two variances\n\ndata:  total_length_mm by lake\nF = 0.68714, num df = 36, denom df = 72, p-value = 0.218\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n 0.3982241 1.2529495\nsample estimates:\nratio of variances \n          0.687141 \n\n# Calculate variance ratio\nvar_ratio &lt;- max(lake_stats$var) / min(lake_stats$var)\ncat(\"Variance ratio (larger/smaller):\", var_ratio)\n\nVariance ratio (larger/smaller): 1.455305"
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#interpretation-of-assumption-tests",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#interpretation-of-assumption-tests",
    "title": "Two Sample Permutation Test",
    "section": "Interpretation of Assumption Tests",
    "text": "Interpretation of Assumption Tests\nBased on the results of our assumption tests, a permutation test is appropriate because:\n\nNormality: The normal quantile plots and Shapiro-Wilk tests suggest potential departures from normality in the data.\nVariance Equality: The F-test indicates that the variances may not be equal between the two lakes.\nRobustness: Permutation tests are robust to violations of these assumptions and provide a valid test regardless of the underlying distributions."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#understanding-the-permutation-test-results",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#understanding-the-permutation-test-results",
    "title": "Two Sample Permutation Test",
    "section": "Understanding the Permutation Test Results",
    "text": "Understanding the Permutation Test Results\nThe permutation test provides a p-value that represents the probability of observing a difference as extreme as, or more extreme than, the observed difference in means between the two lakes if the null hypothesis were true (i.e., if there were no real difference between lakes).\nOur analysis shows:\n\nObserved Difference: The observed difference in mean total length between Lake S 07 and Lake NE 14 is 8.29 mm.\np-value: The permutation test yielded a p-value of &lt; 0.001 based on 10^{4} random permutations.\nInterpretation: Since the p-value is less than 0.05, we reject the null hypothesis. This indicates that the observed difference in fish length between the two lakes is statistically significant and unlikely to have occurred by chance.\nVisualization: The permutation distribution graph shows the distribution of mean differences we would expect to see under the null hypothesis, with the observed difference marked by the red line. The fact that the observed difference falls in the extreme tails of this distribution supports our conclusion."
  },
  {
    "objectID": "test_overviews/03_permutation_two_sample_test_html.html#advantages-of-the-permutation-test",
    "href": "test_overviews/03_permutation_two_sample_test_html.html#advantages-of-the-permutation-test",
    "title": "Two Sample Permutation Test",
    "section": "Advantages of the Permutation Test",
    "text": "Advantages of the Permutation Test\nThe permutation test offered several advantages for this analysis:\n\nNo Distributional Assumptions: Unlike parametric tests like the t-test, permutation tests don’t require the data to follow a normal distribution.\nRobust to Unequal Variances: Permutation tests are valid even when the two groups have different variances.\nAppropriate for Small Samples: Permutation tests can provide valid inference even with smaller sample sizes.\nIntuitive Interpretation: The permutation approach provides a direct, intuitive way to assess how likely the observed difference is under the null hypothesis.\nExact p-values: With enough permutations, we can get very precise p-values without relying on theoretical approximations."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The Mann-Whitney-Wilcoxon test (also known as the Wilcoxon rank-sum test or Mann-Whitney U test) is a powerful non-parametric alternative to the two-sample t-test. This test is particularly useful when:\n\nThe data do not follow a normal distribution\nThe sample sizes are small\nData are measured on an ordinal scale\nOutliers are present\n\nUnlike the t-test, which compares means, the Mann-Whitney-Wilcoxon test compares the distributions of two independent groups. Specifically, it tests whether one distribution is stochastically greater than the other.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The distributions of both groups are identical}\\] \\[H_A: \\text{The distributions of the two groups differ in location (median)}\\]\n\n\n\nThe test follows these steps:\n\nCombine all observations from both groups and rank them from lowest to highest.\nCalculate the sum of ranks for each group.\nCalculate the U statistic, which represents the number of times observations in one group precede observations in the other group.\nCompare the calculated U statistic to the critical value from the Mann-Whitney-Wilcoxon distribution, or calculate a p-value for larger samples.\n\nThe U statistic is calculated as:\n\\[U_1 = R_1 - \\frac{n_1(n_1 + 1)}{2}\\]\nWhere: - \\(R_1\\) is the sum of ranks in group 1 - \\(n_1\\) is the sample size of group 1\nIf U is sufficiently small or large compared to what would be expected by chance, we reject the null hypothesis."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#background-and-theory",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#background-and-theory",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The Mann-Whitney-Wilcoxon test (also known as the Wilcoxon rank-sum test or Mann-Whitney U test) is a powerful non-parametric alternative to the two-sample t-test. This test is particularly useful when:\n\nThe data do not follow a normal distribution\nThe sample sizes are small\nData are measured on an ordinal scale\nOutliers are present\n\nUnlike the t-test, which compares means, the Mann-Whitney-Wilcoxon test compares the distributions of two independent groups. Specifically, it tests whether one distribution is stochastically greater than the other.\nThe null and alternative hypotheses are:\n\\[H_0: \\text{The distributions of both groups are identical}\\] \\[H_A: \\text{The distributions of the two groups differ in location (median)}\\]"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#how-the-mann-whitney-wilcoxon-test-works",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#how-the-mann-whitney-wilcoxon-test-works",
    "title": "Two Sample Mann_Whitney Test",
    "section": "",
    "text": "The test follows these steps:\n\nCombine all observations from both groups and rank them from lowest to highest.\nCalculate the sum of ranks for each group.\nCalculate the U statistic, which represents the number of times observations in one group precede observations in the other group.\nCompare the calculated U statistic to the critical value from the Mann-Whitney-Wilcoxon distribution, or calculate a p-value for larger samples.\n\nThe U statistic is calculated as:\n\\[U_1 = R_1 - \\frac{n_1(n_1 + 1)}{2}\\]\nWhere: - \\(R_1\\) is the sum of ranks in group 1 - \\(n_1\\) is the sample size of group 1\nIf U is sufficiently small or large compared to what would be expected by chance, we reject the null hypothesis."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#loading-libraries-and-data",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#loading-libraries-and-data",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Loading Libraries and Data",
    "text": "Loading Libraries and Data\n\n# Load required libraries\nlibrary(tidyverse)\nlibrary(car)  # For Levene's test\n\nLoading required package: carData\n\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\nlibrary(ggpubr)  # For adding p-values to plots\nlibrary(coin)  # For permutation tests\nlibrary(rcompanion)  # For plotNormalHistogram\n\n\n# Load the data\nsculpin_data &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the data\nhead(sculpin_data)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-overview",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-overview",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Data Overview",
    "text": "Data Overview\nLet’s first examine the structure of our dataset:\n\n# Structure of the dataset\nstr(sculpin_data)\n\nspc_tbl_ [1,052 × 5] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ site           : num [1:1052] 146 146 146 146 146 146 146 146 146 146 ...\n $ lake           : chr [1:1052] \"E 01\" \"E 01\" \"E 01\" \"E 01\" ...\n $ species        : chr [1:1052] \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" \"slimy sculpin\" ...\n $ total_length_mm: num [1:1052] 53 61 53 77 45 48 51 57 51 56 ...\n $ mass_g         : num [1:1052] 1.25 1.9 1.75 4.25 0.9 0.9 1.05 1.15 1.15 1.3 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   site = col_double(),\n  ..   lake = col_character(),\n  ..   species = col_character(),\n  ..   total_length_mm = col_double(),\n  ..   mass_g = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n# Summary statistics\nsummary(sculpin_data)\n\n      site           lake             species          total_length_mm \n Min.   :100.0   Length:1052        Length:1052        Min.   : 11.00  \n 1st Qu.:107.0   Class :character   Class :character   1st Qu.: 44.00  \n Median :108.0   Mode  :character   Mode  :character   Median : 52.00  \n Mean   :121.8                                         Mean   : 52.44  \n 3rd Qu.:141.0                                         3rd Qu.: 60.00  \n Max.   :152.0                                         Max.   :194.00  \n NA's   :79                                            NA's   :329     \n     mass_g       \n Min.   : 0.0037  \n 1st Qu.: 0.7000  \n Median : 1.1500  \n Mean   : 1.4577  \n 3rd Qu.: 1.7700  \n Max.   :46.0000  \n                  \n\n# Check for missing values\ncolSums(is.na(sculpin_data))\n\n           site            lake         species total_length_mm          mass_g \n             79               0               0             329               0"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-preparation",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#data-preparation",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Data Preparation",
    "text": "Data Preparation\nFor our analysis, we’ll filter the data to include only the two lakes we’re interested in comparing (S 07 and NE 14) and remove any missing values:\n\n# Select lakes for comparison\nlakes_to_compare &lt;- c(\"S 07\", \"NE 14\")\n\n# Filter data\nsculpin_filtered &lt;- sculpin_data %&gt;%\n  filter(lake %in% lakes_to_compare) %&gt;%\n  filter(!is.na(total_length_mm))\n\n# Create individual datasets for each lake\ns07_data &lt;- sculpin_filtered %&gt;% filter(lake == \"S 07\")\nne14_data &lt;- sculpin_filtered %&gt;% filter(lake == \"NE 14\")\n\n# Check the number of observations per lake and get basic statistics\nlake_stats &lt;- sculpin_filtered %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(total_length_mm),\n    sd = sd(total_length_mm),\n    se = sd / sqrt(n),\n    median = median(total_length_mm),\n    min = min(total_length_mm),\n    max = max(total_length_mm),\n    Q1 = quantile(total_length_mm, 0.25),\n    Q3 = quantile(total_length_mm, 0.75)\n  )\n\nprint(lake_stats)\n\n# A tibble: 2 × 10\n  lake      n  mean    sd    se median   min   max    Q1    Q3\n  &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 NE 14    37  47.3  10.5  1.72     48    25    67    42    54\n2 S 07     73  55.6  12.7  1.48     57    31    87    45    64"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#box-plot-with-individual-data-points",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#box-plot-with-individual-data-points",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Box Plot with Individual Data Points",
    "text": "Box Plot with Individual Data Points\n\n# Create boxplot with individual points using position_dodge2\nggplot(sculpin_filtered, aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot(alpha = 0.7, outlier.shape = NA) +\n  geom_point(position = position_dodge2(width = 0.3), \n             alpha = 0.5, size = 2) +\n  labs(\n    title = \"Total Length of Slimy Sculpin Fish by Lake\",\n    x = \"Lake\",\n    y = \"Total Length (mm)\",\n    fill = \"Lake\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe boxplot shows the distribution of total lengths for each lake. The box represents the interquartile range (IQR, from the 25th to 75th percentile), with the horizontal line inside the box indicating the median. The individual points help us visualize the full distribution of the data."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#density-plots-with-median-lines",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#density-plots-with-median-lines",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Density Plots with Median Lines",
    "text": "Density Plots with Median Lines\n\n# Create density plots with median markers\nggplot(sculpin_filtered, aes(x = total_length_mm, fill = lake)) +\n  geom_density(alpha = 0.6) +\n  geom_vline(data = lake_stats, \n             aes(xintercept = median, color = lake),\n             linewidth = 1, linetype = \"dashed\") +\n  labs(\n    title = \"Density Distribution of Slimy Sculpin Total Length by Lake\",\n    subtitle = \"Dashed lines indicate medians\",\n    x = \"Total Length (mm)\",\n    y = \"Density\",\n    fill = \"Lake\",\n    color = \"Median\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5, face = \"bold\"),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"bottom\"\n  ) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\n\n\n\n\n\nThe density plots show the continuous distribution of lengths for each lake, with dashed lines marking the median values. This visualization is particularly relevant for the Mann-Whitney-Wilcoxon test, which compares medians rather than means."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assessing-normality",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assessing-normality",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Assessing Normality",
    "text": "Assessing Normality\n\n# Create QQ plots for each lake\npar(mfrow = c(1, 2))\n\n# Lake S 07\nqqnorm(s07_data$total_length_mm, \n       main = \"Q-Q Plot for Lake S 07\", \n       col = \"blue\")\nqqline(s07_data$total_length_mm)\n\n# Lake NE 14\nqqnorm(ne14_data$total_length_mm, \n       main = \"Q-Q Plot for Lake NE 14\", \n       col = \"green\")\nqqline(ne14_data$total_length_mm)\n\n\n\n\n\n\n\n\n\n# Shapiro-Wilk test for normality\nshapiro_s07 &lt;- shapiro.test(s07_data$total_length_mm)\nshapiro_ne14 &lt;- shapiro.test(ne14_data$total_length_mm)\n\ncat(\"Shapiro-Wilk test for Lake S 07:\\n\")\n\nShapiro-Wilk test for Lake S 07:\n\nprint(shapiro_s07)\n\n\n    Shapiro-Wilk normality test\n\ndata:  s07_data$total_length_mm\nW = 0.98035, p-value = 0.3125\n\ncat(\"\\nShapiro-Wilk test for Lake NE 14:\\n\")\n\n\nShapiro-Wilk test for Lake NE 14:\n\nprint(shapiro_ne14)\n\n\n    Shapiro-Wilk normality test\n\ndata:  ne14_data$total_length_mm\nW = 0.9479, p-value = 0.08258\n\n\nBased on the Q-Q plots and Shapiro-Wilk tests, we can assess whether our data follow a normal distribution. The Mann-Whitney-Wilcoxon test is appropriate regardless of the outcome because it doesn’t assume normality."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assumptions-of-the-mann-whitney-wilcoxon-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#assumptions-of-the-mann-whitney-wilcoxon-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Assumptions of the Mann-Whitney-Wilcoxon Test",
    "text": "Assumptions of the Mann-Whitney-Wilcoxon Test\nThe Mann-Whitney-Wilcoxon test has the following assumptions:\n\nIndependent samples: The observations in each group are independent of each other, and the two groups are independent of each other.\nOrdinal data: The measurements must be at least on an ordinal scale (can be ranked).\nSimilar distributions: If testing for differences in medians specifically, the shapes of the distributions should be similar (though not necessarily normal).\n\nLet’s check if our data meet these assumptions:\n\n# Compare distribution shapes visually\nggplot(sculpin_filtered, aes(x = total_length_mm, fill = lake)) +\n  geom_density(alpha = 0.5) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(\n    title = \"Comparing Distribution Shapes Between Lakes\",\n    x = \"Total Length (mm)\",\n    y = \"Density\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5, face = \"bold\"))\n\n\n\n\n\n\n\n\nIf the distributions have roughly similar shapes (even if they’re shifted), we can interpret the Mann-Whitney-Wilcoxon test as testing for differences in medians. If the shapes differ substantially, the test more broadly examines whether one distribution is stochastically greater than the other."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-base-rs-wilcox.test-function",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-base-rs-wilcox.test-function",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Using Base R’s wilcox.test Function",
    "text": "Using Base R’s wilcox.test Function\n\n# Perform the Mann-Whitney-Wilcoxon test\nwilcox_test &lt;- wilcox.test(total_length_mm ~ lake, \n                          data = sculpin_filtered,\n                          exact = FALSE,  # Use approximate method for larger samples\n                          correct = TRUE)  # Apply continuity correction\n\n# Display the results\nprint(wilcox_test)\n\n\n    Wilcoxon rank sum test with continuity correction\n\ndata:  total_length_mm by lake\nW = 867, p-value = 0.00223\nalternative hypothesis: true location shift is not equal to 0\n\n# Store the p-value for later use\np_value &lt;- wilcox_test$p.value"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-the-coin-package-for-an-exact-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#using-the-coin-package-for-an-exact-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Using the coin Package for an Exact Test",
    "text": "Using the coin Package for an Exact Test\nFor more precise results, especially with smaller samples, we can use the coin package to perform an exact Mann-Whitney-Wilcoxon test:\n\n# Convert lake to factor (required for the coin package)\nsculpin_filtered$lake_factor &lt;- factor(sculpin_filtered$lake)\n\n# Perform the Mann-Whitney test using the approximate method\n# (which works reliably for all sample sizes)\ncoin_wilcox &lt;- coin::wilcox_test(\n  total_length_mm ~ lake_factor,\n  data = sculpin_filtered,\n  distribution = \"approximate\"\n)\n\n# Display the results\nprint(coin_wilcox)\n\n\n    Approximative Wilcoxon-Mann-Whitney Test\n\ndata:  total_length_mm by lake_factor (NE 14, S 07)\nZ = -3.0609, p-value = 0.0023\nalternative hypothesis: true mu is not equal to 0\n\n# Extract the p-value\npvalue_coin &lt;- pvalue(coin_wilcox)\ncat(\"p-value:\", pvalue_coin, \"\\n\")\n\np-value: 0.0023"
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#calculating-effect-size",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#calculating-effect-size",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Calculating Effect Size",
    "text": "Calculating Effect Size\nThe Mann-Whitney-Wilcoxon test tells us whether there’s a statistically significant difference, but it doesn’t indicate the magnitude of that difference. Let’s calculate an effect size measure:\n\n## Calculating Effect Size\n\n# The Mann-Whitney-Wilcoxon test tells us whether there's a statistically significant difference, but it doesn't indicate the magnitude of that difference. Let's calculate an effect size measure:\n\n# Calculate standardized effect size using rank-biserial correlation\n# (equivalent to r = Z / sqrt(N))\nz_score &lt;- qnorm(p_value/2)  # Convert p-value to Z-score\nN &lt;- nrow(sculpin_filtered)\nr &lt;- abs(z_score) / sqrt(N)  # Rank-biserial correlation\n\ncat(\"Effect size (rank-biserial correlation):\", round(r, 3), \"\\n\")\n\nEffect size (rank-biserial correlation): 0.292 \n\n# Interpret effect size\neffect_size &lt;- r\nif(effect_size &lt; 0.1) {\n  effect_interpretation &lt;- \"negligible\"\n} else if(effect_size &lt; 0.3) {\n  effect_interpretation &lt;- \"small\"\n} else if(effect_size &lt; 0.5) {\n  effect_interpretation &lt;- \"moderate\"\n} else if(effect_size &lt; 0.7) {\n  effect_interpretation &lt;- \"large\"\n} else {\n  effect_interpretation &lt;- \"very large\"\n}\n\ncat(\"This represents a\", effect_interpretation, \"effect.\\n\")\n\nThis represents a small effect."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#understanding-the-mann-whitney-wilcoxon-test-results",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#understanding-the-mann-whitney-wilcoxon-test-results",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Understanding the Mann-Whitney-Wilcoxon Test Results",
    "text": "Understanding the Mann-Whitney-Wilcoxon Test Results\nThe Mann-Whitney-Wilcoxon test provides a p-value that represents the probability of observing the rank sum (or a more extreme value) if the null hypothesis were true (i.e., if there were no difference in the distributions of the two lakes).\nOur analysis shows:\n\nObserved Difference: The observed difference in median total length between Lake S 07 and Lake NE 14 is 9 mm.\np-value: The Mann-Whitney-Wilcoxon test yielded a p-value of 0.002.\nEffect Size: The rank-biserial correlation (r = 0.29) indicates a small effect size.\nInterpretation: Since the p-value is less than 0.05, we reject the null hypothesis. This indicates that the distributions of fish lengths between the two lakes are significantly different."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#advantages-of-the-mann-whitney-wilcoxon-test",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#advantages-of-the-mann-whitney-wilcoxon-test",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Advantages of the Mann-Whitney-Wilcoxon Test",
    "text": "Advantages of the Mann-Whitney-Wilcoxon Test\nThe Mann-Whitney-Wilcoxon test offered several advantages for this analysis:\n\nNo Normality Assumption: It doesn’t require the data to follow a normal distribution, making it appropriate for many ecological datasets.\nRobust to Outliers: By using ranks instead of actual values, it’s less sensitive to extreme observations.\nApplicable to Ordinal Data: It can be used even when data are measured on an ordinal rather than interval scale.\nEfficiency: With normally distributed data, the test has 95% efficiency compared to the t-test, but can be more powerful when distributions are non-normal.\nInterpretability: It provides a clear assessment of whether one population tends to have larger values than the other."
  },
  {
    "objectID": "test_overviews/04_mann_whitnely_two_sample_test_html.html#comparison-to-parametric-tests",
    "href": "test_overviews/04_mann_whitnely_two_sample_test_html.html#comparison-to-parametric-tests",
    "title": "Two Sample Mann_Whitney Test",
    "section": "Comparison to Parametric Tests",
    "text": "Comparison to Parametric Tests\nFor comparison, let’s see what a standard t-test would have concluded:\n\n# Perform a t-test for comparison\nt_test_result &lt;- t.test(total_length_mm ~ lake, data = sculpin_filtered)\nprint(t_test_result)\n\n\n    Welch Two Sample t-test\n\ndata:  total_length_mm by lake\nt = -3.6483, df = 85.45, p-value = 0.0004533\nalternative hypothesis: true difference in means between group NE 14 and group S 07 is not equal to 0\n95 percent confidence interval:\n -12.809687  -3.773061\nsample estimates:\nmean in group NE 14  mean in group S 07 \n           47.27027            55.56164 \n\n# Compare p-values\ncat(\"Mann-Whitney-Wilcoxon p-value:\", p_value, \"\\n\")\n\nMann-Whitney-Wilcoxon p-value: 0.002230158 \n\ncat(\"t-test p-value:\", t_test_result$p.value, \"\\n\")\n\nt-test p-value: 0.0004532708 \n\n\nIn this case, both tests lead to the same conclusions regarding statistical significance. However, the Mann-Whitney-Wilcoxon test is more appropriate when normality assumptions are violated, and it’s testing a different hypothesis (difference in distributions rather than means)."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the Instructor",
    "section": "",
    "text": "Department of Biology\nUniversity of Minnesota Duluth\nI am a professor specializing in aquatic biology, biogeochemistry, invasion biology, and biostatistics and data analysis. My research focuses on statistical methods for aquatic ecology but am open to any data and science. I am particularily adept at working wtih high frequency data from field monitors."
  },
  {
    "objectID": "about.html#bill-perry",
    "href": "about.html#bill-perry",
    "title": "About the Instructor",
    "section": "",
    "text": "Department of Biology\nUniversity of Minnesota Duluth\nI am a professor specializing in aquatic biology, biogeochemistry, invasion biology, and biostatistics and data analysis. My research focuses on statistical methods for aquatic ecology but am open to any data and science. I am particularily adept at working wtih high frequency data from field monitors."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About the Instructor",
    "section": "Research Interests",
    "text": "Research Interests\n\nAquatic Ecology - lakes and streams\nInvasion Biology\nEnvironmental Monitoring\nStatistical methods for ecological data\nData visualization"
  },
  {
    "objectID": "about.html#teaching-philosophy",
    "href": "about.html#teaching-philosophy",
    "title": "About the Instructor",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\nMy goal is to make statistics accessible and relevant by doing code and data collection in the classroom. The hands-on approach to learning, using real-world data and practical examples to illustrate statistical concepts is essential and actively doing the work. I use of modern tools like R and the tidyverse to prepare students for careers in biological research."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About the Instructor",
    "section": "Education",
    "text": "Education\n\nPhD in Statistics, University of Notre Dame\nMS in Biology, University of Cincinnati\nBS in Biology, University of Cincinnati"
  },
  {
    "objectID": "about.html#contact-information",
    "href": "about.html#contact-information",
    "title": "About the Instructor",
    "section": "Contact Information",
    "text": "Contact Information\nFeel free to contact me with any questions about the course or research opportunities:\n\nEmail: wlperry@d.umn.edu\nOffice: TBD\nOffice Hours: by appointment"
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Semester - Fall 2025\nInstructor: Bill Perry\n\n\n\n\nWhen: Monday and Wednesday 14:00 - 15:50\nEmail: wlperry@d.umn.edu\n\n\nWhere: We will see\nAssistant: someone please\n\n\nHow: Mostly working through the ideas in the PowerPoint slides using code some of which is prewritten\nAssistant email: someone@d.umn.edu\n\n\nRequired materials: The textbook and a laptop and tenacity\nOffice hours: TBD and by appointment or walk in\n\n\n\n\n\nWelcome to Ecological Statistics! This is a practical course that will introduce you to the topics of experimental design, hypothesis testing, data analysis and visualization in the ecological sciences context. The goal of the course is to enable you to interpret statistical methods and results in the published literature, carry out your own data analyses and conduct a productive conversation with a statistician. We will learn about common statistical approaches in ecology and use the R statistical computing environment to get practice implementing these approaches. Additionally, this course will provide opportunities for students to practice scientific writing skills.\n\n\n\nLearn some useful data skills nd organizaiton and statistical methods and R skills.\n\n\n\nThe course will be taught in a combined lecture and computer lab format with mostly hands on activities assuming you have read the materials. Some of our meetings will take a primarily lecture and discussion format, others a primarily computer lab format, but most will have elements of both. Lecture portions will be dedicated to taking up homework questions, going over course concepts and discussing examples. Computer laboratory elements will be dedicated to using the R statistical computing package for data analysis and visualization.\nMaterials such as lecture slides, computer labs, homework, and assignments will be distributed through the course Canvas site or on the website. The course will mostly be based on the textbook The Analysis of Biological Data (3rd Edition) Experimental Design and Data Analysis for Biologists (1st edition) by Michael C. Whitlock and Dolph Schluter. The text is required and we can talk about how to get it. Other textbooks and resources that you may find of use are:\n\nThe R Book (1st or 2nd edition) by MJ Crawley. Highly recommended as an all-around reference book.\nA Primer of Ecological Statistics (1st or 2nd edition) by NJ Gotelli and AM Ellison.\nBiostatistical Analysis (4th or 5th edition) by JH Zar.\nHandbook of Biological Statistics by JH McDonald (online at http://www.biostathandbook.com/).\nInstitute for Digital Research and Education (https://stats.idre.ucla.edu/other/dae/).\nA Compendium of Clean Graphs in R (http://shinyapps.org/apps/RGraphCompendium/index.php)\nNumerical Ecology with R by D Borcard, F Gillet and P Legendre. A practical introduction to common multivariate methods in ecology.\nOrdination Methods for Ecologists by Mike Palmer (https://ordination.okstate.edu/)\n\n\n\n\nYour grade will be based on homework, in-class assignments, and take-home assignments:\nI will be making this out of points as we get there\n\n\n\nHomework\n10%\n\n\nIn-class assignments\n20%\n\n\nTake-home assignments (4 assignments)\n70%\n\n\n\n\n\n\nyou are required to read the class readings (see ‘course schedule’) ahead of each class. Homework questions based on the readings will also need to be completed ahead of many classes. Homework questions will be assigned one class meeting before they are due and will need to be submitted before the start of class. We will spend the portions class taking up and discussing homework questions. I will cold-call students to present and explain their answers to the rest of the class. Homework assignments will receive one of four possible grades: 100% (A) for work that meets or exceeds expectations, 85% (B) for work that meets most expectations, 65% (D) for work that misses most expectations or 0% (F) for work deemed unacceptable.\n\n\n\nmost computer lab exercises will be accompanied by in-class questions and disucssion. You will submit answers to these questions and your functional R code for the exercise by the end of the day of each exercise. In-class assignments will be submitted as QUARTO files. In-class assignments will be graded on the same scale as homework.\nTake-home assignments: four assignments will be completed outside of class. Each assignment will involve the independent analysis, presentation and interpretation of a dataset. The written report will include 3 sections: abstract, statistical materials and methods and results (statistical results and figures/ tables), prepared to “publication quality” standards. Assignments will be graded out of 100% and assessed on metrics including statistical literacy (performing analyses correctly), graphical presentation of data, adherence to correct statistical reporting norms, grammar and the quality/functionality of the accompanying R code. The assignments will comprise 70% of your final grade; A1 will be worth 10% of the final grade, A2 15%, A3 20%, and A4 25%.\n\n\n\nwill be assigned on a straight 10% scale, with 90-100% receiving some form of A, 80-89% some form of B, etc. + and – grades will fall on the upper (&gt;X7) and lower (&lt;X4) end of the ranges, respectively. Percentage grades below 60% are equivalent to an ‘F’ letter grade.\n\n\n\n10% of total possible grade will be deducted per day late in absence of valid excuse. A grade of 0 will be given for assignments that are more than 3 days late.\n\n\n\nValid excuses for missed class or late work consist of subpoenas, jury duty, military duty, religious observances, illness, bereavement for immediate family and NCAA varsity intercollegiate athletics. Conflicts with work, vacations, weddings, travel, or some other private situation that was foreseen will not be accommodated.  For further information on excused absences see https://evcaa.d.umn.edu/excused-absences\n\n\n\nI take plagiarism and academic dishonesty very seriously and will invoke the full weight of UMD-approved sanctions at the first instance of plagiarism. I am happy to answer questions on what is considered a violation of academic integrity in this class. Please also refer to UMD’s academic integrity policy at: https://evcaa.d.umn.edu/student-academic-integrity\n\n\n\nI will enforce, and expect you to follow the University’s Student Code of Conduct. Appropriate classroom conduct promotes an environment of academic achievement and integrity. Disruptive classroom behaviour that substantially or repeatedly interrupts either the instructor’s ability to teach, or student learning, is prohibited. Disruptive behaviour includes inappropriate use of technology in the classroom. Examples include ringing cell phones, text-messaging, watching videos of funny cats (and other videos), playing computer games, doing email, or surfing the Internet on your computer instead of note-taking or other instructor-sanctioned activities. See more here: https://regents.umn.edu/sites/regents.umn.edu/files/2022-07/policy_student_conduct_code.pdf\n\n\n\nIndividuals who have any disability, either permanent or temporary, which might affect their ability to perform in this course are encouraged to inform the instructor at the start of the quarter. Methods, materials or testing may be modified to provide for equitable participation.\n\n\n\nThe University of Minnesota is committed to the policy that all of its students shall have equal educational opportunities. The University expressly forbids discrimination on the basis of race, color, gender, sexual orientation, disability, veteran’s status, ethnicity, religion, creed, national origin or marital status. If you believe that your Ecology instructor has not followed this policy, you are invited to bring this to the attention of the Biology Department Head (207 Swenson Science Building; 218-726-8123). Your conference will be kept confidential.\nYou may review other relevant UMD policy statements at: https://evcaa.d.umn.edu/recommended-syllabi-policy-statements"
  },
  {
    "objectID": "syllabus.html#aim-and-scope",
    "href": "syllabus.html#aim-and-scope",
    "title": "Syllabus",
    "section": "",
    "text": "Welcome to Ecological Statistics! This is a practical course that will introduce you to the topics of experimental design, hypothesis testing, data analysis and visualization in the ecological sciences context. The goal of the course is to enable you to interpret statistical methods and results in the published literature, carry out your own data analyses and conduct a productive conversation with a statistician. We will learn about common statistical approaches in ecology and use the R statistical computing environment to get practice implementing these approaches. Additionally, this course will provide opportunities for students to practice scientific writing skills."
  },
  {
    "objectID": "syllabus.html#student-learning-outcomes",
    "href": "syllabus.html#student-learning-outcomes",
    "title": "Syllabus",
    "section": "",
    "text": "Learn some useful data skills nd organizaiton and statistical methods and R skills."
  },
  {
    "objectID": "syllabus.html#course-structure",
    "href": "syllabus.html#course-structure",
    "title": "Syllabus",
    "section": "",
    "text": "The course will be taught in a combined lecture and computer lab format with mostly hands on activities assuming you have read the materials. Some of our meetings will take a primarily lecture and discussion format, others a primarily computer lab format, but most will have elements of both. Lecture portions will be dedicated to taking up homework questions, going over course concepts and discussing examples. Computer laboratory elements will be dedicated to using the R statistical computing package for data analysis and visualization.\nMaterials such as lecture slides, computer labs, homework, and assignments will be distributed through the course Canvas site or on the website. The course will mostly be based on the textbook The Analysis of Biological Data (3rd Edition) Experimental Design and Data Analysis for Biologists (1st edition) by Michael C. Whitlock and Dolph Schluter. The text is required and we can talk about how to get it. Other textbooks and resources that you may find of use are:\n\nThe R Book (1st or 2nd edition) by MJ Crawley. Highly recommended as an all-around reference book.\nA Primer of Ecological Statistics (1st or 2nd edition) by NJ Gotelli and AM Ellison.\nBiostatistical Analysis (4th or 5th edition) by JH Zar.\nHandbook of Biological Statistics by JH McDonald (online at http://www.biostathandbook.com/).\nInstitute for Digital Research and Education (https://stats.idre.ucla.edu/other/dae/).\nA Compendium of Clean Graphs in R (http://shinyapps.org/apps/RGraphCompendium/index.php)\nNumerical Ecology with R by D Borcard, F Gillet and P Legendre. A practical introduction to common multivariate methods in ecology.\nOrdination Methods for Ecologists by Mike Palmer (https://ordination.okstate.edu/)"
  },
  {
    "objectID": "syllabus.html#evaluation-scheme",
    "href": "syllabus.html#evaluation-scheme",
    "title": "Syllabus",
    "section": "",
    "text": "Your grade will be based on homework, in-class assignments, and take-home assignments:\nI will be making this out of points as we get there\n\n\n\nHomework\n10%\n\n\nIn-class assignments\n20%\n\n\nTake-home assignments (4 assignments)\n70%"
  },
  {
    "objectID": "syllabus.html#homework",
    "href": "syllabus.html#homework",
    "title": "Syllabus",
    "section": "",
    "text": "you are required to read the class readings (see ‘course schedule’) ahead of each class. Homework questions based on the readings will also need to be completed ahead of many classes. Homework questions will be assigned one class meeting before they are due and will need to be submitted before the start of class. We will spend the portions class taking up and discussing homework questions. I will cold-call students to present and explain their answers to the rest of the class. Homework assignments will receive one of four possible grades: 100% (A) for work that meets or exceeds expectations, 85% (B) for work that meets most expectations, 65% (D) for work that misses most expectations or 0% (F) for work deemed unacceptable."
  },
  {
    "objectID": "syllabus.html#in-class-assignments",
    "href": "syllabus.html#in-class-assignments",
    "title": "Syllabus",
    "section": "",
    "text": "most computer lab exercises will be accompanied by in-class questions and disucssion. You will submit answers to these questions and your functional R code for the exercise by the end of the day of each exercise. In-class assignments will be submitted as QUARTO files. In-class assignments will be graded on the same scale as homework.\nTake-home assignments: four assignments will be completed outside of class. Each assignment will involve the independent analysis, presentation and interpretation of a dataset. The written report will include 3 sections: abstract, statistical materials and methods and results (statistical results and figures/ tables), prepared to “publication quality” standards. Assignments will be graded out of 100% and assessed on metrics including statistical literacy (performing analyses correctly), graphical presentation of data, adherence to correct statistical reporting norms, grammar and the quality/functionality of the accompanying R code. The assignments will comprise 70% of your final grade; A1 will be worth 10% of the final grade, A2 15%, A3 20%, and A4 25%."
  },
  {
    "objectID": "syllabus.html#final-letter-grades",
    "href": "syllabus.html#final-letter-grades",
    "title": "Syllabus",
    "section": "",
    "text": "will be assigned on a straight 10% scale, with 90-100% receiving some form of A, 80-89% some form of B, etc. + and – grades will fall on the upper (&gt;X7) and lower (&lt;X4) end of the ranges, respectively. Percentage grades below 60% are equivalent to an ‘F’ letter grade."
  },
  {
    "objectID": "syllabus.html#latemissed-assignment-policy",
    "href": "syllabus.html#latemissed-assignment-policy",
    "title": "Syllabus",
    "section": "",
    "text": "10% of total possible grade will be deducted per day late in absence of valid excuse. A grade of 0 will be given for assignments that are more than 3 days late."
  },
  {
    "objectID": "syllabus.html#valid-excuses",
    "href": "syllabus.html#valid-excuses",
    "title": "Syllabus",
    "section": "",
    "text": "Valid excuses for missed class or late work consist of subpoenas, jury duty, military duty, religious observances, illness, bereavement for immediate family and NCAA varsity intercollegiate athletics. Conflicts with work, vacations, weddings, travel, or some other private situation that was foreseen will not be accommodated.  For further information on excused absences see https://evcaa.d.umn.edu/excused-absences"
  },
  {
    "objectID": "syllabus.html#academic-integrity",
    "href": "syllabus.html#academic-integrity",
    "title": "Syllabus",
    "section": "",
    "text": "I take plagiarism and academic dishonesty very seriously and will invoke the full weight of UMD-approved sanctions at the first instance of plagiarism. I am happy to answer questions on what is considered a violation of academic integrity in this class. Please also refer to UMD’s academic integrity policy at: https://evcaa.d.umn.edu/student-academic-integrity"
  },
  {
    "objectID": "syllabus.html#student-conduct-code",
    "href": "syllabus.html#student-conduct-code",
    "title": "Syllabus",
    "section": "",
    "text": "I will enforce, and expect you to follow the University’s Student Code of Conduct. Appropriate classroom conduct promotes an environment of academic achievement and integrity. Disruptive classroom behaviour that substantially or repeatedly interrupts either the instructor’s ability to teach, or student learning, is prohibited. Disruptive behaviour includes inappropriate use of technology in the classroom. Examples include ringing cell phones, text-messaging, watching videos of funny cats (and other videos), playing computer games, doing email, or surfing the Internet on your computer instead of note-taking or other instructor-sanctioned activities. See more here: https://regents.umn.edu/sites/regents.umn.edu/files/2022-07/policy_student_conduct_code.pdf"
  },
  {
    "objectID": "syllabus.html#access-for-students-with-disabilities",
    "href": "syllabus.html#access-for-students-with-disabilities",
    "title": "Syllabus",
    "section": "",
    "text": "Individuals who have any disability, either permanent or temporary, which might affect their ability to perform in this course are encouraged to inform the instructor at the start of the quarter. Methods, materials or testing may be modified to provide for equitable participation."
  },
  {
    "objectID": "syllabus.html#promotion-of-bias-free-instruction",
    "href": "syllabus.html#promotion-of-bias-free-instruction",
    "title": "Syllabus",
    "section": "",
    "text": "The University of Minnesota is committed to the policy that all of its students shall have equal educational opportunities. The University expressly forbids discrimination on the basis of race, color, gender, sexual orientation, disability, veteran’s status, ethnicity, religion, creed, national origin or marital status. If you believe that your Ecology instructor has not followed this policy, you are invited to bring this to the attention of the Biology Department Head (207 Swenson Science Building; 218-726-8123). Your conference will be kept confidential.\nYou may review other relevant UMD policy statements at: https://evcaa.d.umn.edu/recommended-syllabi-policy-statements"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#tapestry-plot",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#tapestry-plot",
    "title": "01_Class_Activity",
    "section": "Tapestry Plot ——",
    "text": "Tapestry Plot ——\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm))"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#xy-plot",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#xy-plot",
    "title": "01_Class_Activity",
    "section": "XY Plot —–",
    "text": "XY Plot —–\nnotice the points are layered on top but some overlap\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n  \nggplot(data = p_df, aes(x=wind, y=len_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#xy-plot-with-dodged-points",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#xy-plot-with-dodged-points",
    "title": "01_Class_Activity",
    "section": "XY Plot with dodged points ——",
    "text": "XY Plot with dodged points ——\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm)) + \n  geom_point(position = position_dodge2(width=0.2) )\n\n# this dodges the points # position_dodge2 or can use position_dodge depending on grouping\n\nWhat are the other ways to display the data?"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#histogram",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#histogram",
    "title": "01_Class_Activity",
    "section": "Histogram —–",
    "text": "Histogram —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm)) + \n  geom_histogram()\n\n\nNote we really want to see the histograms colored by wind direction\nWe can map the wind aesthetic to a fill in the histogram"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#histogram-colors",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#histogram-colors",
    "title": "01_Class_Activity",
    "section": "Histogram Colors —–",
    "text": "Histogram Colors —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm, fill = wind)) + geom_histogram( position = position_dodge2(width = 0.5))"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#histogram-bins",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#histogram-bins",
    "title": "01_Class_Activity",
    "section": "Histogram Bins —–",
    "text": "Histogram Bins —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#other-plots-if-time",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#other-plots-if-time",
    "title": "01_Class_Activity",
    "section": "Other Plots if time",
    "text": "Other Plots if time"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_slides.html#box-and-whisker-plots",
    "href": "lectures/lecture_01/01_2_class_activity_slides.html#box-and-whisker-plots",
    "title": "01_Class_Activity",
    "section": "Box and Whisker Plots",
    "text": "Box and Whisker Plots\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm, fill = wind)) + geom_boxplot()"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html",
    "href": "lectures/lecture_01/01_2_class_activity_html.html",
    "title": "01_Class_Activity",
    "section": "",
    "text": "Note: This activity is really in place of the outline above which you should have read before class."
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#tapestry-plot",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#tapestry-plot",
    "title": "01_Class_Activity",
    "section": "Tapestry Plot ——",
    "text": "Tapestry Plot ——\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm))"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot",
    "title": "01_Class_Activity",
    "section": "XY Plot —–",
    "text": "XY Plot —–\nnotice the points are layered on top but some overlap\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n  \nggplot(data = p_df, aes(x=wind, y=len_mm)) + \n  geom_point()"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot-with-dodged-points",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#xy-plot-with-dodged-points",
    "title": "01_Class_Activity",
    "section": "XY Plot with dodged points ——",
    "text": "XY Plot with dodged points ——\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm)) + \n  geom_point(position = position_dodge2(width=0.2) )\n\n\n\n\n\n\n\n# this dodges the points # position_dodge2 or can use position_dodge depending on grouping\n\nWhat are the other ways to display the data?"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram",
    "title": "01_Class_Activity",
    "section": "Histogram —–",
    "text": "Histogram —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm)) + \n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote we really want to see the histograms colored by wind direction\nWe can map the wind aesthetic to a fill in the histogram"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram-colors",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram-colors",
    "title": "01_Class_Activity",
    "section": "Histogram Colors —–",
    "text": "Histogram Colors —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm, fill = wind)) + geom_histogram( position = position_dodge2(width = 0.5))\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#histogram-bins",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#histogram-bins",
    "title": "01_Class_Activity",
    "section": "Histogram Bins —–",
    "text": "Histogram Bins —–\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#other-plots-if-time",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#other-plots-if-time",
    "title": "01_Class_Activity",
    "section": "Other Plots if time",
    "text": "Other Plots if time"
  },
  {
    "objectID": "lectures/lecture_01/01_2_class_activity_html.html#box-and-whisker-plots",
    "href": "lectures/lecture_01/01_2_class_activity_html.html#box-and-whisker-plots",
    "title": "01_Class_Activity",
    "section": "Box and Whisker Plots",
    "text": "Box and Whisker Plots\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm, fill = wind)) + geom_boxplot()"
  },
  {
    "objectID": "lectures/lecture_01/01_03_homework_reveal.html#what-to-turn-in--",
    "href": "lectures/lecture_01/01_03_homework_reveal.html#what-to-turn-in--",
    "title": "01_Homework",
    "section": "What to turn in -",
    "text": "What to turn in -\n\na zipped or compressed folder of the entire project folder\na self-contained html file showing the output\nannotations in the quarto file that shows or tells what is being done like in the class activities"
  },
  {
    "objectID": "lectures/lecture_01/01_03_homework_html.html",
    "href": "lectures/lecture_01/01_03_homework_html.html",
    "title": "01_Homework",
    "section": "",
    "text": "This is an assignment for you to practice the code and all the work we do in class on a different dataframe. We will assign one of these each week for you to do that encompasses all fo the work we do. We give it out on Monday so you can start early and see how this work."
  },
  {
    "objectID": "lectures/lecture_01/01_03_homework_html.html#what-to-turn-in--",
    "href": "lectures/lecture_01/01_03_homework_html.html#what-to-turn-in--",
    "title": "01_Homework",
    "section": "What to turn in -",
    "text": "What to turn in -\n\na zipped or compressed folder of the entire project folder\na self-contained html file showing the output\nannotations in the quarto file that shows or tells what is being done like in the class activities"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#in-rstudio",
    "href": "lectures/lecture_02/02_02_class_activity.html#in-rstudio",
    "title": "02_Class_Activity",
    "section": "In RStudio:",
    "text": "In RStudio:\n\nclick file - open project and select the 2025_UMD_BioStats_Student_Code.Rproj file or double click on it in the finder or data explorer.\nyour screen will now change as RStudio knows where home is\n\n\n\nNote that in the upper right you will see 2025_UMD_BioStats_Student_Code so you know you are in the right spot\nNow click File - New File - Quarto File\n\n\n\nCreate a file that starts with 02_ and then something that will help you know what is going on like 02_class_activity_in_class.qmd\nNow this file thinks this is home.\nSo I usually copy stuff for the header from another file as its just too hard to remember all this…\n\n---\ntitle: \"Title of your file\" # Title of the file\nauthor: \"Your Name\" # who you are\nformat: # this is the formats that it will render to\n  html:\n    toc: false # not table of contents\n    default: true\n    embed-resources: true # makes everything go into the html file\n    self-contained: true # also makes self contained\neditor: visual # type of editing\nproject:\n  execute-dir: project # where it will look for files\nexecute:\n  keep-md: true # retains the images when you start again\n  cache: true # also heps reatain images and code\n---"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#the-whole-dataframe---wont-show-all",
    "href": "lectures/lecture_02/02_02_class_activity.html#the-whole-dataframe---wont-show-all",
    "title": "02_Class_Activity",
    "section": "The whole dataframe - wont show all",
    "text": "The whole dataframe - wont show all\n\np_df\n\n# A tibble: 48 × 6\n   date    group       n_s   wind  tree_no len_mm\n   &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n 1 3/20/25 cephalopods n     lee         1     20\n 2 3/20/25 cephalopods n     lee         1     21\n 3 3/20/25 cephalopods n     lee         1     23\n 4 3/20/25 cephalopods n     lee         1     25\n 5 3/20/25 cephalopods n     lee         1     21\n 6 3/20/25 cephalopods n     lee         1     16\n 7 3/20/25 cephalopods s     wind        1     15\n 8 3/20/25 cephalopods s     wind        1     16\n 9 3/20/25 cephalopods s     wind        1     14\n10 3/20/25 cephalopods s     wind        1     17\n# ℹ 38 more rows"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#the-top-of-the-datafame-to-see-what-it-looks-like",
    "href": "lectures/lecture_02/02_02_class_activity.html#the-top-of-the-datafame-to-see-what-it-looks-like",
    "title": "02_Class_Activity",
    "section": "The top of the datafame to see what it looks like",
    "text": "The top of the datafame to see what it looks like\n\nhead(p_df)\n\n# A tibble: 6 × 6\n  date    group       n_s   wind  tree_no len_mm\n  &lt;chr&gt;   &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 3/20/25 cephalopods n     lee         1     20\n2 3/20/25 cephalopods n     lee         1     21\n3 3/20/25 cephalopods n     lee         1     23\n4 3/20/25 cephalopods n     lee         1     25\n5 3/20/25 cephalopods n     lee         1     21\n6 3/20/25 cephalopods n     lee         1     16\n\n# tail(p_df) # the bottom"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#how-to-look-at-one-variable",
    "href": "lectures/lecture_02/02_02_class_activity.html#how-to-look-at-one-variable",
    "title": "02_Class_Activity",
    "section": "How to look at one variable",
    "text": "How to look at one variable\n\np_df$group # name of dataframe $ varaible\n\n [1] \"cephalopods\" \"cephalopods\" \"cephalopods\" \"cephalopods\" \"cephalopods\"\n [6] \"cephalopods\" \"cephalopods\" \"cephalopods\" \"cephalopods\" \"cephalopods\"\n[11] \"cephalopods\" \"cephalopods\" \"salmon\"      \"salmon\"      \"salmon\"     \n[16] \"salmon\"      \"salmon\"      \"salmon\"      \"salmon\"      \"salmon\"     \n[21] \"salmon\"      \"salmon\"      \"salmon\"      \"salmon\"      \"crayfish\"   \n[26] \"crayfish\"    \"crayfish\"    \"crayfish\"    \"crayfish\"    \"crayfish\"   \n[31] \"crayfish\"    \"crayfish\"    \"crayfish\"    \"crayfish\"    \"crayfish\"   \n[36] \"crayfish\"    \"snail\"       \"snail\"       \"snail\"       \"snail\"      \n[41] \"snail\"       \"snail\"       \"snail\"       \"snail\"       \"snail\"      \n[46] \"snail\"       \"snail\"       \"snail\""
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#xy-plot-with-dodged-points",
    "href": "lectures/lecture_02/02_02_class_activity.html#xy-plot-with-dodged-points",
    "title": "02_Class_Activity",
    "section": "XY Plot with dodged points",
    "text": "XY Plot with dodged points\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm)) + \n  geom_point(position = position_dodge2(width=0.2) )\n\n\n\n\n\n\n\n# this dodges the points # position_dodge2 or can use position_dodge depending on grouping"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#histogram-color-with-bins",
    "href": "lectures/lecture_02/02_02_class_activity.html#histogram-color-with-bins",
    "title": "02_Class_Activity",
    "section": "Histogram Color with Bins",
    "text": "Histogram Color with Bins\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=len_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#box-and-whisker-plots",
    "href": "lectures/lecture_02/02_02_class_activity.html#box-and-whisker-plots",
    "title": "02_Class_Activity",
    "section": "Box and Whisker Plots",
    "text": "Box and Whisker Plots\n\nknitr::opts_chunk$set(\n  comment = '', fig.width = 4, fig.height = 3)\n\nggplot(data = p_df, aes(x=wind, y=len_mm, fill = wind)) + geom_boxplot()"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#xy_plot-is-not-in-the-environment",
    "href": "lectures/lecture_02/02_02_class_activity.html#xy_plot-is-not-in-the-environment",
    "title": "02_Class_Activity",
    "section": "xy_plot is not in the environment",
    "text": "xy_plot is not in the environment\nwe can save this plot by typing\n\nggsave(xy_plot, # name of the plot you called it\n       file = \"figures/xy_plot.pdf\", # type of file you want - pdf is good\n       units = \"in\", # what are the units you want to measure in\n       width = 6, height = 6) # dimensions - not if large the fonts need to be adjusted\n\nThe file should be in the figures directory"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#we-can-also-add-themes",
    "href": "lectures/lecture_02/02_02_class_activity.html#we-can-also-add-themes",
    "title": "02_Class_Activity",
    "section": "we can also add themes",
    "text": "we can also add themes\n\nxy_plot &lt;- ggplot(data = p_df, aes(x=wind, y=len_mm, color = wind, shape = wind)) + \n  geom_point(\n    size = 2,\n    position = position_dodge2(width=0.2)) +\n  labs(x=\"Wind Side\", y = \"Length (mm)\", color = \"Wind Side\", shape = \"Wind Side\") +\n  theme_classic()\nxy_plot"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#change-the-scale-by-zooming-in",
    "href": "lectures/lecture_02/02_02_class_activity.html#change-the-scale-by-zooming-in",
    "title": "02_Class_Activity",
    "section": "change the scale by zooming in",
    "text": "change the scale by zooming in\n\nxy_plot &lt;- ggplot(data = p_df, aes(x=wind, y=len_mm, color = wind, shape = wind)) + \n  geom_point(\n    size = 2,\n    position = position_dodge2(width=0.2)) +\n  labs(x=\"Wind Side\", y = \"Length (mm)\", color = \"Wind Side\", shape = \"Wind Side\") +\n  theme_classic() +\n  coord_cartesian(ylim = c(0,30))\nxy_plot"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#we-need-to-learn-to-pipe-things",
    "href": "lectures/lecture_02/02_02_class_activity.html#we-need-to-learn-to-pipe-things",
    "title": "02_Class_Activity",
    "section": "we need to learn to pipe things",
    "text": "we need to learn to pipe things\n\nthe dataframe –&gt; pipe command that feed the dataframe into –&gt; next command\n\n\np_df %&gt;% summarize(mean_length = mean(len_mm, na.rm = TRUE))\n\n# A tibble: 1 × 1\n  mean_length\n        &lt;dbl&gt;\n1        17.7"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#what-is-cool-is-we-can-do-a-lot-of-different-things-now",
    "href": "lectures/lecture_02/02_02_class_activity.html#what-is-cool-is-we-can-do-a-lot-of-different-things-now",
    "title": "02_Class_Activity",
    "section": "What is cool is we can do a lot of different things now",
    "text": "What is cool is we can do a lot of different things now\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(len_mm, na.rm = TRUE),\n    sd_length = sd(len_mm, na.rm = TRUE),\n    n_length = n())\n\n# A tibble: 1 × 3\n  mean_length sd_length n_length\n        &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1        17.7      3.53       48"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#super-cool-code-in-case-there-are-missing-values",
    "href": "lectures/lecture_02/02_02_class_activity.html#super-cool-code-in-case-there-are-missing-values",
    "title": "02_Class_Activity",
    "section": "Super cool code in case there are missing values",
    "text": "Super cool code in case there are missing values\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(len_mm, na.rm = TRUE),\n    sd_length = sd(len_mm, na.rm = TRUE),\n    n_length = sum(!is.na(len_mm)))\n\n# A tibble: 1 × 3\n  mean_length sd_length n_length\n        &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1        17.7      3.53       48"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#what-else-do-we-want-to-know-though",
    "href": "lectures/lecture_02/02_02_class_activity.html#what-else-do-we-want-to-know-though",
    "title": "02_Class_Activity",
    "section": "what else do we want to know though",
    "text": "what else do we want to know though\n\nwe want to know the mean and such for each group of the treatments\nhow could we do this?\n\nwe need to add a command - `group_by(wind)\nbut how can we do this for this code?\n\n\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(len_mm, na.rm = TRUE),\n    sd_length = sd(len_mm, na.rm = TRUE),\n    n_length = sum(!is.na(len_mm)))\n\n# A tibble: 1 × 3\n  mean_length sd_length n_length\n        &lt;dbl&gt;     &lt;dbl&gt;    &lt;int&gt;\n1        17.7      3.53       48"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#what-if-we-wanted-to-save-this-output",
    "href": "lectures/lecture_02/02_02_class_activity.html#what-if-we-wanted-to-save-this-output",
    "title": "02_Class_Activity",
    "section": "what if we wanted to save this output",
    "text": "what if we wanted to save this output\nwhat else do we need to do to save the output"
  },
  {
    "objectID": "lectures/lecture_02/02_02_class_activity.html#given-enough-time-there-are-some-other-tweaks-to-make",
    "href": "lectures/lecture_02/02_02_class_activity.html#given-enough-time-there-are-some-other-tweaks-to-make",
    "title": "02_Class_Activity",
    "section": "given enough time there are some other tweaks to make",
    "text": "given enough time there are some other tweaks to make\nin this code we can do some piping that will be very useful later on\n\nmean_se_plot &lt;- p_df %&gt;% \n  ggplot(aes(wind, len_mm , color = wind)) +\n  stat_summary(\n    fun = mean, \n    na.rm = TRUE, \n    geom = \"point\", \n    size = 3) +\n  stat_summary(\n    fun.data = mean_se, \n    na.rm = TRUE, \n    geom = \"errorbar\", \n    width = 0.2) +\n  # annotate(\"rect\", \n  #        xmin = -Inf, xmax = Inf,  # Cover the entire x range\n  #        ymin = -Inf, ymax = Inf,  # Cover the entire y range\n  #        fill = \"white\", \n  #        alpha = 1) +\n  labs(\n    x = \"Wind Side\",\n    y = \"Length (mm)\",\n    color = \"Windy Side\"\n  ) +\n  theme_classic()\n\nmean_se_plot"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 05",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#one-sample-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#one-sample-t-test",
    "title": "Lecture 05",
    "section": "One-sample t-test",
    "text": "One-sample t-test\nUsed when we want to compare a sample mean to a known or hypothesized population value."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbarx---mussqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbarx---mussqrtn",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\)",
    "text": "\\(t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\)\nwhere:\n\n- \\(\\bar{x}\\) is the sample mean\n- \\(\\mu\\) is the hypothesized population mean\n- \\(s\\) is the sample standard deviation\n- \\(n\\) is the sample size"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#assumptions-for-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#assumptions-for-t-test",
    "title": "Lecture 05",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)",
    "text": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwhere:\n\nx̄₁ and x̄₂: These represent the sample means of the two groups you’re comparing. \ns²ₚ: This is the pooled variance, calculated as: s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups. \nn₁ and n₂: These are the sample sizes of the two groups. \n√(1/n₁ + 1/n₂): This represents the pooled standard error."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbards_dsqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_slides.html#t-fracbards_dsqrtn",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{d}}{s_d/\\sqrt{n}}\\)",
    "text": "\\(t = \\frac{\\bar{d}}{s_d/\\sqrt{n}}\\)\nwhere:\n\n- \\(\\bar{d}\\) is the mean difference\n- \\(s_d\\) is the standard deviation of differences\n- \\(n\\) is the number of pairs"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html",
    "title": "Lecture 05",
    "section": "",
    "text": "Introduction to hypothesis testing\nThe standard normal distribution\nStandard error\nConfidence intervals\nStudent’s t-distribution\nH testing sequence\np-values\n\n\nOur last graphs"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 05",
    "section": "The objectives:",
    "text": "The objectives:\n\np-values\nBrief review\nH test for a single population\n1- and 2-sided tests\nHypothesis tests for two populations\nAssumptions of parametric tests"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#one-sample-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#one-sample-t-test",
    "title": "Lecture 05",
    "section": "One-sample t-test",
    "text": "One-sample t-test\nUsed when we want to compare a sample mean to a known or hypothesized population value."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx---mussqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx---mussqrtn",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\)",
    "text": "\\(t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}\\)\nwhere:\n\n- \\(\\bar{x}\\) is the sample mean\n- \\(\\mu\\) is the hypothesized population mean\n- \\(s\\) is the sample standard deviation\n- \\(n\\) is the sample size"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#assumptions-for-t-test",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#assumptions-for-t-test",
    "title": "Lecture 05",
    "section": "Assumptions for t-test:",
    "text": "Assumptions for t-test:\n\nData is normally distributed\nObservations are independent\nNo significant outliers"
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbarx_1---barx_2s_psqrtfrac1n_1-frac1n_2",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)",
    "text": "\\(t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}\\)\nwhere:\n\nx̄₁ and x̄₂: These represent the sample means of the two groups you’re comparing. \ns²ₚ: This is the pooled variance, calculated as: s²ₚ = [(n₁ - 1)s₁² + (n₂ - 1)s₂²] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups. \nn₁ and n₂: These are the sample sizes of the two groups. \n√(1/n₁ + 1/n₂): This represents the pooled standard error."
  },
  {
    "objectID": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbards_dsqrtn",
    "href": "lectures/lecture_05/05_01_lecture_powerpoint_html.html#t-fracbards_dsqrtn",
    "title": "Lecture 05",
    "section": "\\(t = \\frac{\\bar{d}}{s_d/\\sqrt{n}}\\)",
    "text": "\\(t = \\frac{\\bar{d}}{s_d/\\sqrt{n}}\\)\nwhere:\n\n- \\(\\bar{d}\\) is the mean difference\n- \\(s_d\\) is the standard deviation of differences\n- \\(n\\) is the number of pairs"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#the-objectives",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#the-objectives",
    "title": "Lecture 04",
    "section": "The objectives:",
    "text": "The objectives:\n\nIntroduction to hypothesis testing\nThe standard normal distribution\nStandard error\nConfidence intervals\nStudent’s t-distribution\nH testing sequence\np-values"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#textz-fracx_i-musigma",
    "title": "Lecture 04",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#important-properties",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#important-properties",
    "title": "Lecture 04",
    "section": "3 important properties:",
    "text": "3 important properties:\n\nSampling distribution of means (SDM) from normal population will be normal\nLarge Sampling distribution of means from any population will be normal (Central Limit Theorem)\nThe mean of Sampling distribution of means will equal µ or the mean"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#text95-ci-bary-pm-z-cdot-fracsigmasqrtn",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_slides.html#text95-ci-bary-pm-z-cdot-fracsigmasqrtn",
    "title": "Lecture 04",
    "section": "\\(\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)",
    "text": "\\(\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere:\n\nȳ is the sample mean\n𝑛 is the sample size\nσ is the population standard deviation\nz is the z-value corresponding the probability of the CI"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html",
    "title": "Lecture 04",
    "section": "",
    "text": "Introduction to histograms or frequency distributions\nProbability Distribution Functions (PDF)\nDescriptive Statistics\n\nCenter - mean, median, mode\nSpread - range, variance, standard deviation\n\n\n\nOur last graphs\n\n\n\n\n\n\n\n\n\n\nPractice Exercise 1: Recreating Our Last Histograms\n\n\n\nLet’s recreate the basic histogram of fish lengths from our last class. Use the sculpin_df data frame that’s already loaded.\n\n# Write your code here to create a histogram of fish lengths from Toolik Lake\n# Remember to use the pipe operator %&gt;% and ggplot with geom_histogram()"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#the-objectives",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#the-objectives",
    "title": "Lecture 04",
    "section": "The objectives:",
    "text": "The objectives:\n\nIntroduction to hypothesis testing\nThe standard normal distribution\nStandard error\nConfidence intervals\nStudent’s t-distribution\nH testing sequence\np-values"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#textz-fracx_i-musigma",
    "title": "Lecture 04",
    "section": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)",
    "text": "\\(\\text{Z = }\\frac{X_i-\\mu}{\\sigma}\\)\n\nz = z-score for observation\nxi = original observation\nµ = mean of data distribution\nσ = SD of data distribution"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#important-properties",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#important-properties",
    "title": "Lecture 04",
    "section": "3 important properties:",
    "text": "3 important properties:\n\nSampling distribution of means (SDM) from normal population will be normal\nLarge Sampling distribution of means from any population will be normal (Central Limit Theorem)\nThe mean of Sampling distribution of means will equal µ or the mean"
  },
  {
    "objectID": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#text95-ci-bary-pm-z-cdot-fracsigmasqrtn",
    "href": "lectures/lecture_04/04_01_lecture_powerpoint_html.html#text95-ci-bary-pm-z-cdot-fracsigmasqrtn",
    "title": "Lecture 04",
    "section": "\\(\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)",
    "text": "\\(\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}\\)\nWhere:\n\nȳ is the sample mean\n𝑛 is the sample size\nσ is the population standard deviation\nz is the z-value corresponding the probability of the CI"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html",
    "href": "lectures/lecture_03/03_02_class_activity.html",
    "title": "03_Class_Activity",
    "section": "",
    "text": "Setting up a project and variable names and code names\nHow to use the pipe command %&gt;%\nHow to create descriptive statistics of a sample\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(len_mm, na.rm = TRUE),\n    sd_length = sd(len_mm, na.rm = TRUE),\n    n_length = sum(!is.na(len_mm)))\n\nMore graphs…\nggplot(data = p_df, aes(x=len_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))\n\nWhat questions do you have and what is unclear - what did not work so far when you started the homework?"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#what-did-we-do-last-time-in-activity-2",
    "href": "lectures/lecture_03/03_02_class_activity.html#what-did-we-do-last-time-in-activity-2",
    "title": "03_Class_Activity",
    "section": "",
    "text": "Setting up a project and variable names and code names\nHow to use the pipe command %&gt;%\nHow to create descriptive statistics of a sample\n\np_df %&gt;% \n  summarize(\n    mean_length = mean(len_mm, na.rm = TRUE),\n    sd_length = sd(len_mm, na.rm = TRUE),\n    n_length = sum(!is.na(len_mm)))\n\nMore graphs…\nggplot(data = p_df, aes(x=len_mm, fill = wind)) +\n  geom_histogram( binwidth = 2, \n# sets the width in units of the bins - try different nubmers\n   position = position_dodge2(width = 0.5))\n\nWhat questions do you have and what is unclear - what did not work so far when you started the homework?"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#setup",
    "href": "lectures/lecture_03/03_02_class_activity.html#setup",
    "title": "03_Class_Activity",
    "section": "Setup",
    "text": "Setup\nFirst, let’s load the packages we need and the dataset:\n\n# # Install the patchwork package if needed\n\n# install.packages(\"patchwork\")\nlibrary(patchwork)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Read in the data file\nsculpin_df &lt;- read_csv(\"data/sculpin.csv\")\n\nRows: 1052 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): lake, species\ndbl (3): site, total_length_mm, mass_g\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Look at the first few rows\nhead(sculpin_df)\n\n# A tibble: 6 × 5\n   site lake  species       total_length_mm mass_g\n  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;                   &lt;dbl&gt;  &lt;dbl&gt;\n1   146 E 01  slimy sculpin              53   1.25\n2   146 E 01  slimy sculpin              61   1.9 \n3   146 E 01  slimy sculpin              53   1.75\n4   146 E 01  slimy sculpin              77   4.25\n5   146 E 01  slimy sculpin              45   0.9 \n6   146 E 01  slimy sculpin              48   0.9"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#basic-data-summary",
    "href": "lectures/lecture_03/03_02_class_activity.html#basic-data-summary",
    "title": "03_Class_Activity",
    "section": "Basic Data Summary",
    "text": "Basic Data Summary\nLet’s first check what lakes are in our dataset:\n\n# Get a list of unique lakes\nunique(sculpin_df$lake)\n\n[1] \"E 01\"   \"E 05\"   \"NE 12\"  \"NE 14\"  \"S 06\"   \"S 07\"   \"Toolik\"\n\n\nHow many fish do we have from each lake?\n\n# Count observations by lake\nsculpin_df %&gt;%\n  group_by(lake) %&gt;% \n  summarize(sculpin_n = sum(!is.na(total_length_mm)))\n\n# A tibble: 7 × 2\n  lake   sculpin_n\n  &lt;chr&gt;      &lt;int&gt;\n1 E 01          79\n2 E 05          14\n3 NE 12        180\n4 NE 14         37\n5 S 06         132\n6 S 07          73\n7 Toolik       208"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#basic-histograms",
    "href": "lectures/lecture_03/03_02_class_activity.html#basic-histograms",
    "title": "03_Class_Activity",
    "section": "Basic Histograms",
    "text": "Basic Histograms\nA histogram shows how many observations fall into certain ranges (or “bins”).\nLet’s create a simple histogram of fish lengths from Lake E 01 :\n\n# Filter for Toolik Lake and create a histogram\nsculpin_df %&gt;%\n  filter(lake == \"E 01\") %&gt;%\n  ggplot(aes(x = total_length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  labs(title = \"Fish Lengths in Lake E 01\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\nWarning: Removed 189 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 1\n\n\n\nTry changing the binwidth parameter to 5 and then to 1. How does the appearance of the histogram change?"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#comparing-lakes",
    "href": "lectures/lecture_03/03_02_class_activity.html#comparing-lakes",
    "title": "03_Class_Activity",
    "section": "Comparing Lakes",
    "text": "Comparing Lakes\nNow let’s compare two lakes\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = total_length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, \n                 position = \"identity\") +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\nWarning: Removed 268 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = total_length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, \n                 position = position_dodge2(width=1)) +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\nWarning: Removed 268 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\nNow let’s compare two lakes side by side:\n\n# Compare histograms from Toolik and E 01 lakes\nsculpin_df %&gt;%\n  filter(lake %in% c(\"Toolik\", \"E 01\")) %&gt;%\n  ggplot(aes(x = total_length_mm, fill = lake)) +\n  geom_histogram(binwidth = 2, alpha = 0.7, position = \"identity\") +\n  labs(title = \"Fish Lengths in Different Lakes\",\n       x = \"Length (mm)\",\n       y = \"Count\") +\n  # facet_wrap(~lake, ncol = 1) +\n  facet_grid(lake~.)\n\nWarning: Removed 268 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 2\n\n\n\nChoose two new lakes to compare. What differences do you notice in their distributions?\n\n\nAdd code here\n\n# enter code here"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#small-vs.-large-samples",
    "href": "lectures/lecture_03/03_02_class_activity.html#small-vs.-large-samples",
    "title": "03_Class_Activity",
    "section": "Small vs. Large Samples",
    "text": "Small vs. Large Samples\nWe’ll randomly select different sample sizes from Toolik Lake:\n\n# Set a seed for reproducibility\nset.seed(123)\n\n# Create small sample (15 fish)\nsmall_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(15)\n\n# Create larger sample (50 fish)\nlarger_sample &lt;- sculpin_df %&gt;%\n  filter(lake == \"Toolik\") %&gt;%\n  sample_n(100)\n\n# Plot both samples\np1 &lt;- small_sample %&gt;%\n  ggplot(aes(x = total_length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"red\", alpha = 0.7) +\n  # coord_cartesian(xlim = c(20,80)) +\n  labs(title = \"Small Sample (n=15)\",\n       x = \"Length (mm)\",\n       y = \"Count\") +\n  coord_cartesian(xlim = c(20,80))\n\np2 &lt;- larger_sample %&gt;%\n  ggplot(aes(x = total_length_mm)) +\n  geom_histogram(binwidth = 2, fill = \"blue\", alpha = 0.7) +\n  # coord_cartesian(xlim = c(20,80)) +\n  labs(title = \"Larger Sample (n=50)\",\n       x = \"Length (mm)\",\n       y = \"Count\")\n\n\n# Display the plots side by side\np1 + p2 +\n  plot_layout(ncol = 1)\n\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\nWarning: Removed 26 rows containing non-finite outside the scale range\n(`stat_bin()`).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 3\n\n\n\nTry changing the sample sizes. What happens when you use very small samples (n=5)? What about larger samples (n=150)?\n\n\nadd code here\n\n# enter code here"
  },
  {
    "objectID": "lectures/lecture_03/03_02_class_activity.html#woah---what-happened-there---there-are-na-values-in-the-data",
    "href": "lectures/lecture_03/03_02_class_activity.html#woah---what-happened-there---there-are-na-values-in-the-data",
    "title": "03_Class_Activity",
    "section": "WOAH - what happened there - there are NA values in the data",
    "text": "WOAH - what happened there - there are NA values in the data\nyou need to either remove missing values or you can do that in the formulas\nWhat is the advantage to manually removing or doing it in formulas?\n\n# Calculate mean, standard deviation, and sample size by lake\nsculpin_stats_df &lt;- sculpin_df %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    se_length = sd(total_length_mm, na.rm = TRUE)/ sum(!is.na(total_length_mm))^.5,\n    count = sum(!is.na(total_length_mm)),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(count))\nsculpin_stats_df\n\n# A tibble: 7 × 5\n  lake   mean_length sd_length se_length count\n  &lt;chr&gt;        &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Toolik        51.7      12.0     0.834   208\n2 NE 12         49.8      15.2     1.13    180\n3 S 06          54.0      10.9     0.949   132\n4 E 01          58.2      15.3     1.72     79\n5 S 07          55.6      12.7     1.48     73\n6 NE 14         47.3      10.5     1.72     37\n7 E 05          47.1      10.8     2.88     14\n\n\nNow let’s visualize these statistics:\n\n# Create a bar plot of mean lengths with error bars\nsculpin_df %&gt;%  \n  ggplot(aes(lake, total_length_mm)) +\n  stat_summary(\n    fun = mean, na.rm = TRUE, \n    geom = \"bar\",\n    fill = \"skyblue\"\n    ) +\n  stat_summary(\n    fun.data = mean_se, na.rm = TRUE, \n    geom = \"errorbar\", \n    width = 0.2) +\n  labs(title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\nWe could also do this from the dataframe we just made\n\n# Create a bar plot of mean lengths with error bars\nsculpin_stats_df %&gt;%  \n  ggplot(aes(x = reorder(lake, mean_length), y = mean_length)) +\n  geom_bar(stat = \"identity\", \n           fill = \"skyblue\") +\n  geom_errorbar(aes(\n    ymin = mean_length - se_length, \n    ymax = mean_length + se_length),\n    width = 0.2\n    ) +\n  labs(\n    title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\nThe power of the pipe command is you can do this without hving to make a new dataframe\n\n# Create a bar plot of mean lengths with error bars\nsculpin_df %&gt;%\n  group_by(lake) %&gt;%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    se_length = sd_length / sqrt(n()),\n    count = n(),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(count &gt;= 10) %&gt;%  # Only include lakes with sufficient sample size\n  ggplot(aes(x = reorder(lake, mean_length), y = mean_length)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\") +\n  geom_errorbar(aes(ymin = mean_length - se_length, \n                    ymax = mean_length + se_length),\n                width = 0.2) +\n  labs(title = \"Mean Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Mean Length (mm)\") \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nActivity 5\n\n\n\nBased on the mean plot and what you’ve seen in the distributions, what can you say about fish sizes in different lakes? Are there lakes with particularly large or small fish?\nWe will start to ask how different are they and is it by chance?\nThis is the inductive phase of doing reserach."
  }
]