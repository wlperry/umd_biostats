{
  "hash": "dec55a0750cd3bc29e5955f3ebf3c929",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 09 - Class Activity\"\nauthor: \"Your Name\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true\n  fig-height: 3\n  fig-width: 5\n  paged-print: false\nformat:\n  html:\n    freeze: false\n    toc: false\n    output-file: \"09_02_class_activity.html\"\n    default: true\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch\n    css: ../../css/activity.css\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n---\n\n\n\n\n\n\n# In class activity 9: Correlation and Linear Regression\n\n## Introduction\n\nThis document demonstrates key concepts in correlation and regression analysis using ecological examples, focusing on:\n\n1.  **Understanding correlation vs. regression**\n2.  **Calculating and interpreting correlation coefficients**\n3.  **Testing correlation assumptions**\n4.  **Performing simple linear regression**\n5.  **Checking regression assumptions**\n6.  **Interpreting regression output and ANOVA tables**\n\nWe'll work with real ecological datasets to practice these concepts.\n\n# **Part 1:** Load Required Packages and Data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(patchwork)  # For combining plots\nlibrary(car)        # For regression diagnostics\nlibrary(broom)      # For tidy model output\n\n# Set seed for reproducible results\nset.seed(123)\n\n# Create the datasets from the lecture\n# Lion data from Example 17.1\nlion_data <- tibble(\n  proportion_black = c(0.21, 0.14, 0.11, 0.13, 0.12, 0.13, 0.12, 0.18, 0.23, 0.22, \n                      0.20, 0.17, 0.15, 0.27, 0.26, 0.21, 0.30, 0.42, 0.43, 0.59, \n                      0.60, 0.72, 0.29, 0.10, 0.48, 0.44, 0.34, 0.37, 0.34, 0.74, 0.79, 0.51),\n  age_years = c(1.1, 1.5, 1.9, 2.2, 2.6, 3.2, 3.2, 2.9, 2.4, 2.1, \n               1.9, 1.9, 1.9, 1.9, 2.8, 3.6, 4.3, 3.8, 4.2, 5.4, \n               5.8, 6.0, 3.4, 4.0, 7.3, 7.3, 7.8, 7.1, 7.1, 13.1, 8.8, 5.4)\n)\n\n# Booby data from Example 16.1\nbooby_data <- tibble(\n  visits_as_nestling = c(1, 7, 15, 4, 11, 14, 23, 14, 9, 5, 4, 10, \n                         13, 13, 14, 12, 13, 9, 8, 18, 22, 22, 23, 31),\n  future_aggression = c(-0.80, -0.92, -0.80, -0.46, -0.47, -0.46, -0.23, -0.16, \n                        -0.23, -0.23, -0.16, -0.10, -0.10, 0.04, 0.13, 0.19, \n                        0.25, 0.23, 0.15, 0.23, 0.31, 0.18, 0.17, 0.39)\n)\n\n# Prairie stability data from Example 17.3\nprairie_data <- tibble(\n  species_number = rep(c(1, 2, 4, 8, 16), times = c(32, 32, 32, 32, 33)),\n  log_stability = 1.20 + 0.033 * species_number + rnorm(161, 0, 0.35)\n)\n```\n:::\n\n\n\n\n\n\n::: callout-tip\n## Package Overview\n\n-   **tidyverse**: Collection of packages for data science\n-   **patchwork**: Combine multiple ggplot2 plots easily\n-   **car**: Companion to Applied Regression (diagnostic tools)\n-   **broom**: Convert statistical objects into tidy data frames\n:::\n\n# **Part 2:** Correlation Analysis\n\n::: callout-note\n## Correlation Analysis: Data Types and Assumptions\n\n**Data Types Required:**\n\n-   \\- **X variable**: Continuous numerical\n-   \\- **Y variable**: Continuous numerical - Both variables should be measured (not manipulated)\n\n**Assumptions for Pearson Correlation:**\n\n-   \\- Random sampling from the population\n-   \\- Bivariate normality (both variables normally distributed)\n-   \\- Linear relationship between variables\n-   \\- No extreme outliers\n:::\n\n## Calculating Correlation Coefficients\n\nLet's start with the Nazca booby data to explore correlation:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Pearson correlation coefficient\nbooby_corr <- cor(booby_data$visits_as_nestling, booby_data$future_aggression)\nprint(paste(\"Correlation coefficient (r):\", round(booby_corr, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Correlation coefficient (r): 0.534\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform correlation test\nbooby_cor_test <- cor.test(booby_data$visits_as_nestling, booby_data$future_aggression)\nprint(booby_cor_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  booby_data$visits_as_nestling and booby_data$future_aggression\nt = 2.9603, df = 22, p-value = 0.007229\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1660840 0.7710999\nsample estimates:\n      cor \n0.5337225 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate R-squared (variance explained)\nr_squared <- booby_corr^2\nprint(paste(\"R-squared (variance explained):\", round(r_squared * 100, 1), \"%\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R-squared (variance explained): 28.5 %\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Visualizing the Correlation\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create scatterplot with correlation\nbooby_plot <- ggplot(booby_data, aes(x = visits_as_nestling, y = future_aggression)) +\n  geom_point(size = 3, alpha = 0.7) +\n  # geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", alpha = 0.2) +\n  labs(title = \"Nazca Booby: Early Experience vs. Adult Aggression\",\n       subtitle = paste(\"r =\", round(booby_corr, 3), \", p =\", round(booby_cor_test$p.value, 3)),\n       x = \"Visits by Non-parent Adults (as nestling)\",\n       y = \"Future Aggression Score\") +\n  theme_minimal()\n\nbooby_plot\n```\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/booby_plot-1.png)\n:::\n:::\n\n\n\n\n\n\n::: callout-important\n## Activity 1: Interpret the Correlation\n\nBased on the output above, answer these questions:\n\n1.  **Direction**: Is the correlation positive or negative? What does this mean biologically?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Strength**: How would you classify this correlation (weak, moderate, strong)?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Significance**: Is the correlation statistically significant? What is the p-value?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Variance explained**: What percentage of variance in adult aggression is explained by nestling visits?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n## Testing Correlation Assumptions\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test normality of each variable\nshapiro_visits <- shapiro.test(booby_data$visits_as_nestling)\nshapiro_aggression <- shapiro.test(booby_data$future_aggression)\nshapiro_visits\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  booby_data$visits_as_nestling\nW = 0.95783, p-value = 0.3965\n```\n\n\n:::\n\n```{.r .cell-code}\nshapiro_aggression\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  booby_data$future_aggression\nW = 0.91575, p-value = 0.04709\n```\n\n\n:::\n\n```{.r .cell-code}\n# print(\"Normality Tests:\")\n# print(paste(\"Visits as nestling - Shapiro-Wilk p-value:\", round(shapiro_visits$p.value, 4)))\n# print(paste(\"Future aggression - Shapiro-Wilk p-value:\", round(shapiro_aggression$p.value, 4)))\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create diagnostic plots\np1 <- ggplot(booby_data, aes(x = visits_as_nestling)) +\n  geom_histogram(bins = 10, fill = \"lightblue\", color = \"black\") +\n  labs(title = \"Distribution of Visits\", x = \"Visits as Nestling\", y = \"Count\") +\n  theme_minimal()\n\np2 <- ggplot(booby_data, aes(x = future_aggression)) +\n  geom_histogram(bins = 10, fill = \"lightgreen\", color = \"black\") +\n  labs(title = \"Distribution of Aggression\", x = \"Future Aggression\", y = \"Count\") +\n  theme_minimal()\n\np3 <- ggplot(booby_data, aes(sample = visits_as_nestling)) +\n  stat_qq() + stat_qq_line() +\n  labs(title = \"Q-Q Plot: Visits\", x = \"Theoretical\", y = \"Sample\") +\n  theme_minimal()\n\np4 <- ggplot(booby_data, aes(sample = future_aggression)) +\n  stat_qq() + stat_qq_line() +\n  labs(title = \"Q-Q Plot: Aggression\", x = \"Theoretical\", y = \"Sample\") +\n  theme_minimal()\n\n# Combine plots\n(p1 + p2) / (p3 + p4)\n```\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/test_correlation_assumptions-1.png)\n:::\n:::\n\n\n\n\n\n\n::: callout-warning\n## When Assumptions Are Violated\n\nIf normality assumptions are violated (p \\< 0.05 in Shapiro-Wilk test), consider:\n\n1.  **Spearman's rank correlation** (non-parametric alternative)\n2.  **Data transformation** (log, square root, etc.)\n3.  **Removing outliers** (if justified)\n\nLet's try Spearman's correlation:\n:::\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate Spearman's rank correlation\nspearman_test <- cor.test(booby_data$visits_as_nestling, \n                          booby_data$future_aggression, \n                          method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in cor.test.default(booby_data$visits_as_nestling,\nbooby_data$future_aggression, : Cannot compute exact p-value with ties\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(spearman_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSpearman's rank correlation rho\n\ndata:  booby_data$visits_as_nestling and booby_data$future_aggression\nS = 1213.5, p-value = 0.01976\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n     rho \n0.472374 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Compare with Pearson\nprint(paste(\"Pearson r:\", round(booby_corr, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Pearson r: 0.534\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Spearman rho:\", round(spearman_test$estimate, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Spearman rho: 0.472\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n# **Part 3:** Simple Linear Regression\n\nNow let's move from correlation to regression using the lion nose data.\n\n::: callout-note\n## Linear Regression: Data Types and Assumptions\n\n**Data Types Required:**\n\n-   \\- **X variable (predictor)**: Continuous numerical\n-   \\- **Y variable (response)**: Continuous numerical - X can be fixed/controlled, Y is the outcome of interest\n\n**Assumptions for Linear Regression:**\n\n-   \\- **Linearity**: Relationship between X and Y is linear\n\n-   \\- **Independence**: Observations are independent\n\n-   \\- **Homoscedasticity**: Constant variance of residuals\n\n-   \\- **Normality**: Residuals are normally distributed\n\n-   \\- **No influential outliers**\n:::\n\n## Fitting a Linear Regression Model\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit linear regression model\nlion_model <- lm(age_years ~ proportion_black, data = lion_data)\n\n# Get model summary\nsummary(lion_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,\tAdjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get tidy output\ntidy_output <- tidy(lion_model)\nprint(tidy_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term             estimate std.error statistic      p.value\n  <chr>               <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)         0.879     0.569      1.55 0.133       \n2 proportion_black   10.6       1.51       7.05 0.0000000768\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get model fit statistics\nglance_output <- glance(lion_model)\nprint(glance_output)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic      p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>        <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.624         0.611  1.67      49.8 0.0000000768     1  -60.8  128.  132.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: callout-important\n## Activity 2: Interpret the Regression Output\n\nFrom the regression output above:\n\n1.  **Regression equation**: Write the equation in the form: age = \\_\\_\\_ + \\_\\_\\_ × proportion_black\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Slope interpretation**: What does the slope value mean in biological terms?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **R-squared**: What percentage of variation in age is explained by nose blackness?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Significance**: Is the relationship statistically significant? How do you know?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n## Visualizing the Regression\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create regression plot with confidence interval\nlion_plot <- ggplot(lion_data, aes(x = proportion_black, y = age_years)) +\n  geom_point(size = 3, alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\", fill = \"pink\", alpha = 0.3) +\n  labs(title = \"Lion Age Prediction from Nose Pigmentation\",\n       subtitle = paste(\"R² =\", round(glance_output$r.squared, 3), \n                       \", p <\", round(glance_output$p.value, 4)),\n       x = \"Proportion of Black on Nose\",\n       y = \"Age (years)\") +\n  theme_minimal()\n\nlion_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/lion_regression_plot-1.png)\n:::\n:::\n\n\n\n\n\n\n::: callout-tip\n## Confidence vs. Prediction Intervals\n\n-   **Confidence Interval**: Range for the mean age of ALL lions with that nose blackness\n-   **Prediction Interval**: Range for an INDIVIDUAL lion with that nose blackness\n-   Prediction intervals are always wider than confidence intervals\n:::\n\n# **Part 4:** Testing Regression Assumptions\n\n## Diagnostic Plots\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create diagnostic plots\npar(mfrow = c(2, 2))\nplot(lion_model)\n```\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/regression_diagnostics-1.png)\n:::\n\n```{.r .cell-code}\npar(mfrow = c(1, 1))\n```\n:::\n\n\n\n\n\n\n## Interpreting Diagnostic Plots\n\n::: callout-note\n## Understanding Regression Diagnostic Plots\n\n1.  **Residuals vs Fitted**:\n    -   Look for: Random scatter around horizontal line at 0\n    -   Problems: Patterns indicate non-linearity or heteroscedasticity\n2.  **Q-Q Plot**:\n    -   Look for: Points following the diagonal line\n    -   Problems: Deviations indicate non-normal residuals\n3.  **Scale-Location**:\n    -   Look for: Random scatter with horizontal trend line\n    -   Problems: Increasing spread indicates heteroscedasticity\n4.  **Residuals vs Leverage**:\n    -   Look for: Points within Cook's distance lines\n    -   Problems: Points outside indicate influential observations\n:::\n\n## Formal Tests of Assumptions\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for normality of residuals\nshapiro_residuals <- shapiro.test(residuals(lion_model))\nprint(paste(\"Shapiro-Wilk test for residuals: p =\", round(shapiro_residuals$p.value, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Shapiro-Wilk test for residuals: p = 0.0692\"\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for homoscedasticity (Breusch-Pagan test)\nlibrary(lmtest)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: zoo\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'zoo'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n```\n\n\n:::\n\n```{.r .cell-code}\nbp_test <- bptest(lion_model)\nprint(bp_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  lion_model\nBP = 6.8946, df = 1, p-value = 0.008646\n```\n\n\n:::\n:::\n\n\n\n\n\n\n::: callout-important\n## Activity 3: Assess Assumption Violations\n\nBased on the diagnostic plots and tests:\n\n1.  **Linearity**: Does the relationship appear linear? (Check Residuals vs Fitted plot)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Normality**: Are the residuals normally distributed? (Check Q-Q plot and Shapiro test)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Homoscedasticity**: Is the variance constant? (Check Scale-Location plot and BP test)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Influential points**: Are there any concerning influential observations?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n# **Part 5:** ANOVA for Regression\n\n## Understanding Variance Partitioning\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get ANOVA table for regression\nanova_table <- anova(lion_model)\nprint(\"ANOVA Table for Lion Regression:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ANOVA Table for Lion Regression:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: age_years\n                 Df  Sum Sq Mean Sq F value    Pr(>F)    \nproportion_black  1 138.544 138.544   49.75 7.677e-08 ***\nResiduals        30  83.543   2.785                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate sums of squares manually to understand partitioning\nss_total <- sum((lion_data$age_years - mean(lion_data$age_years))^2)\nss_residual <- sum(residuals(lion_model)^2)\nss_regression <- ss_total - ss_residual\n\nprint(\"\\nManual calculation of sums of squares:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nManual calculation of sums of squares:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Total:\", round(ss_total, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Total: 222.09\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Regression:\", round(ss_regression, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Regression: 138.54\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Residual:\", round(ss_residual, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Residual: 83.54\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Regression + SS Residual:\", round(ss_regression + ss_residual, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Regression + SS Residual: 222.09\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n## Visualizing Variance Components\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a plot showing variance components\n# Get predicted values\nlion_data$predicted <- predict(lion_model)\nmean_age <- mean(lion_data$age_years)\n\n# Select one point to illustrate\nexample_point <- 10\n\n# Create the visualization\nvariance_plot <- ggplot(lion_data, aes(x = proportion_black, y = age_years)) +\n  geom_point(size = 3, alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", size = 1) +\n  geom_hline(yintercept = mean_age, linetype = \"dashed\", color = \"darkgreen\") +\n  # Add lines for one example point\n  geom_segment(aes(x = proportion_black[example_point], \n                   y = age_years[example_point],\n                   xend = proportion_black[example_point], \n                   yend = predicted[example_point]),\n               color = \"red\", size = 1) +\n  geom_segment(aes(x = proportion_black[example_point], \n                   y = predicted[example_point],\n                   xend = proportion_black[example_point], \n                   yend = mean_age),\n               color = \"darkgreen\", size = 1) +\n  # Add labels\n  annotate(\"text\", x = 0.15, y = mean_age + 0.5, \n           label = \"Mean\", color = \"darkgreen\") +\n  annotate(\"text\", x = lion_data$proportion_black[example_point] + 0.05, \n           y = (lion_data$age_years[example_point] + lion_data$predicted[example_point])/2,\n           label = \"Residual\", color = \"red\") +\n  annotate(\"text\", x = lion_data$proportion_black[example_point] + 0.05, \n           y = (lion_data$predicted[example_point] + mean_age)/2,\n           label = \"Regression\", color = \"darkgreen\") +\n  labs(title = \"Variance Components in Regression\",\n       subtitle = \"Total variation = Regression + Residual\",\n       x = \"Proportion Black\", y = \"Age (years)\") +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nvariance_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = proportion_black[example_point], y = age_years[example_point], : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning in geom_segment(aes(x = proportion_black[example_point], y = predicted[example_point], : All aesthetics have length 1, but the data has 32 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/variance_visualization-1.png)\n:::\n:::\n\n\n\n\n\n\n# **Part 6:** Comparing Multiple Datasets\n\nLet's practice regression with the prairie biodiversity data:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit regression for prairie data\nprairie_model <- lm(log_stability ~ species_number, data = prairie_data)\n\n# Get summary\nsummary(prairie_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log_stability ~ species_number, data = prairie_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-0.8146 -0.2165 -0.0094  0.2228  0.7780 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.222902   0.039094  31.281  < 2e-16 ***\nspecies_number 0.028881   0.004694   6.153 5.94e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3271 on 159 degrees of freedom\nMultiple R-squared:  0.1923,\tAdjusted R-squared:  0.1872 \nF-statistic: 37.86 on 1 and 159 DF,  p-value: 5.94e-09\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create plot\nprairie_plot <- ggplot(prairie_data, aes(x = species_number, y = log_stability)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"darkgreen\", fill = \"lightgreen\") +\n  labs(title = \"Biodiversity-Stability Relationship\",\n       subtitle = paste(\"R² =\", round(glance(prairie_model)$r.squared, 3)),\n       x = \"Number of Plant Species\",\n       y = \"Log(Stability)\") +\n  theme_minimal()\n\nprairie_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](09_02_class_activity_files/figure-docx/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n\n\n\n\n::: callout-important\n## Activity 4: Compare the Two Regressions\n\nCompare the lion and prairie regression models:\n\n1.  **Which model explains more variance?** (Compare R² values)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Which has a stronger relationship?** (Compare standardized slopes or correlation)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Which has more precise estimates?** (Compare standard errors relative to estimates)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n    -   \n:::\n\n# **Summary and Key Takeaways**\n\n::: callout-tip\n## What We Learned Today\n\n1.  **Correlation vs. Regression:**\n    -   Correlation: Measures association between two variables\n    -   Regression: Predicts one variable from another\n2.  **Assumptions Matter:**\n    -   Always check assumptions before interpreting results\n    -   Use appropriate alternatives when assumptions are violated\n3.  **Interpretation:**\n    -   R² tells us proportion of variance explained\n    -   Slopes tell us rate of change\n    -   P-values tell us if relationships are statistically significant\n4.  **Practical Considerations:**\n    -   Correlation ≠ Causation\n    -   Outliers can have major impacts\n    -   Sample size affects power to detect relationships\n:::\n\n::: callout-warning\n## Common Mistakes to Avoid\n\n1.  **Using correlation when you mean regression** (or vice versa)\n2.  **Ignoring assumption violations**\n3.  **Extrapolating beyond the range of data**\n4.  **Confusing confidence and prediction intervals**\n5.  **Over-interpreting R² values**\n6.  **Forgetting about biological significance vs. statistical significance**\n:::\n\n## Additional Resources\n\n-   Whitlock & Schluter Chapter 16 (Correlation)\n-   Whitlock & Schluter Chapter 17 (Regression)\n-   R for Data Science: <https://r4ds.had.co.nz/>\n-   Quick-R Regression: <https://www.statmethods.net/stats/regression.html>\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}