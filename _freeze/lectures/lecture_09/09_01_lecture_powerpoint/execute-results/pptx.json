{
  "hash": "7c6a4cb66f6314b62bceb6694832f202",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 09\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true\n  message: false\n  warning: false\n  fig-height: 6\n  fig-width: 5\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"09_01_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"09_01_lecture_powerpoint_slides.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: true\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n\n\n# Lecture 8: Review\n\n::::: columns\n::: {.column width=\"60%\"}\nCovered\n\n-   Study design\n-   Causality in ecology\n-   Experimental design:\n    -   Replication, controls, randomization, independence\n-   Sampling in field studies\n-   Power analysis: *a priori* and *post hoc*\n-   Study design and analysis\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/review-plot-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Overview\n\n## The objectives:\n\nThis lecture covers two fundamental statistical techniques in biology: correlation and regression analysis. Based on Chapters 16-17 from Whitlock & Schluter's *The Analysis of Biological Data* (3rd edition), we'll explore:\n\n-   Correlation analysis: measuring relationships between variables\n-   The distinction between correlation and regression\n-   Simple linear regression: predicting one variable from another\n-   Estimating and interpreting regression parameters\n-   Testing assumptions and handling violations\n-   Analysis of variance in regression\n-   Model selection and comparison\n\n# **Lecture 9:** Correlation vs. Regression:\n\n## What's the Difference?\n\n::::: columns\n::: {.column width=\"60%\"}\n**Correlation Analysis:**\n\n-   Measures the strength and direction of a relationship between two numerical variables\n-   Both X and Y are random variables (both measured, neither manipulated)\n-   Variables are typically on equal footing (either could be X or Y)\n-   No cause-effect relationship implied\n-   Quantifies the degree to which variables are related\n-   Expressed as a correlation coefficient (r) from -1 to +1\n\n**Regression Analysis:**\n\n-   Predicts one variable (Y) from another (X)\n-   X is often fixed or controlled (manipulated)\n-   Y is the response variable of interest\n-   Often implies a cause-effect relationship\n-   Produces an equation for prediction\n-   Estimates slope and intercept parameters\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/unnamed-chunk-1-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Correlation Analysis\n\n::::: columns\n::: {.column width=\"60%\"}\n## What Is Correlation?\n\n**Correlation analysis** measures the strength and direction of a relationship between two numerical variables:\n\n-   Ranges from -1 to +1\n-   +1 indicates perfect positive correlation\n-   0 indicates no correlation\n-   -1 indicates perfect negative correlation\n\nThe **Pearson correlation coefficient (r)** is defined as:\n\n$$r = \\frac{\\sum_{i}(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sqrt{\\sum_{i}(X_i - \\bar{X})^2 \\sum_{i}(Y_i - \\bar{Y})^2}}$$\n\nThis can be simplified as:\n\n$$r = \\frac{\\text{Covariance}(X, Y)}{s_X \\cdot s_Y}$$\n\nWhere $s_X$ and $s_Y$ are the standard deviations of X and Y.\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1i-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Correlation Analysis\n\n## Example 16.1: Flipping the Bird\n\nNazca boobies (*Sula granti*) - Do aggressive behaviors as a chick predict future aggressive behavior as an adult?\n\n-   correlation is r = 0.534 - moderate positive relationship\n-   p-value = 0.007 correlation is statistically significant.\n\nFor a Pearson correlation coefficient (r) of 0.53372:\n\n-   This is r (not rho as Spearman nonparticipant below), as indicated by \"cor\" in your output\n-   To determine the amount of variation explained, you square this value: r¬≤ = 0.53372¬≤ = 0.2849 (or approximately 28.49%)\n-   means about 28.49% of the variance in one variable can be explained by the other variable\n\n## Note $\\text{t}=\\frac{r}{SE_r}$\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5337225\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's product-moment correlation\n\ndata:  booby_data$visits_as_nestling and booby_data$future_aggression\nt = 2.9603, df = 22, p-value = 0.007229\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.1660840 0.7710999\nsample estimates:\n      cor \n0.5337225 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Lecture 9:** Correlation Analysis\n\n::::: columns\n::: {.column width=\"60%\"}\n## Example 16.1: Flipping the Bird\n\n**Interpretation:** The correlation coefficient of r = 0.534 suggests that Nazca boobies who experienced more visits from non-parent adults as nestlings tend to display more aggressive behavior as adults. This supports the hypothesis that early experiences influence adult behavior patterns in this species.\n\n**Standard Error:**\n\n## $\\text{SE}_r = \\sqrt{\\frac{1-r^2}{n-2}}$\n\n## SE = 0.180\n\nNeed to be sure relationship is not curved - note below\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1k-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Correlation Analysis\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Assumptions for Correlation\n\nAs described in Section 16.3, correlation analysis has key assumptions:\n\n1.  **Random sampling**: Observations should be a random sample from the population\n2.  **Bivariate normality**: Both variables follow a normal distribution, and their joint distribution is bivariate normal\n3.  **Linear relationship**: The relationship between variables is linear, not curved\n\nLet's check these assumptions using the lion data from Example 17.1 Lion Noses:\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lion_data$proportion_black\nW = 0.88895, p-value = 0.003279\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lion_data$age_years\nW = 0.87615, p-value = 0.001615\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Correlation Analysis\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Assumptions for Correlation\n\nAs described in Section 16.3, correlation analysis has key assumptions:\n\n1.  **Random sampling**: Observations should be a random sample from the population\n2.  **Bivariate normality**: Both variables follow a normal distribution, and their joint distribution is bivariate normal\n3.  **Linear relationship**: The relationship between variables is linear, not curved\n\nLet's check these assumptions using the lion data from Example 17.1 Lion Noses:\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1m-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Correlation Analysis\n\n## **What to do if assumptions are violated:**\n\nTransform one or both variables (log, square root, etc.)\n\nUse non-parametric correlation (**Spearman's rank correlation**) or Kendall's tau ùõï\n\nExamine the data for outliers or influential points\n\nTo understand the amount of variation explained, you can square the Spearman's rho value.\n\nFor your value of 0.74485:\n\nœÅ¬≤ = 0.74485¬≤ = 0.5548\n\nThis means approximately 55.48% of the variance in ranks of one variable can be explained by the ranks of the other variable. This is similar to how R¬≤ works in linear regression, but specifically for ranked data.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tSpearman's rank correlation rho\n\ndata:  lion_data$proportion_black and lion_data$age_years\nS = 1392.1, p-value = 1.013e-06\nalternative hypothesis: true rho is not equal to 0\nsample estimates:\n      rho \n0.7448561 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Lecture 9:** Correlation Analysis\n\n::::: columns\n::: {.column width=\"60%\"}\n## Correlation: Important Considerations\n\n**The correlation coefficient depends on the range**\n\n-   Restricting range of values can reduce the correlation coefficient\n-   Comparing correlations between studies requires similar ranges of values\n\n**Measurement error affects correlation**\n\n-   Measurement error in X or Y tends to weaken observed correlation\n-   This bias is called **attenuation**\n-   True correlation typically stronger than observed correlation\n\n**Correlation vs. Causation**\n\n-   Correlation does not imply causation\n-   Three possible explanations for correlation:\n    1.  X causes Y\n    2.  Y causes X\n    3.  Z (a third variable) causes both X and Y\n\n**Correlation significance test**\n\n-   H‚ÇÄ: œÅ = 0 (no correlation in population)\n-   H‚ÇÅ: œÅ ‚â† 0 (correlation exists in population)\n-   **Test statistic: t = r / SE(r) with df = n-2**\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1o-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Simple Linear Regression Model\n\n**Simple linear regression** models the relationship between a response variable (Y) and a predictor variable (X).\n\nThe **population** regression model $$Y = \\alpha + \\beta X + \\varepsilon$$\n\nWhere:\n\n-   Y is the response variable\n-   X is the predictor variable\n-   Œ± (alpha) is the intercept (value of Y when X=0)\n-   Œ≤ (beta) is the slope (change in Y per unit change in X)\n-   Œµ (epsilon) is the error term (random deviation from the line)\n\nThe **sample** regression equation is:\n\n$$\\hat{Y} = a + bX$$\n\nWhere:\n\n-   $\\hat{Y}$ is the predicted value of Y\n-   a is the estimate of Œ± (intercept)\n-   b is the estimate of Œ≤ (slope)\n\n**Method of Least Squares**: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values.\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3360244964.png){width=\"329\"}\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Simple Linear Regression Model\n\n**Simple linear regression** models the relationship between a response variable (Y) and a predictor variable (X).\n\nThe **population** regression model is:\n\n$$Y = \\alpha + \\beta X + \\varepsilon$$\n\nWhere:\n\n-   Y is the response variable\n\n-   X is the predictor variable\n\n-   Œ± (alpha) is the intercept (value of Y when X=0)\n\n-   Œ≤ (beta) is the slope (change in Y per unit change in X)\n\n-   Œµ (epsilon) is the error term (random deviation from the line)\n\nThe **sample** regression equation is:\n\n$$\\hat{Y} = a + bX$$\n\nWhere:\n\n-   $\\hat{Y}$ is the predicted value of Y\n\n-   a is the estimate of Œ± (intercept)\n\n-   b is the estimate of Œ≤ (slope)\n\n**Method of Least Squares**: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values.\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1p-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"40%\"}\nFrom Example 17.1 in the textbook the regression line for the lion data is:\n\n## $\\text{age} = 0.88 + 10.65 \\times \\text{proportion}_{black}$\n\nThis means: - When a lion has no black on its nose (proportion = 0), its predicted age is 0.88 years - For each 0.1 increase in the proportion of black, age increases by 1.065 years - The slope (10.65) indicates that lions with more black on their noses tend to be older\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1q-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n## Simple Linear Regression Model\n\n-   male lions develop more black pigmentation on their noses as they age.\n-   can be used to estimate the age of lions in the field.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,\tAdjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Simple Linear Regression Model\n\nThe calculation for slope (b) is:\\\n$$b = \\frac{\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i(X_i - \\bar{X})^2}$$\n\nGiven: - $\\bar{X} = 0.3222$ - $\\bar{Y} = 4.3094$ - $\\sum_i(X_i - \\bar{X})^2 = 1.2221$ - $\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y}) = 13.0123$\n\nb = 13.0123 / 1.2221 = 10.647\n\nIntercept (a): $a = \\bar{Y} - b\\bar{X} = 4.3094 - 10.647(0.3222) = 0.879$\n\n**Making predictions:**\n\nTo predict the age of a lion with 0.50 proportion of black on its nose:\n\n$$\\hat{Y} = 0.88 + 10.65(0.50) = 6.2 \\text{ years}$$\n\n**Confidence intervals vs. Prediction intervals:**\n\n-   **Confidence interval**: Range for the mean age of all lions with 0.50 black\n-   **Prediction interval**: Range for an individual lion with 0.50 black\n\nBoth intervals are narrowest near $\\bar{X}$ and widen as X moves away from the mean.\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1b-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n## Example Prairie Home Companion\n\n-   Does biodiversity affect ecosystem stability?\n-   Tilman et al. (2006) investigated using experimental plots varying plant species\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 √ó 2\n  species_number log_stability\n           <dbl>         <dbl>\n1              1         0.763\n2              1         1.45 \n3              1         1.51 \n4              1         0.747\n5              1         0.983\n6              1         1.12 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log_stability ~ species_number, data = prairie_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.82774 -0.25344 -0.00426  0.27498  0.75240 \n\nCoefficients:\n               Estimate Std. Error t value Pr(>|t|)    \n(Intercept)    1.252629   0.041023  30.535  < 2e-16 ***\nspecies_number 0.025984   0.004926   5.275 4.28e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3433 on 159 degrees of freedom\nMultiple R-squared:  0.149,\tAdjusted R-squared:  0.1436 \nF-statistic: 27.83 on 1 and 159 DF,  p-value: 4.276e-07\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"rsquared is:  0.148953385305455\"\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: log_stability\n                Df  Sum Sq Mean Sq F value    Pr(>F)    \nspecies_number   1  3.2792  3.2792  27.829 4.276e-07 ***\nResiduals      159 18.7358  0.1178                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\nThe hypothesis test asks whether the slope equals zero:\n\n-   H‚ÇÄ: Œ≤ = 0 (species number does not affect stability)\n-   H‚ÇÅ: Œ≤ ‚â† 0 (species number does affect stability)\n\n## The test statistic is: $t = \\frac{b - \\beta_0}{SE_b}$\n\nWith df = n - 2 = 161 - 2 = 159\n\n**Interpretation:**\n\nThe slope estimate is 0.033, indicating that log stability increases by 0.033 units for each additional plant species in the plot.\n\nThe p-value is very small (2.73e-10), providing strong evidence to reject the null hypothesis that species number has no effect on ecosystem stability.\n\nR¬≤ = 0.222, meaning that approximately 22.2% of the variation in log stability is explained by the number of plant species.\n\nThis supports the biodiversity-stability hypothesis: more diverse plant communities have more stable biomass production over time.\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1d-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Regression Assumptions\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nLet's check these assumptions for the lion regression model:\\\n\\\nAssume that **error ùûÆ i**s $e_i = y_i - \\hat{y}_i$\n\n-   normally distributed for each x~i~\n\n-   has the same variance\n\n-   has a mean of 0 at each xi\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2092034232.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Regression Assumptions\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nLet's check these assumptions for the lion regression model:\\\n\\\nAssume that **error ùûÆ i**s - estimated as the residuals: $e_i = y_i - \\hat{y}_i$\n\n-   ordinary lease square estimates a and b or slope and intercept to minimize the sum of the residuals squared or Mean Squared Error (MSE) as\n\n## $\\sum_{i=1}^{n} = (y_i - \\hat{y}_i)^2$\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1411052425.png)\n:::\n:::::\n\n# \n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Regression Assumptions\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nLet's check these assumptions for the lion regression model:\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1e-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Regression Assumptions\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nLet's check these assumptions for the lion regression model:\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1f-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Testing Regression Assumptions\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nLet's check these assumptions for the lion regression model:\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  residuals(lion_model)\nW = 0.93879, p-value = 0.0692\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression\n\n::::: columns\n::: {.column width=\"40%\"}\n## Simple Linear Regression Model\n\nlinear regression has four key assumptions:\n\n1.  **Linearity**: The relationship between X and Y is linear\n2.  **Independence**: Observations are independent\n3.  **Homoscedasticity**: Equal variance across all values of X\n4.  **Normality**: Residuals are normally distributed\n\nIf assumptions are violated: 1. Transform the data (Section 17.6) 2. Use weighted least squares for heteroscedasticity 3. Consider non-linear models (Section 17.8)\n:::\n\n::: {.column width=\"60%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-pptx/overview-plot-1h-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of error and significance\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Estimates of standard error and confidence intervals for slow and intercept to determine confidence bands\n\n-   the 95% confidence band will contain the true population line 95/100 under repeated sampling\n\n-   this is usually done in R\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-934594306.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of error and significance\n\n::::: columns\n::: {.column width=\"60%\"}\nIn addition to getting estimates of population parameters (Œ≤0 , Œ≤1), want to test hypotheses about them\n\n-   This is accomplished by analysis of variance\n-   Partition variance in Y: due to variation in X, due to other things (error)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2817787184.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is ‚Äúpartitioned‚Äù into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   difference between predicted values (≈∑i ) and mean y (»≥)\n    -   dfs= 1 for simple linear (parameters-1)\n-   $SS_{residual}$: variation not explained by regression\n    -   difference between observed ($y_i$) and predicted ($\\hat{y}_i$) values\n    -   dfs= n-2\n-   $SS_{total}$: total variation\n    -   sum of squared deviations of each observation ($y_i$) from mean ($\\bar{y}$)\n\n    -   dfs = n-1\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2817787184.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is ‚Äúpartitioned‚Äù into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   difference between predicted values (≈∑i ) and mean y (»≥)\n    -   dfs= 1 for simple linear (parameters-1)\n-   $SS_{residual}$: variation not explained by regression\n    -   difference between observed ($y_i$) and predicted ($\\hat{y}_i$) values\n    -   dfs= n-2\n-   $SS_{total}$: total variation\n    -   sum of squared deviations of each observation ($y_i$) from mean ($\\bar{y}$)\n\n    -   dfs = n-1\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-77063277.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is ‚Äúpartitioned‚Äù into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   GREATER IN C than D\n-   $SS_{residual}$: variation not explained by regression\n    -   GREATER IN B THAN A\n-   $SS_{total}$: total variation\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-217151541.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nSums of Squares and degress of freedome are:\n\n$SS_{regression} +SS_{residual} = SS_{total}$\n\n$df_{regression}+df_{residual} = df_{total}$\n\n-   Sums of Squares depends on n\n-   We need a different estimate of variance\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2758972276.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nSums of Squares converted to Mean Squares\n\n-   Sums of Squares divided by degrees of freedom - does not depend on n\n-   $MS_{residual}$: estimate population variation\n-   $MS_{regression}$: estimate pop variation and variation due to X-Y relationship\n-   Mean Squares are not additive\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2758972276.png)\n:::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}