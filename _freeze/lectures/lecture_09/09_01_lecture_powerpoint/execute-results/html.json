{
  "hash": "64fe9961b1ad29df9b05a3925321184f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 9\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true # retains the images when you start again\n  message: false\n  warning: false\n  fig-height: 3\n  fig-width: 3\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"09_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch  # Limits line length to approximately 80 characters\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"09_lecture_powerpoint_slides.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n# Lecture 8: Review\n\n::::: columns\n::: {.column width=\"60%\"}\nCovered\n\n-   Decision errors\n-   Data exploration and transformation\n    -   Exploratory graphical data analysis\n    -   Graphical testing of assumptions\n    -   Data transformation and standardization\n    -   Outliers\n-   R practice: robust tests, basic graphics\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/review-plot-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Overview\n\n::::: columns\n::: {.column width=\"60%\"}\n## The objectives:\n\n-   Note on test assumptions\n-   Multiple testing\n-   Graphics:\n    -   Why graphics?\n\n    -   Rules of good graphics\n\n    -   Some bad graphics\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1496527660.png){width=\"413\"}\n:::\n:::::\n\n# **Lecture 9:** Overview\n\n:::::: columns\n:::: columns\n::: {.column width=\"60%\"}\nAssumption testing: iterative process\n\nIf unable to transform: non-parametric approach\n\nWhen assumptions are violated, we can:\n\n1.  1\\. Transform data\n\n2.  2\\. Use robust methods\n\n3.  3\\. Use non-parametric tests\n:::\n::::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1496527660.png)\n:::\n::::::\n\n# \n\n# **Lecture 9:** Overview\n\n:::::: columns\n:::: columns\n::: {.column width=\"60%\"}\nAssumption testing: iterative process\n\nIf unable to transform: non-parametric approach\n\nWhen assumptions are violated, we can:\n\n1.  1\\. Transform data\n2.  2\\. Use robust methods\n3.  3\\. Use non-parametric tests\n:::\n::::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/overview-plot-1-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n::::::\n\n# **Testing assumptions**\n\n::::: columns\n::: {.column width=\"60%\"}\nAssumption testing: iterative process\n\nIf unable to transform: non-parametric approach\n\nWhen assumptions are violated, we can:\n\n1.  1\\. Transform data\n\n2.  2\\. Use robust methods\n\n3.  3\\. Use non-parametric tests\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/unnamed-chunk-1-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# \n\n# **Testing assumptions**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Assumption testing: iterative process\n-   If unable to transform: non-parametric approach\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Testing normality assumption on mice weights\nshapiro.test(trout_data$mass_g)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  trout_data$mass_g\nW = 0.87436, p-value < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Testing equality of variances across sampling sites\n# First create a model\nmice_model <- lm(mass_g ~ sampling_site, data = trout_data)\n# Then test for homogeneity of variances\ncar::leveneTest(mice_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n       Df F value    Pr(>F)    \ngroup   1  26.352 3.911e-07 ***\n      569                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\nWhen assumptions are violated, we can:\n\n1\\. Transform data\n\n2\\. Use robust methods\n\n3\\. Use non-parametric tests\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/assumption-plots-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 9:** Overview\n\n::::: columns\n::: {.column width=\"60%\"}\n## The objectives:\n\n-   Note on test assumptions\n-   Multiple testing\n-   Graphics:\n    -   Why graphics?\n    -   Rules of good graphics\n    -   Some bad graphics\n-   \n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/overview-plot-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Multiple testing**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Multiple tests: simultaneous tests of related hypotheses on single dataset\n    -   e.g., 5 pops of snails, are mean sizes different among all groups (1-2, 1-3, 1-5, 2-3, 2-4...)?\n-   Multiple testing increases possibility of type I error\n-   If 5% chance of falsely rejecting Ho in 1 test, with each additional test your **\"family-wise\"** type I error rate increases:\n    -   1 test = 0.05\n    -   2 tests = 0.098\n    -   5 tests = 0.23\n    -   20 tests = 0.64\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to calculate family-wise error rate\nfamily_wise_error <- function(alpha_per_test, num_tests) {\n  1 - (1 - alpha_per_test)^num_tests\n}\n\n# Create a data frame of family-wise error rates\nerror_rates <- tibble(\n  num_tests = c(1, 2, 5, 10, 20, 50, 100),\n  error_rate = family_wise_error(0.05, num_tests)\n)\n\nerror_rates\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 7 × 2\n  num_tests error_rate\n      <dbl>      <dbl>\n1         1     0.0500\n2         2     0.0975\n3         5     0.226 \n4        10     0.401 \n5        20     0.642 \n6        50     0.923 \n7       100     0.994 \n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/fwer-plot-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Multiple testing adjustments**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Adjust **family-wise** rate using lower pair-wise rate (α=0.01), but increase type II error rate...\n-   Common correction methods:\n    -   **Bonferroni correction**: pairwise α₍ₚₑₚ = α₍fwₑₚ/c\n        -   For 20 tests, desired α₍fwₑₚ=0.05, α₍ₚₑₚ = 0.0025\n    -   **Holm-Sidak**: 1 - (1 - α₍fwₑₚ)^1/c^\n        -   For 20 tests α₍ₚₑₚ = 0.0026\n    -   **Sequential Holm**: p-values ranked, smallest tested at α₍fwₑₚ/c (0.005), second at α₍fwₑₚ/(c-1) (0.0055), etc.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's perform multiple t-tests on our mice data\n# Compare mass between each pair of sampling sites\n\n# Get unique sampling sites\nsites <- unique(trout_data$lake)\nnum_sites <- length(sites)\nnum_comparisons <- num_sites * (num_sites - 1) / 2\n\n# Matrix to store results\nresults <- data.frame(\n  comparison = character(num_comparisons),\n  p_value = numeric(num_comparisons),\n  stringsAsFactors = FALSE\n)\n\n# Perform pairwise t-tests\ncounter <- 1\nfor (i in 1:(num_sites-1)) {\n  for (j in (i+1):num_sites) {\n    site_i_data <- trout_data$mass_g[trout_data$lake == sites[i]]\n    site_j_data <- trout_data$mass_g[trout_data$lake == sites[j]]\n    \n    test_result <- t.test(site_i_data, site_j_data)\n    \n    results$comparison[counter] <- paste(sites[i], \"vs\", sites[j])\n    results$p_value[counter] <- test_result$p.value\n    \n    counter <- counter + 1\n  }\n}\n\n# Apply different p-value adjustments\nresults$bonferroni <- p.adjust(results$p_value, method = \"bonferroni\")\nresults$holm <- p.adjust(results$p_value, method = \"holm\")\nresults$BH <- p.adjust(results$p_value, method = \"BH\")  # Benjamini-Hochberg\n\n# Display results\nresults %>%\n  arrange(p_value) %>%\n  mutate(across(where(is.numeric), round, 4))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       comparison p_value bonferroni   holm     BH\n1 NE 12 vs Toolik  0.6718     0.6718 0.6718 0.6718\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/adjustment-plot-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Graphics: Why use them?**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Graphics are visual metaphors for data\n-   Closest to actual data: table\n-   But graphics can:\n    -   Summarize data (means, CVs, R²)\n    -   Make patterns more apparent\n    -   Communicate results efficiently\n    -   Tell a story with the data\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First, let's look at the data as a table\nmice_summary <- trout_data %>%\n  group_by(lake) %>%\n  summarize(\n    n = n(),\n    mean_mass = mean(mass_g),\n    sd_mass = sd(mass_g),\n    min_mass = min(mass_g),\n    max_mass = max(mass_g)\n  )\n\nmice_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 6\n  lake       n mean_mass sd_mass min_mass max_mass\n  <chr>  <int>     <dbl>   <dbl>    <dbl>    <dbl>\n1 NE 12    322      534.    520.     9        2320\n2 Toolik   249      518.    373.     0.15     3400\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/mice-plot-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Good scientific graphics**\n\n::::: columns\n::: {.column width=\"60%\"}\nAccording to Tufte (2001), good scientific graphics:\n\n-   Show the data\n-   Are efficient: show many numbers in small space\n-   Make large datasets coherent by using appropriate graphic methods\n-   Encourage comparison\n-   Reveal several layers of information (e.g., averages, relationships, variability)\n-   Serve clear purpose: important to telling the main story\n-   Integrated with statistical methods (e.g., boxplots with t-tests, scatter plots with regression)\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's create a plot showing several layers of information\npine_summary <- pine_data %>%\n  group_by(group) %>%\n  summarize(\n    mean_length = mean(length_mm),\n    sd_length = sd(length_mm),\n    n = n()\n  ) %>%\n  mutate(se_length = sd_length / sqrt(n),\n         conf_low = mean_length - qt(0.975, n-1) * se_length,\n         conf_high = mean_length + qt(0.975, n-1) * se_length)\n\npine_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 7\n  group       mean_length sd_length     n se_length conf_low conf_high\n  <chr>             <dbl>     <dbl> <int>     <dbl>    <dbl>     <dbl>\n1 cephalopods        18        3.86    12     1.11      15.5      20.5\n2 crayfish           18        3.86    12     1.11      15.5      20.5\n3 salmon             16.3      3.94    12     1.14      13.8      18.8\n4 snail              18.3      2.27    12     0.655     16.9      19.8\n```\n\n\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/good-graphics-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Principles of good graphics**\n\n::::: columns\n::: {.column width=\"60%\"}\nTo make good graphics:\n\n-   Above all, focus on data\n-   Do not distort data\n-   Graphical representation of numbers → directly proportional to numbers\n-   Strive for clarity through labelling\n-   Maximize data-ink ratio\n    -   Remove non-data ink\n    -   Reduce redundant data ink\n-   Revise and redraw\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Let's create two versions of the same plot\n# First, a \"poor\" version with low data-ink ratio\nlibrary(ggthemes)\np1 <- ggplot(trout_data, aes(x = lake, y = mass_g)) +\n  geom_bar(stat = \"summary\", fun = \"mean\", fill = \"lightblue\", \n           color = \"black\") +\n  geom_errorbar(stat = \"summary\", fun.data = \"mean_se\", width = 0.5) +\n  # theme_excel() +\n  labs(title = \"Average trout Mass by Sampling Site\",\n       subtitle = \"This plot has a low data-ink ratio\",\n       x = \"Sampling Site\", y = \"Average Mass (g)\")\n```\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/compare-plots-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Bad graphics examples**\n\n::::: columns\n::: {.column width=\"60%\"}\nCommon problems in graphics:\n\n1.  **Distorting the data**:\n    -   Using non-zero baselines for bar charts\n    -   Using 3D effects that distort perspective\n    -   Using inappropriate scales\n2.  **Chart junk**:\n    -   Excessive gridlines\n    -   Unnecessary legends\n    -   Decorative elements that don't add information\n3.  **Poor color choices**:\n    -   Too many colors\n    -   Non-color-blind friendly palettes\n    -   Colors that don't print well in grayscale\n4.  **Misleading representations**:\n    -   Pie charts for many categories\n    -   Dual y-axes with different scales\n    -   Truncated axes without clear indication\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/bad-graphics-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **R practice: ggplot2**\n\n::::: columns\n::: {.column width=\"60%\"}\nLet's create some plots with ggplot2 using the mice data:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Basic scatter plot\nggplot(trout_data, aes(x = lake, y = mass_g)) +\n  geom_point() +\n  labs(title = \"Mouse Mass vs. Year\",\n       x = \"Year\", y = \"Mass (g)\")\n```\n\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/basic-plots-1.png){width=480}\n:::\n\n```{.r .cell-code}\n# Scatter plot with grouping and trend line\nggplot(trout_data, aes(x = lake, y = mass_g, color = lake)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"Mouse Mass vs. Year by Sampling Site\",\n       x = \"Year\", y = \"Mass (g)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/basic-plots-2.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](09_01_lecture_powerpoint_files/figure-html/complex-plots-1.png){width=480}\n:::\n:::\n\n\n\n\n\n:::\n:::::\n\n# **Final Activity:** Take home messages\n\n::::: columns\n::: {.column width=\"60%\"}\n**Key points about multiple testing:**\n\n1.  Running multiple tests increases the family-wise error rate\n2.  Various correction methods exist (Bonferroni, Holm, Benjamini-Hochberg)\n3.  Choose the appropriate correction based on your research question\n4.  Report both uncorrected and corrected p-values for transparency\n\n**Principles of good graphics:**\n\n1.  Focus on the data, not decoration\n2.  Maximize data-ink ratio\n3.  Ensure proportional representation\n4.  Clear labeling and annotation\n5.  Choose appropriate visualization for your data type\n:::\n\n::: {.column width=\"40%\"}\nWhen applying multiple testing corrections:\n\n-   Bonferroni: Most conservative, controls family-wise error rate\n-   Holm: Less conservative than Bonferroni, still controls FWER\n-   Benjamini-Hochberg: Controls false discovery rate instead of FWER\n-   No correction: Highest power, but highest type I error rate\n\nAlternative to multiple pairwise tests: - ANOVA with post-hoc tests - Planned comparisons - Multilevel models\n:::\n:::::\n\n# **Summary and Conclusions**\n\nIn this lecture, we've:\n\n1.  Explored the problem of multiple testing and why it increases type I error rates\n2.  Learned various methods for correcting p-values in multiple test scenarios\n3.  Discussed principles of good scientific graphics based on Tufte's work\n4.  Identified common pitfalls in data visualization\n5.  Practiced creating effective visualizations using ggplot2\n\n**Key takeaways:**\n\n-   Be cautious when conducting multiple tests on the same dataset\n-   Apply appropriate corrections to control error rates\n-   Focus on clear, efficient data visualization that emphasizes the data\n-   Remove chart junk and maximize the data-ink ratio\n-   Choose visualization methods that match your research question and data type\n-   Consider both statistical significance and visual presentation when communicating results\n\n# **What do you see as the key points?**\n\nThings that stood out\n\n1.  The dramatic increase in type I error rate with multiple testing\n2.  The trade-off between type I error control and statistical power\n3.  The importance of choosing appropriate graphics to communicate your findings\n4.  How poor visualization choices can mislead readers even when the statistics are correct\n\n# **What are the muddy points?**\n\nWhat does not make sense or what questions do you have...\n\nWhat makes you nervous?\n\n1.  When to choose which multiple testing correction method\n2.  How to balance aesthetic appeal with statistical accuracy in graphics\n3.  Deciding between different visualization types for the same data\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}