{
  "hash": "09f92ce9f381eb4ed6b34c0191c4ea64",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10 - Regression and Linear Models\"\nauthor: \"Your Name\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true\n  message: false\n  warning: false\n  fig-height: 6\n  fig-width: 5\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"10_01_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"10_01_lecture_powerpoint_slides.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: true\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n\n\n# Lecture 9: Review\n\n::::: columns\n::: {.column width=\"60%\"}\nCovered\n\n-   Correlation analysis: measuring relationships between variables\n-   The distinction between correlation and regression\n-   Simple linear regression: predicting one variable from another\n-   Estimating and interpreting regression parameters\n-   Testing assumptions and handling violations\n-   Analysis of variance in regression\n-   Model selection and comparison\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_01_lecture_powerpoint_files/figure-pptx/review-plot-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# Lecture 10: Overview\n\n::::: columns\n::: {.column width=\"60%\"}\nLinear regression:\n\n-   REGRESSIONS:\n    -   Analysis of variance\n    -   Explained variance\n    -   Assumptions and diagnostics\n    -   Dealing w violations\n    -   Model II regression\n    -   Robust regression\n-   Smoothing Regressions\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1429633952.png)\n:::\n:::::\n\n# **Lecture 10:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Simple Linear Regression Model\n\n**Simple linear regression** models the relationship between a response variable (Y) and a predictor variable (X).\n\nThe **sample** regression equation is:\n\n$$\\hat{Y} = a + bX$$\n\nWhere:\n\n-   $\\hat{Y}$ is the predicted value of Y\n-   a is the estimate of α (intercept) sometimes $\\beta_0$\n-   b is the estimate of β (slope) sometimes $\\beta_1$\n\n**Method of Least Squares**: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values.\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3360244964.png){width=\"329\"}\n:::\n:::::\n\n# **Lecture 10:** Linear Regression\n\n::::: columns\n::: {.column width=\"60%\"}\n## Simple Linear Regression Model\n\n**Simple linear regression** models the relationship between a response variable (Y) and a predictor variable (X).\n\nThe **sample** regression equation is:\n\n$$\\hat{Y} = a + bX$$\n\nWhere:\n\n-   $\\hat{Y}$ is the predicted value of Y\n-   a is the estimate of α (intercept) sometimes $\\beta_0$\n-   b is the estimate of β (slope) sometimes $\\beta_1$\n\n**Method of Least Squares**: The line is chosen to minimize the sum of squared vertical distances (residuals) between observed and predicted Y values.\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_01_lecture_powerpoint_files/figure-pptx/overview-plot-1p-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 10:** Linear Regression\n\n## Simple Linear Regression Model\n\n-   male lions develop more black pigmentation on their noses as they age.\n-   can be used to estimate the age of lions in the field.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,\tAdjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Lecture 10:** Linear Regression\n\n:::: columns\n-   ::: {.column width=\"60%\"}\n    ## Simple Linear Regression Model\n\n    The calculation for slope (b) is:\\\n    $$b = \\frac{\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y})}{\\sum_i(X_i - \\bar{X})^2}$$\n\n    Given: -\n\n    $\\bar{X} = 0.3222$\n\n    $\\bar{Y} = 4.3094$\n\n    $\\sum_i(X_i - \\bar{X})^2 = 1.2221$\n\n    $\\sum_i(X_i - \\bar{X})(Y_i - \\bar{Y}) = 13.0123$\n\n    b = 13.0123 / 1.2221 = 10.647\n\n    Intercept (a):\n\n    $a = \\bar{Y} - b\\bar{X} = 4.3094 - 10.647(0.3222) = 0.879$\n\n    **Making predictions:**\n\n    To predict the age of a lion with 0.50 proportion of black on its nose:\n\n    $$\\hat{Y} = 0.88 + 10.65(0.50) = 6.2 \\text{ years}$$\n\n    **Confidence intervals vs. Prediction intervals:**\n\n    -   **Confidence interval**: Range for the mean age of all lions with 0.50 black\n    -   **Prediction interval**: Range for an individual lion with 0.50 black\n\n    Both intervals are narrowest near $\\bar{X}$ and widen as X moves away from the mean.\n    :::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_01_lecture_powerpoint_files/figure-pptx/overview-plot-1b-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n::::\n\n# **Lecture 10:** Linear Regression - estimates of error and significance\n\n::::: columns\n::: {.column width=\"60%\"}\nIn addition to getting estimates of population parameters (intercept - β0 , slope - β1)\n\nwant to test hypotheses about them\n\n-   This is accomplished by analysis of variance\n-   Partition variance in Y: due to variation in X, due to other things (error)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2817787184.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is “partitioned” into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   difference between predicted values (ŷi ) and mean y (ȳ)\n    -   dfs= 1 for simple linear (parameters-1)\n-   $SS_{residual}$: variation not explained by regression\n    -   difference between observed ($y_i$) and predicted ($\\hat{y}_i$) values\n    -   dfs= n-2\n-   $SS_{total}$: total variation\n    -   sum of squared deviations of each observation ($y_i$) from mean ($\\bar{y}$)\n\n    -   dfs = n-1\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2817787184.png)\n:::\n:::::\n\n# **Lecture 9:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is “partitioned” into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   difference between predicted values (ŷi ) and mean y (ȳ)\n    -   dfs= 1 for simple linear (parameters-1)\n-   $SS_{residual}$: variation not explained by regression\n    -   difference between observed ($y_i$) and predicted ($\\hat{y}_i$) values\n    -   dfs= n-2\n-   $SS_{total}$: total variation\n    -   sum of squared deviations of each observation ($y_i$) from mean ($\\bar{y}$)\n\n    -   dfs = n-1\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-77063277.png)\n:::\n:::::\n\n# **Lecture 10:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nTotal variation in Y is “partitioned” into 3 components:\n\n-   $SS_{regression}$: variation explained by regression\n    -   GREATER IN C than D\n-   $SS_{residual}$: variation not explained by regression\n    -   GREATER IN B THAN A\n-   $SS_{total}$: total variation\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-217151541.png){width=\"693\"}\n:::\n:::::\n\n# **Lecture 10:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nSums of Squares and degress of freedome are:\n\n$SS_{regression} +SS_{residual} = SS_{total}$\n\n$df_{regression}+df_{residual} = df_{total}$\n\n-   Sums of Squares depends on n\n-   We need a different estimate of variance\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2758972276.png)\n:::\n:::::\n\n# **Lecture 10:** Linear Regression - estimates of variance\n\n::::: columns\n::: {.column width=\"60%\"}\nSums of Squares converted to Mean Squares\n\n-   Sums of Squares divided by degrees of freedom - does not depend on n\n-   $MS_{residual}$: estimate population variation\n-   $MS_{regression}$: estimate pop variation and variation due to X-Y relationship\n-   Mean Squares are not additive\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2758972276.png)\n:::\n:::::\n\n# **Lecture 10:** Linear Regression - Null Hypothesis\n\nRegression typically tests null hypothesis that β1 = 0\n\n-   or no relationship between X and Y\n\nCan test in two ways:\n\n## Using t-statistic:\n\n$$t=\\frac{b_1-\\theta}{s_{b_{1}}}$$\n\n-   $s_{b_{1}}$= Standard error of slope estimate\n-   Bo= 0: t-test: $t=\\frac{b_o}{s_{b_{o}}}$\n-   1 parameter t-test, where testing whether β1 =0\n-   t-statistic test is more general\n-   R can provide both\n-   Can also ask whether β0 =0 using t-test\n-   or whether two regression lines are significantly different\n\n# **Lecture 10:** Linear Regression - Null Hypothesis\n\nRegression typically tests null hypothesis that β1 = 0\n\n-   or no relationship between X and Y\n\nCan test in two ways:\n\n## Using F-ratio:\n\n$$F = \\frac {MS_{regression}}{MS_{residual}}$$\n\n-   if β1 = 0, ratio will be = 1 otherwise \\>1\n-   compare F-ratio to df-specific F-distribution\n-   decide how likely obtain our F-ratio by chance\n\n# **Lecture 10:** Linear Regression - Explained variance\n\n-   Want to know how strong is association between X and Y\n-   Coefficient of determination ($R^2$): proportion of variation in Y explained by X\n\n$$r^2 = \\frac{SS_{regression}}{SS_{total}}=1-\\frac{SS_{residual}}{SS_{total}}$$\n\n-   When more of variation is due to regression rather than ‘error’, $R^2$ closer to 1\n\n# **Lecture 10:** Linear Regression - Explained variance\n\n-   Want to know how strong is association between X and Y\n-   Coefficient of determination ($R^2$): proportion of variation in Y explained by X\n\n$$r^2 = \\frac{SS_{regression}}{SS_{total}}=1-\\frac{SS_{residual}}{SS_{total}}$$\n\n-   When more of variation is due to regression rather than ‘error’, $R^2$ closer to 1\n\n![](images/clipboard-3874208145.png){width=\"472\"}\n\n# **Lecture 10:** Linear Regression - Explained variance\n\n$$F = \\frac{MS_{regression}}{MS_{residual}}$$ $$r^2 = \\frac{SS_{regression}}{SS_{total}}$$\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(lion_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = age_years ~ proportion_black, data = lion_data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.5449 -1.1117 -0.5285  0.9635  4.3421 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(>|t|)    \n(Intercept)        0.8790     0.5688   1.545    0.133    \nproportion_black  10.6471     1.5095   7.053 7.68e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.669 on 30 degrees of freedom\nMultiple R-squared:  0.6238,\tAdjusted R-squared:  0.6113 \nF-statistic: 49.75 on 1 and 30 DF,  p-value: 7.677e-08\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nanova(lion_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: age_years\n                 Df  Sum Sq Mean Sq F value    Pr(>F)    \nproportion_black  1 138.544 138.544   49.75 7.677e-08 ***\nResiduals        30  83.543   2.785                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Reporting results\n\n“Lion age (years) could be predicted from nose spots (percentage) using the simple linear regression model age = 10.67 \\* proportion_black + 0.8749. Regression analysis showed that the slope of the relationship was significantly (at α=0.05) different than 0 ($F_{1,30}$ = 49.93, p \\< 0.0001, R² = 0.6247).”\n\nNote there is an adjusted R² - what is that - accounts for the number of predictors in your model - adjusts the R² value by penalizing the addition of variables that don't improve the model fit significantly\n\nThe formula for adjusted R² is:\n\n$$ R^2 = 1 - \\frac{(1 - R²) × (n - 1)}{(n - p - 1)}$$\n\nWhere:\n\n-   n is the number of observations (32 lions)\n-   p is the number of predictors (1 = proportion_black)\n\n$R^2$ measures the proportion of variance in the dependent variable (age_years) that is explained by the independent variable (proportion_black).\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Assumptions apply to observed values of Y and εi\n-   most can be assessed by looking at residuals (distance from predicted)\n\n**Linearity:**\n\n-   relationship between X and Y in population is straight line\n\n**Check:**\n\n-   examine biplot of Y on X\n\n**If violated:**\n\n-   transform Y\n-   use polynomial or nonlinear regression\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-4068017677.png){width=\"256\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n**Normality:**\n\n-   y-values for each xi are normally distributed.\n-   OLS estimates moderately robust to violation\n\n**Check:**\n\n-   are residuals normally distributed?\n-   qq plots, histogram of residuals, shapiro-wilk test\n\n**If violated:**\n\n-   transform Y\n-   use Generalized Linear Model\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1121573498.png){width=\"299\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n**Homogeneity of variance:**\n\n-   y-values for each xi have same variance.\n-   OLS estimates *NOT* robust to violation\n\n**Check:**\n\n-   plot residuals against x-values or predicted values (ŷi)\n\n**If violated:**\n\n-   transform Y\n-   use GLM\n-   weighted LS regression\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1847603074.png){width=\"344\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n**Independence**:\n\n-   Y values from each xi do not influence each other\n-   Often violated with repeated measurements in time/space -\\> autocorrelation\n\n**Check**: determine correlation coefficient bw adjacent residuals\n\n**If violate**:\n\n-   ANOVA (grouping present)\n-   mixed model ANOVA\n-   time series\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-4068017677.png){width=\"256\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n**Fixed X:**\n\n-   xi are known values fixed by researcher (e.g., drug doses).\n-   Often not true in ecology.\n\n**If violated**:\n\n-   not problem for H-testing, prediction, **but**\n-   error underestimated.\n-   Can use model II regression\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-4068017677.png){width=\"256\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\nOutlier or Influence:\n\n-   how much each point affects slope (Cook’s D)\n-   Large Di (\\>1) indicates influential observation\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3088349564.png){width=\"247\"}\n:::\n:::::\n\n# Assumptions and diagnostics of regression\n\n::::: columns\n::: {.column width=\"60%\"}\n**Residual plots**\n\nresiduals vs. predicted y:\n\n-   can be used to assess assumptions:\n\n    -   linearity\n    -   normality\n    -   equal variance\n    -   outliers\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-78208610.png)\n:::\n:::::\n\n# Dealing with violations\n\nWeighted least squares:\n\n-   when variance unequal, can use WLS approach\n-   each point weighted by reciprocal of variance (points w large variance given less weight)\n\nRobust regression:\n\n-   when distribution distinctly non-normal and/or large outliers\n\nLAD:\n\n-   parameters estimated from non-squared residuals\n-   outliers not as influential\n\nM-estimators:\n\n-   residuals have different weight depending on distance from mean\n\nRank-based: “if all else fails”\n\n# Model II regression\n\n::::: columns\n::: {.column width=\"60%\"}\nFixed X is assumption of regular regression\n\nwhat if X random (typical case)?\n\n-   If goal is prediction (interpolation) then Model I is ok…\n\n-   if goal is correct parameters and error estimates, may need to use Model II\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1269601555.png){width=\"322\"}\n:::\n:::::\n\n# Model II regression\n\n::::: columns\n::: {.column width=\"60%\"}\nModel II regression - Approach underused in ecology\n\n-   Model I regression will still perform well for H-tests\n-   underestimate true slope\n-   minimizes distance between points and line along both axes\\\n-   (vs. on Y only in OLS)\n\nMA and RMA approach slightly different\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3599319832.png){width=\"300\"}\n:::\n:::::\n\n# Model II Regression\n\n## Detailed Explanation of Model II Regression Types\n\n1.  Standardized Major Axis (SMA)\n\n-   SMA regression minimizes the product of the vertical and horizontal distances from the points to the regression line. It's implemented in the smatr package with method=\"SMA\". SMA is appropriate when the measurement scales of X and Y are different.\n\n2.  Major Axis (MA)\n\n-   MA regression minimizes the perpendicular distances from the data points to the regression line. It's implemented in the smatr package with method=\"MA\". MA is appropriate when X and Y are measured in the same units.\n\n3.  Reduced Major Axis (RMA)\n\n-   RMA regression (also called geometric mean regression) is available in the lmodel2 package. It produces a slope that is the geometric mean of the OLS regression slopes of Y on X and X on Y (specifically, it equals the OLS slope of Y on X multiplied by the sign of the correlation between X and Y, divided by the square root of the R² value).\n\n# Model II Regression\n\n## When to Use Each Method\n\nOLS (Model I) - Use when:\n\n-   X is measured without error\n-   The research goal is predicting Y from X\n-   There's a clear dependent variable\n\nMA (Major Axis) - Use when:\n\n-   X and Y are measured in the same units\n-   Both variables have similar error variances\n-   The goal is to understand the symmetric relationship\n\nSMA (Standardized Major Axis) - Use when:\n\n-   X and Y are measured in different units\n-   The goal is to understand the structural relationship\n-   You want to test for isometry or allometry in scaling studies\n\nRMA (Reduced Major Axis) - Use when:\n\n-   The ratio of error variances is approximately equal to the ratio of the true variances\n-   Both variables contain measurement error\n-   Neither variable is clearly dependent or independent\n\n# Model II Regression\n\n## Key Differences in Results\n\nThe slopes of these methods will typically follow this pattern when the correlation coefficient is less than 1: OLS slope \\< MA slope \\< RMA slope \\< inverse of OLS (X on Y) slope This is particularly evident when the correlation between X and Y is weaker. As correlation approaches 1, the differences between methods diminish.\n\n# Model II Regression\n\n## Decision Tree\n\nHere's a simplified decision tree:\n\n-Are X and Y measured with error? If No → Use OLS (Model I) -Are the errors in X and Y approximately equal? If Yes → Use MA -Are X and Y measured in different units/scales? If Yes → Consider SMA -Is the correlation between X and Y weak (\\<0.7)? If Yes → Method choice is critical; consider RMA -Are you uncertain about error structure? If Yes → RMA is a reasonable compromise\n\nRemember that when the correlation between X and Y is very strong (r \\> 0.9), all methods will yield similar results, making the choice less critical. The differences between methods become more pronounced as the correlation weakens.\n\nFinally, it's often valuable to run multiple methods and compare the results. If they lead to different ecological or biological interpretations, this should be explicitly addressed in your discussion.\n\n# Model II Regression\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = y ~ x, data = data_ols_m2)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-8.058 -3.498 -0.990  2.946 16.070 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   3.1346     2.6584   1.179    0.241    \nx             2.8745     0.2617  10.982   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 4.858 on 98 degrees of freedom\nMultiple R-squared:  0.5517,\tAdjusted R-squared:  0.5471 \nF-statistic: 120.6 on 1 and 98 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nModel II regression\n\nCall: lmodel2(formula = y ~ x, data = data_ols_m2, range.y =\n\"relative\", range.x = \"relative\", nperm = 99)\n\nn = 100   r = 0.7427668   r-square = 0.5517025 \nParametric P-values:   2-tailed = 9.070988e-19    1-tailed = 4.535494e-19 \nAngle between the two OLS regression lines = 8.317517 degrees\n\nPermutation tests of OLS, MA, RMA slopes: 1-tailed, tail corresponding to sign\nA permutation test of r is equivalent to a permutation test of the OLS slope\nP-perm for SMA = NA because the SMA slope cannot be tested\n\nRegression results\n  Method  Intercept    Slope Angle (degrees) P-perm (1-tailed)\n1    OLS   3.134600 2.874473        70.81773              0.01\n2     MA -18.688419 5.059928        78.82062              0.01\n3    SMA  -6.805843 3.869953        75.51163                NA\n4    RMA  -8.131038 4.002664        75.97273              0.01\n\nConfidence intervals\n  Method 2.5%-Intercept 97.5%-Intercept 2.5%-Slope 97.5%-Slope\n1    OLS      -2.140952        8.410152   2.355051    3.393894\n2     MA     -29.748586      -10.906922   4.280654    6.167542\n3    SMA     -12.339088       -1.965648   3.385234    4.424077\n4    RMA     -16.211541       -1.554125   3.344023    4.811882\n\nEigenvalues: 54.08817 1.502864 \n\nH statistic used for computing C.I. of MA: 0.001181286 \n```\n\n\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_01_lecture_powerpoint_files/figure-pptx/unnamed-chunk-4-1.png)\n:::\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![](10_01_lecture_powerpoint_files/figure-pptx/unnamed-chunk-5-1.png)\n:::\n:::\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n\n```\n                2.5 %   97.5 %\n(Intercept) -2.140952 8.410152\nx            2.355051 3.393894\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall: sma(formula = y ~ x, data = data_ols_m2, method = \"SMA\") \n\nFit using Standardized Major Axis \n\n------------------------------------------------------------\nCoefficients:\n             elevation    slope\nestimate     -6.805843 3.869953\nlower limit -12.094381 3.385234\nupper limit  -1.517306 4.424077\n\nH0 : variables uncorrelated\nR-squared : 0.5517025 \nP-value : < 2.22e-16 \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall: sma(formula = y ~ x, data = data_ols_m2, method = \"MA\") \n\nFit using Major Axis \n\n------------------------------------------------------------\nCoefficients:\n             elevation    slope\nestimate    -18.688419 5.059928\nlower limit -27.905282 4.280260\nupper limit  -9.471556 6.168337\n\nH0 : variables uncorrelated\nR-squared : 0.5517025 \nP-value : < 2.22e-16 \n```\n\n\n:::\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}