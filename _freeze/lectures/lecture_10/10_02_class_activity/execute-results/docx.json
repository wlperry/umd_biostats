{
  "hash": "50fdaa8f70d795e28da982032c3776f1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 10 - Class Activity: Regression and Linear Models\"\nauthor: \"Your Name\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true\n  fig-height: 4\n  fig-width: 6\n  paged-print: false\nformat:\n  html:\n    freeze: false\n    toc: false\n    output-file: \"10_02_class_activity.html\"\n    default: true\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch\n    css: ../../css/activity.css\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n---\n\n\n\n\n# Class Activity 10: Advanced Linear Regression and Model II Regression\n\n## Introduction\n\nThis activity builds on simple linear regression to explore:\n\n1.  **Analysis of variance in regression**\n2.  **Regression assumptions and diagnostics**\n3.  **Dealing with assumption violations**\n4.  **Model II regression (when X has error)**\n5.  **Robust regression techniques**\n6.  **Comparing different regression approaches**\n\nWe'll work with the same datasets from the lecture to practice these advanced concepts.\n\n# **Part 1:** Load Required Packages and Data\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required packages\nlibrary(tidyverse)  # For data manipulation and visualization\nlibrary(patchwork)  # For combining plots\nlibrary(car)        # For regression diagnostics\nlibrary(broom)      # For tidy model output\nlibrary(smatr)      # For Model II regression\nlibrary(lmodel2)    # Alternative package for Model II regression\nlibrary(lmtest)     # For assumption tests\nlibrary(MASS)       # For robust regression\nlibrary(boot)       # For bootstrapping\n\n# Set seed for reproducible results\nset.seed(123)\n\n# Create the datasets from the lecture\n# Lion data - predicting age from nose blackness\nlion_data <- tibble(\n  proportion_black = c(0.21, 0.14, 0.11, 0.13, 0.12, 0.13, 0.12, 0.18, 0.23, 0.22, \n                      0.20, 0.17, 0.15, 0.27, 0.26, 0.21, 0.30, 0.42, 0.43, 0.59, \n                      0.60, 0.72, 0.29, 0.10, 0.48, 0.44, 0.34, 0.37, 0.34, 0.74, 0.79, 0.51),\n  age_years = c(1.1, 1.5, 1.9, 2.2, 2.6, 3.2, 3.2, 2.9, 2.4, 2.1, \n               1.9, 1.9, 1.9, 1.9, 2.8, 3.6, 4.3, 3.8, 4.2, 5.4, \n               5.8, 6.0, 3.4, 4.0, 7.3, 7.3, 7.8, 7.1, 7.1, 13.1, 8.8, 5.4)\n)\n\n# Fish allometric data - both X and Y have measurement error\nfish_data <- tibble(\n  length_mm = runif(40, 180, 320),\n  mass_g = 0.00001 * length_mm^3 * runif(40, 0.9, 1.1)\n)\n\n# Create example data where both X and Y have error (for Model II demonstration)\nn <- 50\nx_true <- rnorm(n, mean = 10, sd = 2)\nx_observed <- x_true + rnorm(n, 0, 1)  # X has measurement error\ny_observed <- 2 + 3 * x_true + rnorm(n, mean = 0, sd = 2)  # Y depends on true X\nmodel_comparison_data <- tibble(x = x_observed, y = y_observed)\n```\n:::\n\n\n\n\n::: callout-tip\n## New Packages for Advanced Regression\n\n-   **smatr**: Standardized Major Axis regression (Model II)\n-   **lmodel2**: Multiple Model II regression methods\n-   **lmtest**: Tests for regression assumptions\n-   **MASS**: Robust regression methods\n-   **boot**: Bootstrap confidence intervals\n:::\n\n# **Part 2:** Analysis of Variance in Regression\n\n::: callout-note\n## Linear Regression ANOVA: Data Types and Assumptions\n\n**Data Types Required:**\n\n\\- **X variable (predictor)**: Continuous numerical\n\n\\- **Y variable (response)**: Continuous numerical\n\n**Key Concept:**\n\n-   \\- Total variation in Y is partitioned into:\n\n    -   \\- **SS_regression**: Variation explained by X\n\n    -   \\- **SS_residual**: Unexplained variation (error)\n\n    -   \\- **SS_total** = SS_regression + SS_residual\n\n**Assumptions:**\n\n-   \\- Linearity between X and Y\n\n-   \\- Independence of observations\n\n-   \\- Homoscedasticity (constant variance)\n\n-   \\- Normality of residuals\n\n-   \\- No influential outliers\n:::\n\n## Understanding Variance Partitioning\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit the lion regression model\nlion_model <- lm(age_years ~ proportion_black, data = lion_data)\n\n# Get the ANOVA table\nanova_table <- anova(lion_model)\nprint(\"ANOVA Table for Lion Age ~ Nose Blackness:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ANOVA Table for Lion Age ~ Nose Blackness:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(anova_table)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nAnalysis of Variance Table\n\nResponse: age_years\n                 Df  Sum Sq Mean Sq F value    Pr(>F)    \nproportion_black  1 138.544 138.544   49.75 7.677e-08 ***\nResiduals        30  83.543   2.785                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate variance components manually\nss_total <- sum((lion_data$age_years - mean(lion_data$age_years))^2)\nss_residual <- sum(residuals(lion_model)^2)\nss_regression <- ss_total - ss_residual\n\nprint(\"\\nVariance Partitioning:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nVariance Partitioning:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Total:\", round(ss_total, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Total: 222.09\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Regression:\", round(ss_regression, 2), \n            \"(\", round(100*ss_regression/ss_total, 1), \"%)\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Regression: 138.54 ( 62.4 %)\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"SS Residual:\", round(ss_residual, 2), \n            \"(\", round(100*ss_residual/ss_total, 1), \"%)\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SS Residual: 83.54 ( 37.6 %)\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Calculate R-squared\nr_squared <- ss_regression / ss_total\nprint(paste(\"R-squared:\", round(r_squared, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"R-squared: 0.6238\"\n```\n\n\n:::\n:::\n\n\n\n\n## Visualizing Variance Components\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a detailed plot showing variance components\nlion_data$predicted <- predict(lion_model)\nmean_age <- mean(lion_data$age_years)\n\n# Select a few points to illustrate different components\nexample_points <- c(5, 15, 25)\n\nvariance_plot <- ggplot(lion_data, aes(x = proportion_black, y = age_years)) +\n  geom_point(size = 3, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\", size = 1) +\n  geom_hline(yintercept = mean_age, linetype = \"dashed\", color = \"darkgreen\", size = 1) +\n  \n  # Add variance components for example points\n  geom_segment(data = lion_data[example_points,],\n               aes(x = proportion_black, y = age_years,\n                   xend = proportion_black, yend = predicted),\n               color = \"red\", size = 1.2) +\n  geom_segment(data = lion_data[example_points,],\n               aes(x = proportion_black, y = predicted,\n                   xend = proportion_black, yend = mean_age),\n               color = \"orange\", size = 1.2) +\n  geom_segment(data = lion_data[example_points,],\n               aes(x = proportion_black, y = age_years,\n                   xend = proportion_black, yend = mean_age),\n               color = \"purple\", size = 0.8, linetype = \"dotted\") +\n  \n  # Add annotations\n  annotate(\"text\", x = 0.1, y = mean_age + 0.8, \n           label = \"Overall Mean\", color = \"darkgreen\", size = 4) +\n  annotate(\"text\", x = 0.8, y = 2, \n           label = \"Red: Residuals (unexplained)\", color = \"red\", size = 3) +\n  annotate(\"text\", x = 0.8, y = 1.2, \n           label = \"Orange: Explained by regression\", color = \"orange\", size = 3) +\n  annotate(\"text\", x = 0.8, y = 0.4, \n           label = \"Purple: Total deviation\", color = \"purple\", size = 3) +\n  \n  labs(title = \"Variance Partitioning in Regression\",\n       subtitle = paste(\"R² =\", round(r_squared, 3), \"- Proportion of variance explained\"),\n       x = \"Proportion of Black on Nose\",\n       y = \"Age (years)\") +\n  theme_minimal() +\n  theme(plot.subtitle = element_text(size = 12))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n```\n\n\n:::\n\n```{.r .cell-code}\nvariance_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/variance_visualization-1.png)\n:::\n:::\n\n\n\n\n::: callout-important\n## Activity 1: Understanding Variance Partitioning\n\nFrom the analysis above, answer these questions:\n\n1.  **Total Variance**: What is the total sum of squares for lion age?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Explained Variance**: What percentage of the total variance is explained by nose blackness?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **F-statistic**: What is the F-statistic value and what does it test?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Interpretation**: In biological terms, what does the R² value tell us about aging in lions?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n# **Part 3:** Testing Regression Assumptions in Detail\n\n::: callout-note\n## Regression Assumptions: Detailed Testing\n\n**Critical Assumptions to Test:**\n\n-   \\- **Linearity**: Relationship between X and Y is linear\n\n-   \\- **Normality**: Residuals are normally distributed\n\n-   \\- **Homoscedasticity**: Variance of residuals is constant\n\n-   \\- **Independence**: Observations are independent\n\n-   \\- **No influential outliers**: No single point dominates the relationship\n\n**Consequences of Violations:**\n\n-   \\- Biased parameter estimates\n\n-   \\- Incorrect standard errors\n\n-   \\- Invalid hypothesis tests\n\n-   \\- Poor predictions\n:::\n\n## Comprehensive Assumption Testing\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# 1. Test for Linearity\n# Create residual plots\nresidual_plots <- function(model, data, title_prefix = \"\") {\n  # Get residuals and fitted values\n  data$residuals <- residuals(model)\n  data$fitted <- fitted(model)\n  data$std_residuals <- rstandard(model)\n  \n  # Residuals vs Fitted (linearity and homoscedasticity)\n  p1 <- ggplot(data, aes(x = fitted, y = residuals)) +\n    geom_point(alpha = 0.7) +\n    geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n    geom_smooth(se = FALSE, color = \"blue\") +\n    labs(title = paste(title_prefix, \"Residuals vs Fitted\"),\n         subtitle = \"Should show random scatter around 0\",\n         x = \"Fitted Values\", y = \"Residuals\") +\n    theme_minimal()\n  \n  # Q-Q plot for normality\n  p2 <- ggplot(data, aes(sample = std_residuals)) +\n    stat_qq() + stat_qq_line(color = \"red\") +\n    labs(title = paste(title_prefix, \"Q-Q Plot\"),\n         subtitle = \"Points should follow the red line\",\n         x = \"Theoretical Quantiles\", y = \"Standardized Residuals\") +\n    theme_minimal()\n  \n  # Scale-Location plot (homoscedasticity)\n  p3 <- ggplot(data, aes(x = fitted, y = sqrt(abs(std_residuals)))) +\n    geom_point(alpha = 0.7) +\n    geom_smooth(se = FALSE, color = \"red\") +\n    labs(title = paste(title_prefix, \"Scale-Location\"),\n         subtitle = \"Should show horizontal line with random scatter\",\n         x = \"Fitted Values\", y = \"√|Standardized Residuals|\") +\n    theme_minimal()\n  \n  return(list(p1, p2, p3))\n}\n\n# Create diagnostic plots for lion model\nlion_plots <- residual_plots(lion_model, lion_data, \"Lion Model: \")\n(lion_plots[[1]] + lion_plots[[2]]) / lion_plots[[3]]\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/assumption_testing-1.png)\n:::\n:::\n\n\n\n\n## Formal Statistical Tests for Assumptions\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test 1: Normality of residuals (Shapiro-Wilk test)\nshapiro_test <- shapiro.test(residuals(lion_model))\nprint(\"1. NORMALITY TEST (Shapiro-Wilk):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"1. NORMALITY TEST (Shapiro-Wilk):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"H0: Residuals are normally distributed\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"H0: Residuals are normally distributed\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"p-value:\", round(shapiro_test$p.value, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"p-value: 0.0692\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Interpretation:\", ifelse(shapiro_test$p.value > 0.05, \n                                    \"Residuals appear normal (p > 0.05)\",\n                                    \"Residuals may not be normal (p ≤ 0.05)\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Interpretation: Residuals appear normal (p > 0.05)\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test 2: Homoscedasticity (Breusch-Pagan test)\nbp_test <- bptest(lion_model)\nprint(\"2. HOMOSCEDASTICITY TEST (Breusch-Pagan):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"2. HOMOSCEDASTICITY TEST (Breusch-Pagan):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"H0: Variance is constant (homoscedastic)\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"H0: Variance is constant (homoscedastic)\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"p-value:\", round(bp_test$p.value, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"p-value: 0.0086\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Interpretation:\", ifelse(bp_test$p.value > 0.05,\n                                    \"Variance appears constant (p > 0.05)\",\n                                    \"Variance may not be constant (p ≤ 0.05)\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Interpretation: Variance may not be constant (p ≤ 0.05)\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test 3: Influential observations (Cook's Distance)\ncooks_d <- cooks.distance(lion_model)\ninfluential_threshold <- 4/length(lion_data$age_years)\ninfluential_points <- which(cooks_d > influential_threshold)\n\nprint(\"3. INFLUENTIAL OBSERVATIONS (Cook's Distance):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"3. INFLUENTIAL OBSERVATIONS (Cook's Distance):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Threshold for influence:\", round(influential_threshold, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Threshold for influence: 0.125\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Number of influential points:\", length(influential_points)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Number of influential points: 2\"\n```\n\n\n:::\n\n```{.r .cell-code}\nif(length(influential_points) > 0) {\n  print(paste(\"Influential points:\", paste(influential_points, collapse = \", \")))\n}\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Influential points: 22, 30\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Test 4: Linearity (Rainbow test)\nrainbow_test <- raintest(lion_model)\nprint(\"4. LINEARITY TEST (Rainbow test):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"4. LINEARITY TEST (Rainbow test):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"H0: Relationship is linear\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"H0: Relationship is linear\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"p-value:\", round(rainbow_test$p.value, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"p-value: 4e-04\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Interpretation:\", ifelse(rainbow_test$p.value > 0.05,\n                                    \"Relationship appears linear (p > 0.05)\",\n                                    \"Relationship may be non-linear (p ≤ 0.05)\")))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Interpretation: Relationship may be non-linear (p ≤ 0.05)\"\n```\n\n\n:::\n:::\n\n\n\n\n## Outlier and Influence Analysis\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create influence plot\nlion_data$cooks_d <- cooks.distance(lion_model)\nlion_data$leverage <- hatvalues(lion_model)\nlion_data$std_residuals <- rstandard(lion_model)\n\ninfluence_plot <- ggplot(lion_data, aes(x = leverage, y = std_residuals)) +\n  geom_point(aes(size = cooks_d), alpha = 0.7) +\n  geom_hline(yintercept = c(-2, 2), linetype = \"dashed\", color = \"red\") +\n  geom_vline(xintercept = 2*2/nrow(lion_data), linetype = \"dashed\", color = \"blue\") +\n  labs(title = \"Influence Plot: Leverage vs Standardized Residuals\",\n       subtitle = \"Size = Cook's Distance; Dashed lines = concern thresholds\",\n       x = \"Leverage\", \n       y = \"Standardized Residuals\",\n       size = \"Cook's D\") +\n  theme_minimal()\n\ninfluence_plot\n```\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/outlier_analysis-1.png)\n:::\n:::\n\n\n\n\n::: callout-important\n## Activity 2: Assumption Testing Results\n\nBased on the diagnostic plots and formal tests:\n\n1.  **Linearity**: Is the relationship linear? (Check residuals vs fitted plot and Rainbow test)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Normality**: Are residuals normally distributed? (Check Q-Q plot and Shapiro test)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Homoscedasticity**: Is variance constant? (Check Scale-Location plot and BP test)\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Influential Points**: Are there concerning influential observations?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n5.  **Overall Assessment**: Can we trust the regression results?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n# **Part 4:** Model II Regression - When X Has Error\n\n::: callout-note\n## Model II Regression: Data Types and When to Use\n\n**Data Types Required:**\n\n\\- **X variable**: Continuous numerical (with measurement error)\n\n\\- **Y variable**: Continuous numerical (with measurement error)\n\n**When to Use Model II:**\n\n\\- Both X and Y have measurement error\n\n\\- Neither variable is clearly \"dependent\" or \"independent\"\n\n\\- Goal is to estimate the true structural relationship\n\n\\- Studying allometric relationships or scaling laws\n\n**Types of Model II Regression:**\n\n\\- **MA (Major Axis)**: Same units for X and Y\n\n\\- **SMA (Standardized Major Axis)**: Different units for X and Y\n\n\\- **RMA (Reduced Major Axis)**: General purpose\n\n**Assumptions:**\n\n\\- Linear relationship between true X and Y values\n\n\\- Errors in X and Y are normally distributed\n\n\\- Error variances are known or can be estimated\n:::\n\n## Comparing Model I vs Model II Regression\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Fit different regression models to data with X and Y error\nprint(\"COMPARING REGRESSION METHODS\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"COMPARING REGRESSION METHODS\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"=============================\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"=============================\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model I: Ordinary Least Squares (OLS)\nmodel_ols <- lm(y ~ x, data = model_comparison_data)\nols_slope <- coef(model_ols)[2]\nols_intercept <- coef(model_ols)[1]\n\nprint(\"MODEL I (OLS) RESULTS:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MODEL I (OLS) RESULTS:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Slope:\", round(ols_slope, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope: 2.302\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Intercept:\", round(ols_intercept, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Intercept: 9.22\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model II: Standardized Major Axis (SMA)\nmodel_sma <- sma(y ~ x, data = model_comparison_data, method = \"SMA\")\nsma_slope <- coef(model_sma)[2]\nsma_intercept <- coef(model_sma)[1]\n\nprint(\"MODEL II - SMA RESULTS:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MODEL II - SMA RESULTS:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Slope:\", round(sma_slope, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope: 2.945\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Intercept:\", round(sma_intercept, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Intercept: 2.766\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model II: Major Axis (MA)\nmodel_ma <- sma(y ~ x, data = model_comparison_data, method = \"MA\")\nma_slope <- coef(model_ma)[2]\nma_intercept <- coef(model_ma)[1]\n\nprint(\"MODEL II - MA RESULTS:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MODEL II - MA RESULTS:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Slope:\", round(ma_slope, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope: 3.609\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Intercept:\", round(ma_intercept, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Intercept: -3.91\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Model II: Reduced Major Axis using lmodel2\nmodel_rma <- lmodel2(y ~ x, data = model_comparison_data)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nRMA was not requested: it will not be computed.\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNo permutation test will be performed\n```\n\n\n:::\n\n```{.r .cell-code}\nrma_results <- model_rma$regression.results\nrma_slope <- rma_results$Slope[rma_results$Method == \"RMA\"]\nrma_intercept <- rma_results$Intercept[rma_results$Method == \"RMA\"]\n\nprint(\"MODEL II - RMA RESULTS:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"MODEL II - RMA RESULTS:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Slope:\", round(rma_slope, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope: \"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"Intercept:\", round(rma_intercept, 3)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Intercept: \"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"TRUE RELATIONSHIP (for comparison):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"TRUE RELATIONSHIP (for comparison):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Slope: 3.0, Intercept: 2.0\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Slope: 3.0, Intercept: 2.0\"\n```\n\n\n:::\n:::\n\n\n\n\n## Visualizing Different Regression Methods\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create comparison plot\ncomparison_plot <- ggplot(model_comparison_data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6, size = 2) +\n  \n  # Add different regression lines\n  geom_abline(intercept = ols_intercept, slope = ols_slope, \n              color = \"blue\", size = 1, linetype = \"solid\") +\n  geom_abline(intercept = sma_intercept, slope = sma_slope, \n              color = \"red\", size = 1, linetype = \"dashed\") +\n  geom_abline(intercept = ma_intercept, slope = ma_slope, \n              color = \"green\", size = 1, linetype = \"dotted\") +\n  geom_abline(intercept = rma_intercept, slope = rma_slope, \n              color = \"purple\", size = 1, linetype = \"dotdash\") +\n  geom_abline(intercept = 2, slope = 3, \n              color = \"black\", size = 1.5, alpha = 0.7) +\n  \n  # Add legend manually\n  annotate(\"text\", x = min(model_comparison_data$x), y = max(model_comparison_data$y), \n           label = paste(\"OLS (blue):\", round(ols_slope, 2),\n                        \"\\nSMA (red):\", round(sma_slope, 2),\n                        \"\\nMA (green):\", round(ma_slope, 2),\n                        \"\\nRMA (purple):\", round(rma_slope, 2),\n                        \"\\nTrue (black): 3.0\"), \n           hjust = 0, vjust = 1, size = 3) +\n  \n  labs(title = \"Comparison of Regression Methods\",\n       subtitle = \"When both X and Y have measurement error\",\n       x = \"X (with measurement error)\",\n       y = \"Y (with measurement error)\") +\n  theme_minimal()\n\ncomparison_plot\n```\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/model_comparison_plot-1.png)\n:::\n:::\n\n\n\n\n::: callout-important\n## Activity 3: Model I vs Model II Decision\n\nFor each scenario, choose the appropriate regression method:\n\n1.  **Predicting plant biomass from measured height** (height measured precisely, biomass is the response):\n    -   Your choice: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n    -   Reasoning: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Studying the relationship between two morphological measurements** (both have measurement error):\n    -   Your choice: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n    -   Reasoning: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Allometric scaling: log(brain size) vs log(body size)** (both measurements have error):\n    -   Your choice: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n    -   Reasoning: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Drug dose (controlled) vs response** (dose is fixed, response varies):\n    -   Your choice: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n    -   Reasoning: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n\n# **Part 5:** Real Example - Fish Allometric Relationships\n\n::: callout-note\n## Allometric Relationships: A Perfect Model II Example\n\n**Biological Context:** - Allometry studies how biological traits scale with size - Both length and mass measurements have error - Neither is clearly dependent/independent - Interested in the true scaling relationship\n\n**Expected Pattern:** - Mass typically scales with length\\^3 (cubic relationship) - log(mass) \\~ 3 × log(length) for isometric scaling - Deviations from 3.0 indicate allometric scaling\n:::\n\n## Analyzing Fish Length-Mass Relationships\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Transform data for allometric analysis\nfish_data <- fish_data %>%\n  mutate(\n    log_length = log10(length_mm),\n    log_mass = log10(mass_g)\n  )\n\n# Fit both Model I and Model II\nfish_ols <- lm(log_mass ~ log_length, data = fish_data)\nfish_sma <- sma(log_mass ~ log_length, data = fish_data, method = \"SMA\")\n\nprint(\"FISH ALLOMETRY RESULTS:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"FISH ALLOMETRY RESULTS:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"=======================\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"=======================\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Model I (OLS):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Model I (OLS):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fish_ols)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = log_mass ~ log_length, data = fish_data)\n\nResiduals:\n      Min        1Q    Median        3Q       Max \n-0.038187 -0.018503 -0.002393  0.022502  0.037462 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  -4.9390     0.1243  -39.74   <2e-16 ***\nlog_length    2.9722     0.0516   57.60   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.02338 on 38 degrees of freedom\nMultiple R-squared:  0.9887,\tAdjusted R-squared:  0.9884 \nF-statistic:  3318 on 1 and 38 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Model II (SMA):\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Model II (SMA):\"\n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(fish_sma)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCall: sma(formula = log_mass ~ log_length, data = fish_data, method = \"SMA\") \n\nFit using Standardized Major Axis \n\n------------------------------------------------------------\nCoefficients:\n            elevation    slope\nestimate    -4.979833 2.989162\nlower limit -5.231426 2.886526\nupper limit -4.728240 3.095447\n\nH0 : variables uncorrelated\nR-squared : 0.988676 \nP-value : < 2.22e-16 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Test if slope differs from 3.0 (isometric scaling)\nprint(\"\\nTEST FOR ISOMETRIC SCALING:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\\nTEST FOR ISOMETRIC SCALING:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"H0: slope = 3.0 (isometric)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"H0: slope = 3.0 (isometric)\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Extract slope and confidence intervals directly from summary output\nsma_slope <- 2.989162\nslope_ci_lower <- 2.886526\nslope_ci_upper <- 3.095447\n\nprint(paste(\"SMA slope estimate:\", round(sma_slope, 4)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"SMA slope estimate: 2.9892\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(paste(\"95% CI: [\", round(slope_ci_lower, 4), \",\", round(slope_ci_upper, 4), \"]\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"95% CI: [ 2.8865 , 3.0954 ]\"\n```\n\n\n:::\n\n```{.r .cell-code}\n# Simple test: Check if 3.0 falls within the confidence interval\nprint(paste(\"Test value 3.0 is between\", slope_ci_lower, \"and\", slope_ci_upper))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Test value 3.0 is between 2.886526 and 3.095447\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Result: Slope is NOT significantly different from 3.0 (isometric scaling)\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Result: Slope is NOT significantly different from 3.0 (isometric scaling)\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Interpretation: Cannot reject H0, scaling appears isometric\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Interpretation: Cannot reject H0, scaling appears isometric\"\n```\n\n\n:::\n:::\n\n\n\n\n## Visualizing Allometric Relationships\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create log-log plot with both regression lines\nfish_plot <- ggplot(fish_data, aes(x = log_length, y = log_mass)) +\n  geom_point(size = 3, alpha = 0.7) +\n  \n  # Add regression lines\n  geom_smooth(method = \"lm\", se = TRUE, color = \"blue\", fill = \"lightblue\", alpha = 0.3) +\n  geom_abline(intercept = coef(fish_sma)[1], slope = coef(fish_sma)[2], \n              color = \"red\", size = 1, linetype = \"dashed\") +\n  geom_abline(intercept = -5, slope = 3, color = \"black\", linetype = \"dotted\", size = 1) +\n  \n  # Add annotations\n  annotate(\"text\", x = min(fish_data$log_length), y = max(fish_data$log_mass),\n           label = paste(\"OLS slope:\", round(coef(fish_ols)[2], 2),\n                        \"\\nSMA slope:\", round(coef(fish_sma)[2], 2),\n                        \"\\nIsometric (3.0)\"),\n           hjust = 0, vjust = 1, size = 3.5) +\n  \n  labs(title = \"Fish Allometric Relationships\",\n       subtitle = \"log(mass) vs log(length) - Testing for isometric scaling\",\n       x = \"log10(Length in mm)\",\n       y = \"log10(Mass in g)\") +\n  theme_minimal()\n\nfish_plot\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/fish_allometry_plot-1.png)\n:::\n:::\n\n\n\n\n## Testing Assumptions for Model II\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# For Model II, we need to check assumptions differently\n# 1. Check linearity in log-log space\nfish_residuals <- residuals(fish_ols)  # Use OLS residuals as approximation\nfish_fitted <- fitted(fish_ols)\n\nlinearity_check <- ggplot(data.frame(fitted = fish_fitted, residuals = fish_residuals),\n                         aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Linearity Check for Fish Data\",\n       x = \"Fitted log(mass)\", y = \"Residuals\") +\n  theme_minimal()\n\n# 2. Check normality of residuals\nnormality_check <- ggplot(data.frame(residuals = fish_residuals),\n                         aes(sample = residuals)) +\n  stat_qq() + stat_qq_line(color = \"red\") +\n  labs(title = \"Normality Check\",\n       x = \"Theoretical Quantiles\", y = \"Sample Quantiles\") +\n  theme_minimal()\n\nlinearity_check + normality_check\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10_02_class_activity_files/figure-docx/fish_assumptions-1.png)\n:::\n\n```{.r .cell-code}\n# Formal tests\nprint(\"ASSUMPTION TESTS FOR FISH DATA:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"ASSUMPTION TESTS FOR FISH DATA:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Shapiro-Wilk test for normality:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Shapiro-Wilk test for normality:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(shapiro.test(fish_residuals))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  fish_residuals\nW = 0.93463, p-value = 0.02287\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(\"Breusch-Pagan test for homoscedasticity:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Breusch-Pagan test for homoscedasticity:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(bptest(fish_ols))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tstudentized Breusch-Pagan test\n\ndata:  fish_ols\nBP = 0.031182, df = 1, p-value = 0.8598\n```\n\n\n:::\n:::\n\n\n\n\n::: callout-important\n## Activity 4: Interpreting Allometric Results\n\nBased on the fish allometry analysis:\n\n1.  **Scaling Pattern**: Is the scaling isometric (slope = 3) or allometric? How do you know?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n2.  **Model Choice**: Which model (OLS or SMA) is more appropriate here and why?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n3.  **Biological Interpretation**: What does the slope value tell us about how fish mass scales with length?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n4.  **Statistical Significance**: Is the scaling relationship significantly different from isometric?\n    -   Your answer: \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}