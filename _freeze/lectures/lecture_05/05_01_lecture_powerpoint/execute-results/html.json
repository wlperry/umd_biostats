{
  "hash": "4c60f3bf30a7c9d57ad3e4cd17787245",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 04\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true # retains the images when you start again\n  message: false\n  warning: false\n  fig-height: 3\n  fig-width: 3\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"054_01_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch  # Limits line length to approximately 80 characters\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"05_01_lecture_powerpoint_slides.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# **Lecture 5: Review**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Introduction to histograms or frequency distributions\n-   Probability Distribution Functions (PDF)\n-   Descriptive Statistics\n    -   Center - mean, median, mode\n\n    -   Spread - range, variance, standard deviation\n:::\n\n::: {.column width=\"40%\"}\nOur last graphs\\\n![](images/clipboard-536528302.png){width=\"197\"}\n\n![](images/clipboard-3257239263.png){width=\"446\"}\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 1: Recreating Our Last Histograms\n\nLet's recreate the basic histogram of fish lengths from our last class. Use the `sculpin_df` data frame that's already loaded.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Write your code here to create a histogram of fish lengths from Toolik Lake\n# Remember to use the pipe operator %>% and ggplot with geom_histogram()\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Lecture Overview\n\n::::: columns\n::: {.column width=\"60%\"}\n## The objectives:\n\n-   Introduction to hypothesis testing\n-   The standard normal distribution\n-   Standard error\n-   Confidence intervals\n-   Student's t-distribution\n-   H testing sequence\n-   p-values\n:::\n\n::: {.column width=\"40%\"}\n![](images/slimy_sculpin_cottus_cognatus.jpg){width=\"371\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nTo understand hypothesis testing need to understand *standard normal distribution*\n\nRecall - sculpin in Toolik Lake\n\n-   n = 208\n\n-   mean = 51.69 mm\n\n-   std dev = s = 12.02 mm\n\n-   Weight distribution \\~normal\n\n-   Look at the class activity on how to calculate descriptive statistics\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-3-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 2: Compare Fish Distributions from Different Lakes\n\nLet's look at what lakes are in our dataframe:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# View the unique lake names\nunique(sculpin_df$lake)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"E 01\"   \"E 05\"   \"NE 12\"  \"NE 14\"  \"S 06\"   \"S 07\"   \"Toolik\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, select two lakes and create a comparison of their fish length distributions using facet_grid():\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Your code here to compare fish lengths between two lakes of your choice\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nYou want to know things about this population like\n\n-   probability of a baby born at the hospital having a certain length (e.g., \\> 60 mm)\n-   Can solve this by integrating under curve\n-   But it is tedious to do every time\n-   Instead\n    -   we can use the *standard normal distribution* (SND)\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-6-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nStandard Normal Distribution\n\n-   \"benchmark\" normal distribution with Âµ = 0, Ïƒ = 1\n-   The Standard Normal Distribution is defined so that:\n    -   \\~68% of the curve area within +/- 1 Ïƒ of the mean,\n\n    -   \\~95% within +/- 2 Ïƒ of the mean,\n\n    -   \\~99.7% within +/- 3 Ïƒ of the mean\n\n\\*remember Ïƒ = standard deviation\\\n\\\n\nSee the Class Activity on how to calculate Z scores\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-7-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nAreas under curve of Standard Normal Distribution\n\n-   Have been calculated for a range of sample sizes\n-   Can be looked up in z-table\n-   No need to integrate\n-   Any normally distributed data can be standardized\n    -   transformed into the standard normal distribution\n    -   looked up ion a table\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-8-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nDone by converting original data points to z-scores\n\n-   Z-scores calculated as:\n\n## $\\text{Z = }\\frac{X_i-\\mu}{\\sigma}$\n\n-   z = z-score for observation\n-   xi = original observation\n-   Âµ = mean of data distribution\n-   Ïƒ = SD of data distribution\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-9-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 3: Calculate Z-Scores\n\nLet's practice calculating z-scores for fish lengths. Calculate the z-scores for:\n\n1.  A fish that is 25 mm long\n2.  A fish that is at the mean length (approximately 51.7 mm)\n3.  A fish that is 60 mm long\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate z-scores for these three fish lengths\n# Remember the formula: Z = (value - mean) / standard deviation\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow use your calculated z-scores to determine roughly where these fish fall in the overall distribution: - What percentage of fish are smaller than the 25 mm fish? - What percentage of fish are larger than the 60 mm fish?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use the pnorm() function to calculate these percentages\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nThus:\n\n-   z-score = value - mean/s\n-   z-score of 25mm = (25 - 51.7) / 12 = -2.225\n-   z-score of 51.7mm = (51.7 - 51.7) / 12 = 0\n-   z-score of 60mm = (60 - 51.7) / 12 = 0.6916667\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-12-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nArea under curve (probability) of standard normal distribution is known relative to z-values\n\nKnowing z-value, can figure out corresponding area under the curve\n\n*What is the area under curve \\< 0?*\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-13-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Here is z-score table for **right** side or **positive values** of the z distribution **(z \\> 0)**\n\n-   Numbers give area under the curve to **left** of a particular z-score\n\n-   say 60 mm as a z score of 0.6916667\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-14-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n![](images/clipboard-1995791111.png){width=\"332\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nArea under curve (probability) of standard normal distribution is known relative to z-values\n\nKnowing z-value, can figure out corresponding area under the curve\n\n*What is the area under curve \\< 0?*\n\n-   0.5 of the area of the curve is contained to the left of z = 0.00\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-264891377.png){width=\"381\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\narea of the curve is contained to the left of z = 1.22\n\n-   0.8686 or 86.9%\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1995791111.png){width=\"449\"}\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 4: Working with Probabilities\n\nUsing the z-table or the built-in R function `pnorm()`, answer these questions:\n\n1.  What percentage of fish in Toolik Lake are expected to be longer than 65 mm?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Convert 65 mm to a z-score\nz_65mm <- (65 - toolik_result$mean) / toolik_result$sd\n\n# Find the probability of a fish being larger than 65 mm\nprob_larger_than_65mm <- 1 - pnorm(z_65mm)\nprob_larger_than_65mm * 100  # Convert to percentage\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 13.4254\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.  Between what two lengths would you expect to find the middle 90% of fish in Toolik Lake?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Find the z-scores for the 5th and 95th percentiles\nz_5 <- qnorm(0.05)\nz_95 <- qnorm(0.95)\n\n# Convert these z-scores back to fish lengths\nlength_5 <- toolik_result$mean + z_5 * toolik_result$sd\nlength_95 <- toolik_result$mean + z_95 * toolik_result$sd\n\nc(length_5, length_95)  # The middle 90% of fish lengths\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 31.91600 71.47343\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nWhat is the area of the curve is contained between of z = 0 and z=1.5?\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1995791111.png){width=\"380\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nWhat is the area of the curve is contained between of z = 0 and z=1.5?\n\n-   approximately 0.4332 (or 43.32%)\n\n*To calculate this from a standard normal table:*\n\nTo find the area under the standard normal curve between 0 and 1.5 using this table:\n\n-   Locate z = 1.5 in the table - 0.9332.\n    -   represents P(Z â‰¤ 1.5) - probability Z is less than or equal to 1.5\n-   Since need area between 0 and 1.5 - need to subtract P(Z â‰¤ 0) from P(Z â‰¤ 1.5)\n-   From table - P(Z â‰¤ 0) = 0.5000.\n-   Therefore, the area between 0 and 1.5 is: 0.9332 - 0.5000 = 0.4332.\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1995791111.png){width=\"404\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nWhat is the area of the curve is contained to the left of z = -1?\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1995791111.png){width=\"381\"}\n:::\n:::::\n\n# **Lecture 5:** Standard normal distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nWhat is the area of the curve is contained to the left of z = -1?\n\n-   Locate row for 1.0 - (table shows absolute value of z) and the column for .00\n    -   value = 0.8413 - represents P(Z â‰¤ 1.0)\n-   However want P(Z â‰¤ -1.0)\n    -   need to use the symmetry property of the standard normal distribution:\n\n    -   P(Z â‰¤ -1.0) = 1 - P(Z â‰¤ 1.0) = 1 - 0.8413 = 0.1587\n\nTherefore, 15.87% of area falls to the left of z = -1.0\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1995791111.png){width=\"380\"}\n:::\n:::::\n\n# **Lecture 5:** Standard error\n\nTake random samples from fish population:\n\n-   3 random samples (each n=20) from population:\n\n-   Notice the sample statistics and distributions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-17-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: callout-tip\n## Practice Exercise 5: Sampling Distributions\n\nLet's explore how sample size affects our estimates by taking samples of different sizes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(456)\n\n# Create samples of different sizes\nsmall_sample <- toolik_df %>% sample_n(10)\nmedium_sample <- toolik_df %>% sample_n(30)\nlarge_sample <- toolik_df %>% sample_n(100)\n\n# Calculate mean and standard error for each sample\nsmall_mean <- mean(small_sample$total_length_mm, na.rm = TRUE)\nsmall_se <- sd(small_sample$total_length_mm, na.rm = TRUE) / sqrt(10)\n\nmedium_mean <- mean(medium_sample$total_length_mm, na.rm = TRUE)\nmedium_se <- sd(medium_sample$total_length_mm, na.rm = TRUE) / sqrt(30)\n\nlarge_mean <- mean(large_sample$total_length_mm, na.rm = TRUE)\nlarge_se <- sd(large_sample$total_length_mm, na.rm = TRUE) / sqrt(100)\n\n# Create a data frame with the results\nresults <- data.frame(\n  Sample_Size = c(10, 30, 100),\n  Mean = c(small_mean, medium_mean, large_mean),\n  SE = c(small_se, medium_se, large_se)\n)\n\n# Display the results\nresults\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Sample_Size     Mean       SE\n1          10 58.50000 2.260531\n2          30 51.27083 1.656559\n3         100 52.22973 1.062795\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you observe about the standard error as sample size increases? Why does this happen?\n:::\n\n# **Lecture 5:** Standard error\n\n::::: columns\n::: {.column width=\"60%\"}\nEvery sample gives slightly different estimate of Âµ\n\n-   Can take many samples and calculate means\n-   plot the frequency distribution of means\n-   get the \"sampling distribution of means\"\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-19-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard error\n\n::::: columns\n::: {.column width=\"60%\"}\n## 3 important properties:\n\n-   Sampling distribution of means (SDM) from normal population will be normal\n-   Large Sampling distribution of means from any population will be normal (Central Limit Theorem)\n-   The mean of Sampling distribution of means will equal Âµ or the mean\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-20-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard error\n\n::::: columns\n::: {.column width=\"60%\"}\nGiven above\n\n-   can estimate the standard deviation of sample means\n\n-   \"Standard error of sample mean\"\n\n-   How good is your estimate of population mean? (based on the sample collected)\n\n-   quantifies how much the sample means are expected to vary from samples\n\n-   gives an estimate of the error associated with using $\\bar{y}$ to estimate $\\mu$...\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-21-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard error\n\n::::: columns\n::: {.column width=\"60%\"}\n$\\sigma_{\\bar{y}} = \\frac{\\sigma}{\\sqrt{n}}$\n\nbut rarely know Ïƒ, so use s $s_{\\bar{y}} = \\frac{s}{\\sqrt{n}}$ Where: $s_{\\bar{y}}$ = sample standard error of mean s = sample standard deviation n = sample size\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-22-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Standard error\n\n::::: columns\n::: {.column width=\"60%\"}\nNotice: - $s_{\\bar{y}}$ depends on - sample s (standard deviation) - sample n - ($s_{\\bar{y}} = \\frac{s}{\\sqrt{n}}$)\n\nHow and why? - Decreases with sample n - number - increases with sample s - standard deviation\n\n-   Large sample, low s = greater confidence in estimate of $\\mu$\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-23-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Confidence intervals\n\n::::: columns\n::: {.column width=\"60%\"}\nEvery sample gives slightly different estimate of Âµ (population mean)\n\nWant to know how accurate our estimate of Âµ is from a sample\n\nDo this by calculating confidence interval:\n\n-   Range of values that will contain the true population mean with a certain probability\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-24-1.png){width=384}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 6: Calculating Confidence Intervals\n\nLet's calculate 95% confidence intervals for the mean fish length in Toolik Lake:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the standard error for Toolik Lake\ntoolik_se <- toolik_result$sd / sqrt(toolik_result$count)\n\n# Calculate the 95% confidence interval using the normal distribution\n# (since our sample size is large)\ntoolik_ci_lower <- toolik_result$mean - 1.96 * toolik_se\ntoolik_ci_upper <- toolik_result$mean + 1.96 * toolik_se\n\n# Print the results\ncat(\"Mean fish length in Toolik Lake:\", round(toolik_result$mean, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean fish length in Toolik Lake: 51.7 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% Confidence Interval:\", round(toolik_ci_lower, 1), \"to\", round(toolik_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% Confidence Interval: 50.1 to 53.3 mm\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow choose another lake and calculate its 95% confidence interval:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose another lake (e.g., \"E 01\")\nmy_lake <- \"E 01\"  # You can change this to any lake in the dataframe\n\n# Filter data for your chosen lake\nmy_lake_data <- sculpin_df %>% filter(lake == my_lake)\n\n# Calculate mean, standard deviation, and standard error\nmy_lake_stats <- my_lake_data %>%\n  summarize(\n    mean = mean(total_length_mm, na.rm = TRUE),\n    sd = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se = sd / sqrt(n)\n  )\n\n# Calculate 95% confidence interval\nmy_lake_ci_lower <- my_lake_stats$mean - 1.96 * my_lake_stats$se\nmy_lake_ci_upper <- my_lake_stats$mean + 1.96 * my_lake_stats$se\n\n# Print results\ncat(\"Mean fish length in\", my_lake, \":\", round(my_lake_stats$mean, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean fish length in E 01 : 58.2 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% Confidence Interval:\", round(my_lake_ci_lower, 1), \"to\", round(my_lake_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% Confidence Interval: 54.8 to 61.6 mm\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDo the confidence intervals for these two lakes overlap? What does this suggest about the difference between the fish populations?\n:::\n\n# **Lecture 5:** Confidence intervals\n\n::::: columns\n::: {.column width=\"60%\"}\nOften calculate 95% CIs\n\n-   Interpret 95% CI to mean:\n    -   Range of values that contains Âµ (population mean) with 95% probability\n-   More correctly:\n    -   If we took 100 samples from population\n    -   calculate a CI from each\n    -   95 of the 100 CIs will contain the true population mean - Âµ\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2045535544.png){width=\"347\"}\n:::\n:::::\n\n# **Lecture 5:** Confidence intervals\n\n::::: columns\n::: {.column width=\"60%\"}\nFormula for confidence interval\n\n## $\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}$\n\nWhere:\n\n-   È³ is the sample mean\n-   ð‘› is the sample size\n-   Ïƒ is the population standard deviation\n-   z is the z-value corresponding the probability of the CI\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2045535544.png){width=\"383\"}\n:::\n:::::\n\n# **Lecture 5:** Confidence intervals\n\n::::: columns\n::: {.column width=\"60%\"}\nFormula for confidence interval\n\n$\\text{95% CI} = \\bar{y} \\pm z \\cdot \\frac{\\sigma}{\\sqrt{n}}$\n\n95% of probability of SND is bw z= -1.96 and z=1.96\n\nSo for:\n\n-   95% CI z = 1.960\n-   90% CI z = 1.645\n-   99% CI z = 2.576\n-   And so on....\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2045535544.png){width=\"381\"}\n:::\n:::::\n\n# **Lecture 5:** Confidence intervals\n\n:::: columns\n::: {.column width=\"60%\"}\nIn the more typical case DON'T know the population Ïƒ\n\n-   estimate it from the samples when don't know the population Ïƒ\n-   and when sample size is \\<\\~30)\n-   can't use the standard normal (z) distribution\n\n*Instead, we use Student's t distribution*\n:::\n\n![](images/clipboard-1052237789.png){width=\"375\"}\n::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nStudent's t distribution similar to SND\n\n-   changes depending on degrees of freedom (df= n-1)\n-   t distribution more \"conservative\"\n    -   smaller n is, the more conservative the t distribution is\n\nAt df = \\~30 - t distribution becomes close to z distribution\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2481080685.png){width=\"362\"}\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 7: Using the t-Distribution\n\nWhen working with small samples or when the population standard deviation is unknown\n\n-   use the t-distribution\n\n-   take a small sample\n\n-   calculate a confidence interval using the t-distribution:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Set seed for reproducibility\nset.seed(789)\n\n# Take a small sample of 15 fish from Toolik Lake\nsmall_sample <- toolik_df %>% sample_n(15)\n\n# Calculate sample statistics\nsmall_mean <- mean(small_sample$total_length_mm, na.rm = TRUE)\nsmall_sd <- sd(small_sample$total_length_mm, na.rm = TRUE)\nsmall_n <- 15\nsmall_se <- small_sd / sqrt(small_n)\n\n# Calculate degrees of freedom\ndf <- small_n - 1\n\n# Find the critical t-value for 95% confidence interval\nt_critical <- qt(0.975, df)  # 0.975 for a two-tailed 95% CI\n\n# Calculate the confidence interval\nsmall_ci_lower <- small_mean - t_critical * small_se\nsmall_ci_upper <- small_mean + t_critical * small_se\n\n# Print the results\ncat(\"Small sample (n=15) mean:\", round(small_mean, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSmall sample (n=15) mean: 57.3 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"t-critical value (df=14):\", round(t_critical, 3), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nt-critical value (df=14): 2.145 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI using t-distribution:\", round(small_ci_lower, 1), \"to\", round(small_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI using t-distribution: 52.7 to 61.9 mm\n```\n\n\n:::\n\n```{.r .cell-code}\n# For comparison, calculate the CI using the normal distribution (z)\nz_critical <- 1.96\nz_ci_lower <- small_mean - z_critical * small_se\nz_ci_upper <- small_mean + z_critical * small_se\n\ncat(\"95% CI using normal distribution:\", round(z_ci_lower, 1), \"to\", round(z_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI using normal distribution: 53.1 to 61.5 mm\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n-   Which confidence interval is wider?\n\n-   Why is this the case?\n:::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nTo calculate CI for sample from \"unknown\" population:\n\n$\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}$\n\nWhere:\n\n-   È³ is sample mean\n-   ð‘› is sample size\n-   s is sample standard deviation\n-   t t-value corresponding the probability of the CI\n-   t in t-table for different degrees of freedom (n-1)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3203878802.png){width=\"423\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nHere is a t-table\n\n-   Values of t that correspond to probabilities\n-   Probabilities listed along top\n-   Sample dfs are listed in the left-most column\n-   Probabilities are given for one-tailed and two-tailed \"questions\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3203878802.png){width=\"436\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nOne-tailed questions: area of distribution left or (right) of a certain value\n\n-   n=20 (df=19) - 90% of the observations found left\n-   t= 1.328 (10% are outside)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1822465473.png){width=\"225\"}\n\n![](images/clipboard-641796945.png){width=\"365\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nTwo-tailed questions refer to area between certain values\n\n-   n= 20 (df=19), 90% of the observations are between\n-   t=-1.729 and t=1.729 (10% are outside)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2234740159.png){width=\"271\"}\n\n![](images/clipboard-1734806681.png){width=\"549\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nLet's calculate CIs again:\n\nUse two-sided test\n\n-   95% CI Sample A: = 51.7 Â± 1.984 \\* (12/(208\\^0.5)) = 1.650788\n-   The 95% CI is between 50.05 and 53.35\n-   \"The 95% CI for the population mean from sample A is 51.7 Â± 1.65\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-721472248.png){width=\"563\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\nSo:\n\n-   Can assess confidence that population mean is within a certain range\n-   Can use t distribution to ask questions like:\n    -   \"What is probability of getting sample with mean = È³ from population with mean = Âµ?\" (1 sample t-test)\n    -   \"What is the probability that two samples came from same population?\" (2 sample t-test)\n\n::: callout-tip\n## Practice Exercise 8: One-Sample t-Test\n\nLet's perform a one-sample t-test to determine if the mean fish length in Toolik Lake differs from 50 mm:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Perform a one-sample t-test\nt_test_result <- t.test(toolik_df$total_length_mm, mu = 50)\n\n# View the test results\nt_test_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  toolik_df$total_length_mm\nt = 2.0326, df = 207, p-value = 0.04337\nalternative hypothesis: true mean is not equal to 50\n95 percent confidence interval:\n 50.05097 53.33845\nsample estimates:\nmean of x \n 51.69471 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpret this test result by answering these questions: 1. What was the null hypothesis? 2. What was the alternative hypothesis? 3. What does the p-value tell us? 4. Should we reject or fail to reject the null hypothesis at Î± = 0.05? 5. What is the practical interpretation of this result for fish biologists?\n:::\n\n# **Lecture 5:** Next steps\n\nFor example\n\n-   what is probability that population X is the same as our lakes population?\n\nHow would you assess this question using what we learned?\n\n::: callout-tip\n## Practice Exercise 9: Two-Sample t-Test\n\nLet's compare fish lengths between two lakes to see if they differ:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose two lakes to compare\nlake1 <- \"Toolik\"\nlake2 <- \"E 01\"  # Change this to another lake if you prefer\n\n# Filter data for the two lakes\nlake1_data <- sculpin_df %>% \n  filter(lake == lake1) %>% \n  pull(total_length_mm)\n\nlake2_data <- sculpin_df %>% \n  filter(lake == lake2) %>% \n  pull(total_length_mm)\n\n# Perform a two-sample t-test\nlakes_ttest <- t.test(lake1_data, lake2_data)\n\n# View the results\nlakes_ttest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  lake1_data and lake2_data\nt = -3.4051, df = 116.36, p-value = 0.0009082\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -10.313036  -2.727921\nsample estimates:\nmean of x mean of y \n 51.69471  58.21519 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow create a boxplot to visualize the difference in fish lengths between these lakes:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a boxplot comparing the two lakes\nsculpin_df %>%\n  filter(lake %in% c(lake1, lake2)) %>%\n  ggplot(aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(\n    title = paste(\"Comparison of Fish Lengths in\", lake1, \"and\", lake2),\n    x = \"Lake\",\n    y = \"Length (mm)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-30-1.png){width=288}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on the t-test results and the boxplot\n\n-   what can you conclude about the fish populations in these two lakes?\n:::\n\n# **Lecture 5:** Next steps\n\n::::: columns\n::: {.column width=\"60%\"}\nLet's calculate the 95% CI for population X\n\nUse two-sided test\n\n95% CI Sample X: = 54 Â± 1.984 \\* (10.9/(132\\^0.5)) = 1.882267 The 95% CI is between 52.12 and 55.88\n\nNotice: the 95% confidence interval contains 51.7\n\n-   What does this tell us about population X?\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-31-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nMajor goal of statistics:\n\ninferences about populations from samples assign degree of confidence to inferences\n\nStatistical H-testing:\n\nformalized approach to inference\n\n-   hypotheses ask whether samples come from populations with certain properties\n-   often interested in questions about population means (but not only)\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-32-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nRelies on specifying null hypothesis (Ho) and alternate hypothesis (Ha)\n\n-   Ho is the hypothesis of \"no effect\"\n    -   (two samples from population with same mean, sample is from population of mean=0)\n-   Ha (research hypothesis) the opposite of the Ho\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-33-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 10: Formulating Hypotheses\n\nFor the following scenarios, write out the null and alternative hypotheses:\n\n1.  Testing if the mean fish length in Lake S 06 is greater than 50 mm.\n\n2.  Testing if there is a difference in mean fish lengths between lakes Toolik and S 06.\n\n3.  Testing if lake E 01 has a higher variance in fish lengths compared to Lake Toolik.\n\nFor each scenario, remember that the null hypothesis typically represents \"no effect\" or \"no difference\", while the alternative hypothesis represents what you are trying to demonstrate.\n:::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   p = 0.3 means that if study repeated 100 times\n    -   would get this (or more extreme) result due to chance 30 times\n-   p = 0.03 means that if study repeated 100 times\n    -   would get this (or more extreme) result due to chance 3 times\n\nWhich p-value suggests Ho likely false?\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-34-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nAt what point reject Ho?\n\n-   p \\< 0.05 conventional \"significance threshold\" (Î±)\n\n-   p \\< 0.05 means:\n\n    -   if Ho is true - if study repeated 100 times\n        -   would get this (or more extreme) result less than 5 times due to chance\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-35-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Î± is the rate at which we will reject a true null hypothesis (Type I error rate)\n\n-   Lowering Î± will lower likelihood of incorrectly rejecting a true null hypothesis (e.g., 0.01, 0.001)\n\n-   Both hypotheses and Î± are specified *BEFORE* collection of data and analysis\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-36-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nTraditionally Î±=0.05 is used as a cut off for rejecting null hypothesis\n\nNothing magical about 0.0 - actual p-values need to be reported.\n\n| p-value range | Interpretation |\n|----|----|\n| P \\> 0.10 | No evidence against Ho - data appear consistent with Ho |\n| 0.05 \\< P \\< 0.10 | Weak evidence against the Ho in favor of Ha |\n| 0.01 \\< P \\< 0.05 | Moderate evidence against Ho in favor of Ha |\n| 0.001 \\< P \\< 0.01 | Strong evidence against Ho in favor of Ha |\n| P \\< 0.001 | Very strong evidence against Ho in favor of Ha |\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-37-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nFisher:\n\np-value as informal measure of discrepancy betwen data and Ho\n\n\"If p is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it is below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 â€¦\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-694363384.png){width=\"372\"}s\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nGeneral procedure for H testing:\n\n-   Specify Null (Ho) and alternate (Ha)\n-   Determine test (and test statistic) to be used\n-   Test statistic is used to compare your data to expectation under Ho (null hypothesis)\n-   Specify significance (Î± or p value) level below which Ho will be rejected\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-38-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nGeneral procedure for H testing:\n\n-   Collect data - Perform test\n-   If p-value \\< Î±, conclude Ho is likely false and reject it\n-   If p-value \\> Î±, conclude no evidence Ho is false and retain it\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-39-1.png){width=480}\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Final Exercise: Comprehensive Analysis\n\nYou've learned about standard normal distributions, z-scores, standard error, confidence intervals, and hypothesis testing. Now, put it all together with a comprehensive analysis of fish lengths from multiple lakes.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose 3 lakes for comparison\nlakes_to_compare <- c(\"Toolik\", \"E 01\", \"S 06\")\n\n# Filter data for these lakes\ncomparison_data <- sculpin_df %>%\n  filter(lake %in% lakes_to_compare) %>%\n  filter(!is.na(total_length_mm))\n\n# 1. Calculate summary statistics for each lake\nlake_stats <- comparison_data %>%\n  group_by(lake) %>%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n),\n    ci_lower = mean_length - qt(0.975, n-1) * se_length,\n    ci_upper = mean_length + qt(0.975, n-1) * se_length\n  )\n\n# Display the summary statistics\nlake_stats\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 7\n  lake   mean_length sd_length     n se_length ci_lower ci_upper\n  <chr>        <dbl>     <dbl> <int>     <dbl>    <dbl>    <dbl>\n1 E 01          58.2      15.3    79     1.72      54.8     61.6\n2 S 06          54.0      10.9   132     0.949     52.1     55.9\n3 Toolik        51.7      12.0   208     0.834     50.1     53.3\n```\n\n\n:::\n\n```{.r .cell-code}\n# 2. Create boxplots to visualize the distributions\ncomparison_data %>%\n  ggplot(aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(\n    title = \"Fish Length Distributions Across Lakes\",\n    x = \"Lake\",\n    y = \"Length (mm)\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-html/unnamed-chunk-40-1.png){width=288}\n:::\n\n```{.r .cell-code}\n# 3. Perform t-test comparing each lake to the other - this is NOT what you should do and we will show how later on but its practice\n```\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBased on this analysis, write a short summary of your findings: 1. Are there significant differences in fish lengths between the lakes? 2. Which lakes have the largest and smallest fish on average? 3. How do the confidence intervals compare across lakes? 4. What might explain these differences in fish lengths between lakes?\n:::\n\n# \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}