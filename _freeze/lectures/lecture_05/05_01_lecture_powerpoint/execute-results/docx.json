{
  "hash": "f3495ceddd415b9a958b37687929191e",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 05: Probability and Statistical Inference\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  keep-md: true # retains the images when you start again\n  message: false\n  warning: false\n  fig-height: 3\n  fig-width: 3\nformat:\n  html:\n    toc: false\n    output-file: \"05_01_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch  # Limits line length to approximately 80 characters\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"05_01_lecture_powerpoint_slidesl.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n# **Lecture 4: Review**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Introduction to histograms or frequency distributions\n-   Probability Distribution Functions (PDF)\n-   Descriptive Statistics\n    -   Center - mean, median, mode\n\n    -   Spread - range, variance, standard deviation\n:::\n\n::: {.column width=\"40%\"}\n\\\n![](images/clipboard-536528302.png){width=\"197\"}\n:::\n:::::\n\n## **Lecture 4: Review**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Introduction to histograms or frequency distributions\n-   Probability Distribution Functions (PDF)\n-   Descriptive Statistics\n    -   Center - mean, median, mode\n\n    -   Spread - range, variance, standard deviation\n:::\n\n::: {.column width=\"40%\"}\n\\\n\n![](images/clipboard-3257239263.png){width=\"446\"}\n:::\n:::::\n\n## **Lecture 4: Review**\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Introduction to histograms or frequency distributions\n-   Probability Distribution Functions (PDF)\n-   Descriptive Statistics\n    -   Center - mean, median, mode\n\n    -   Spread - range, variance, standard deviation\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(car)          # For diagnostic tests\n\ngrayling_df <- read_csv(\"data/gray_I3_I8.csv\") \ni3_df  <- grayling_df %>% filter(lake==\"I3\")\n\n# Calculate summary statistics\ngrayling_summary <- grayling_df %>% \n  group_by(lake) %>%\n  summarize(\n    mean_length = mean(length_mm, na.rm = TRUE),\n    sd_length = sd(length_mm, na.rm = TRUE),\n    se_length = sd_length/sqrt(sum(!is.na(length_mm))),\n    count = sum(!is.na(length_mm)),\n    .groups = \"drop\")\ngrayling_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  lake  mean_length sd_length se_length count\n  <chr>       <dbl>     <dbl>     <dbl> <int>\n1 I3           266.      28.3      3.48    66\n2 I8           363.      52.3      5.18   102\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n## **Lecture 5: Probability and Statistical Inference**\n\n::::: columns\n::: {.column width=\"60%\"}\nThe goals for today\n\n-   Statistical inference fundamentals\n-   Hypothesis testing principles\n-   T Distributions\n-   One sample T Tests\n-   Two sample T Test\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-2-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Confidence intervals\n\n:::: columns\n::: {.column width=\"60%\"}\nIn the more typical case DON'T know the population σ or standard deviation\n\n-   estimate it from the samples\n-   and when sample size is \\<\\~30)\n-   can't use the standard normal (z) distribution\n\n*Instead, we use Student's t distribution*\n:::\n\n![](images/clipboard-1052237789.png){width=\"375\"}\n::::\n\n# **Lecture 5:** Understanding t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nWhen sample sizes are small, the **t-distribution** is more appropriate than the normal distribution.\n\n-   Similar to normal distribution but with heavier tails\n-   Shape depends on **degrees of freedom** (df = n-1)\n-   With large df (\\>30), approaches the normal distribution\n-   Used for:\n    -   Small sample sizes\n\n    -   When population standard deviation is unknown\n\n    -   Calculating confidence intervals\n\n    -   Conducting t-tests\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-3-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Understanding t-distribution\n\n::::: columns\n::: {.column width=\"50%\"}\nWhen sample sizes are small, the **t-distribution** is more appropriate than the normal distribution.\n\n-   Similar to normal distribution (1.96 = 2.5% tails) but with heavier tails\n-   Shape depends on **degrees of freedom** (df = n-1)\n-   With large df (\\>30), approaches the normal distribution\n-   Used for:\n    -   Small sample sizes\n\n    -   When population standard deviation is unknown\n\n    -   Calculating confidence intervals\n\n    -   Conducting t-tests\n:::\n\n::: {.column width=\"50%\"}\n![](images/clipboard-3203878802.png)\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 4: Using the t-distribution\n\nLet's compare confidence intervals using the normal approximation (z) versus the t-distribution for our fish data.\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate CI using both z and t distributions for a smaller subset\nsmall_sample <- grayling_df %>% \n  filter(lake == \"I3\") %>% \n  slice_sample(n = 10)\n\n# Calculate statistics\nsample_mean <- mean(small_sample$length_mm)\nsample_sd <- sd(small_sample$length_mm)\nsample_n <- nrow(small_sample)\nsample_se <- sample_sd / sqrt(sample_n)\n\n# Calculate confidence intervals\nz_ci_lower <- sample_mean - 1.96 * sample_se\nz_ci_upper <- sample_mean + 1.96 * sample_se\n\n# For t-distribution, get critical value for 95% CI with df = n-1\nt_crit <- qt(0.975, df = sample_n - 1)\nt_ci_lower <- sample_mean - t_crit * sample_se\nt_ci_upper <- sample_mean + t_crit * sample_se\n\n# Display results\ncat(\"Mean:\", round(sample_mean, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean: 267.5 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standard deviation:\", round(sample_sd, 2), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard deviation: 30.87 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Standard error:\", round(sample_se, 2), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nStandard error: 9.76 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI using z:\", round(z_ci_lower, 1), \"to\", round(z_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI using z: 248.4 to 286.6 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"95% CI using t:\", round(t_ci_lower, 1), \"to\", round(t_ci_upper, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n95% CI using t: 245.4 to 289.6 mm\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"t critical value:\", round(t_crit, 3), \"vs z critical value: 1.96\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nt critical value: 2.262 vs z critical value: 1.96\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n# Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nTo calculate CI for sample from \"unknown\" population:\n\n## $\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}$\n\nWhere:\n\n-   ȳ is sample mean\n-   𝑛 is sample size\n-   s is sample standard deviation\n-   t t-value corresponding the probability of the CI\n-   t in t-table for different degrees of freedom (n-1)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3203878802.png){width=\"423\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nHere is a t-table\n\n-   Values of t that correspond to probabilities\n-   Probabilities listed along top\n-   Sample dfs are listed in the left-most column\n-   Probabilities are given for one-tailed and two-tailed \"questions\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3203878802.png){width=\"436\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nOne-tailed questions: area of distribution left or (right) of a certain value\n\n-   n=20 (df=19) - 90% of the observations found left\n-   t= 1.328 (10% are outside)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1822465473.png){width=\"225\"}\n\n![](images/clipboard-641796945.png){width=\"365\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nTwo-tailed questions refer to area between certain values\n\n-   n= 20 (df=19), 90% of the observations are between\n-   t=-1.729 and t=1.729 (10% are outside)\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2234740159.png){width=\"271\"}\n\n![](images/clipboard-1734806681.png){width=\"549\"}\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\n::::: columns\n::: {.column width=\"60%\"}\nLet's calculate CIs again:\n\nUse two-sided test\n\n-   $\\text{CI} = \\bar{y} \\pm t \\cdot \\frac{s}{\\sqrt{n}}$\n-   95% CI Sample A: = 272.8 ± 2.262 \\* (37.81/(9\\^0.5)) = 1.650788\n-   The 95% CI is between 244.3 and 301.3\n-   \"The 95% CI for the population mean from sample A is 272.8 ± 28.5\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3203878802.png)\n:::\n:::::\n\n# **Lecture 5:** Student's t-distribution\n\nSo:\n\n-   Can assess confidence that population mean is within a certain range\n-   Can use t distribution to ask questions like:\n    -   \"What is probability of getting sample with mean = ȳ from population with mean = µ?\" (1 sample t-test)\n    -   \"What is the probability that two samples came from same population?\" (2 sample t-test)\n\n# **Lecture 5:** Single Sample T-Test\n\n::::: columns\n::: {.column width=\"60%\"}\nWe want to test if the mean fish length in I3 differs from 240mm.\n\n**Activity: Define hypotheses and identify assumptions**\n\nH₀: μ = 240 (The mean fish length in I3 is 240mm)\n\nH₁: μ ≠ 240 (The mean fish length in I3 is not 55mm)\n:::\n\n::: {.column width=\"40%\"}\n## Assumptions for t-test:\n\n1.  Data is normally distributed\n2.  Observations are independent\n3.  No significant outliers\n:::\n:::::\n\n# Assumptions in R - qqplots from car\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# Filter for just windward side needles\n\n# YOUR TASK: Test normality of windward pine needle lengths\n# QQ Plot\nqqPlot(i3_df$length_mm, \n       main = \"QQ Plot for length of Grayling\",\n       ylab = \"Sample Quantiles\")\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/test_assumptions-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 53 35\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Statistical Test of Normality - Shapiro-Wilk test\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test\nshapiro_test <- shapiro.test(i3_df$length_mm)\nprint(shapiro_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  i3_df$length_mm\nW = 0.91051, p-value = 0.0001623\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for outliers using boxplot\n# YOUR CODE HERE\ni3_df %>% ggplot(aes(lake, length_mm))+geom_boxplot()\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-6-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Now for Practice\n\n::: callout-tip\n## Practice Exercise 1: One-Sample t-Test\n\nLet's perform a one-sample t-test to determine if the mean fish length in I3 Lake differs from 240 mm:\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# what is the mean\ni3_mean <- mean(i3_df$length_mm, na.rm=TRUE)\ncat(\"Mean:\", round(i3_mean, 1), \"mm\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMean: 265.6 mm\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform a one-sample t-test\nt_test_result <- t.test(i3_df$length_mm, mu = 240)\n\n# View the test results\nt_test_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  i3_df$length_mm\nt = 7.3497, df = 65, p-value = 4.17e-10\nalternative hypothesis: true mean is not equal to 240\n95 percent confidence interval:\n 258.6481 272.5640\nsample estimates:\nmean of x \n 265.6061 \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nInterpret this test result by answering these questions:\n\n1.  What was the null hypothesis?\n2.  What was the alternative hypothesis?\n3.  What does the p-value tell us?\n4.  Should we reject or fail to reject the null hypothesis at α = 0.05?\n5.  What is the practical interpretation of this result for fish biologists?\n:::\n\n# \n\n# **Lecture 5:** Intro to Hypothesis Testing\n\n::::: columns\n::: {.column width=\"60%\"}\nHypothesis testing is a systematic way to evaluate research questions using data.\n\n**Key components:**\n\n1.  **Null hypothesis (Ho)**: Typically assumes \"no effect\" or \"no difference\"\n2.  **Alternative hypothesis (Ha)**: The claim we're trying to support\n3.  **Statistical test**: Method for evaluating evidence against H₀\n4.  **P-value**: Probability of observing our results (or more extreme) if H₀ is true\n5.  **Significance level (α)**: Threshold for rejecting H₀, typically 0.05\n\n**Decision rule**: Reject Ho if p-value \\< α == p \\< 0.05\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Intro to Hypothesis Testing\n\n::::: columns\n::: {.column width=\"60%\"}\nHypothesis testing is a systematic way to evaluate research questions using data.\n\n**Key components:**\n\n1.  **Null hypothesis (H₀)**: Typically assumes \"no effect\" or \"no difference\"\n\n2.  **Alternative hypothesis (Hₐ)**: The claim we're trying to support\n\n3.  **Statistical test**: Method for evaluating evidence against H₀\n\n4.  **P-value**: Probability of observing our results (or more extreme) if H₀ is true\n\n5.  **Significance level (α)**: Threshold for rejecting H₀, typically 0.05\n\n**Decision rule**: Reject H₀ if p-value \\< α\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-9-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# Lecture 5: Interpreting and Reporting Results of a 1 sample T Test\n\n**Activity: Interpret the t-test results**\n\n-   What does the p-value tell us?\n-   Should we reject or fail to reject the null hypothesis?\n\n**How to report this result in a scientific paper:**\n\n\"A two-tailed, one-sample t-test at α=0.05 showed that the mean pine needle length on the windward side (... mm, SD = ...) \\[was/was not\\] significantly different from the expected 55 mm, t(...) = ..., p = ...\"\n\n# **Lecture 5:** Next steps - two sample T Tests\n\n::::: columns\n::: {.column width=\"60%\"}\nFor example\n\n-   what is probability that population X is the same as population Y?\n\nHow would you assess this question using what we learned?\n\nThis is what we will do with the pine needles...\n:::\n\n::: {.column width=\"40%\"}\n![](images/pine_needles.jpg){width=\"176\"}\n:::\n:::::\n\n# **Lecture 5:** Next steps - two samples\n\n::::: columns\n::: {.column width=\"60%\"}\nFor example\n\n-   what is probability that population X is the same as population Y?\n\nHow would you assess this question using what we learned?\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Now create a boxplot to visualize the difference in fish lengths between these lakes:\npine_df <- read_csv(\"data/pine_needles.csv\")\n\n# Create a boxplot comparing the two lakes\npine_wind_plot <- pine_df %>%\n  ggplot(aes(x = wind, y = length_mm, fill = wind)) +\n  geom_boxplot() +\n  labs(title = \"Pine Needle Lengths by Wind Exposure\",\n       x = \"Position\",\n       y = \"Length (mm)\",\n       fill = \"Wind Position\") +\n  scale_fill_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\"))\npine_wind_plot\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-10-1.png)\n:::\n\n```{.r .cell-code}\n# Based on the t-test results and the boxplot\n# \n# what can you conclude about the fish populations in these two lakes?\n```\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 2: Formulating Hypotheses\n\nFor the following research questions about pine needles write the null and alternative hypotheses:\n\n1.  Are needles on the lee side longer than the needles on the windy side?\n\nWhat are the hypotheses?\n\nHo =\n\nHa =\n:::\n\n# **Lecture 5:** Next steps - two samples\n\nNow, let's compare pine needle lengths between windward and leeward sides of trees.\n\nQuestion: **Is there a significant difference in needle length between the windward and leeward sides?**\n\nThis requires a two-sample t-test.\n\nTwo-sample t-test compares means from two independent groups.\n\n## $t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$\n\nwhere:\n\n-   x̄₁ and x̄₂: These represent the sample means of the two groups you're comparing \n-   s²ₚ: This is the pooled variance, calculated as: s²ₚ = \\[(n₁ - 1)s₁² + (n₂ - 1)s₂²\\] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups.\n-   **n₁ and n₂:** These are the sample sizes of the two groups.\n-   **√(1/n₁ + 1/n₂):** This represents the pooled standard error.\n\n## $t = \\frac{SIGNAL}{NOISE}$\n\n::: callout-tip\n## Practice Exercise 3: **Calculate summary statistics grouped by wind exposure**\n\nBefore conducting the test, we need to understand the data for each group.\n\n1.  You need this and the graph to see what is goin on ....\n\n\n\n\n\n\n\n\n\n\n    ::: {.cell}\n    \n    ```{.r .cell-code}\n    group_summary <- pine_df %>%\n      group_by(wind) %>%\n      summarize(\n        mean_length = mean(length_mm),\n        sd_length = sd(length_mm),\n        n = n(),\n        se_length = sd_length / sqrt(n)\n      )\n    \n    print(group_summary)\n    ```\n    \n    ::: {.cell-output .cell-output-stdout}\n    \n    ```\n    # A tibble: 2 × 5\n      wind  mean_length sd_length     n se_length\n      <chr>       <dbl>     <dbl> <int>     <dbl>\n    1 lee          20.4      2.45    24     0.500\n    2 wind         14.9      1.91    24     0.390\n    ```\n    \n    \n    :::\n    :::\n\n\n\n\n\n\n\n\n\n:::\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a boxplot comparing the two sides\npine_wind_plot\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-12-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n::: callout-tip\n## Practice Exercise 4: Effect size\n\nWe could also look at the difference in means... some cool code here\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming your dataframe is called df\ngroup_summary %>%\n  summarize(difference = mean_length[wind == \"wind\"] - mean_length[wind == \"lee\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  difference\n       <dbl>\n1       -5.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 5: Using GGPLOT to get summary stats\n\nGGplot also has code to make the mean and standard error plots we are interested in along whit a lot of others\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming your dataframe is called df\npine_mean_se_plot <- ggplot(pine_df, aes(x = wind, y = length_mm, color = wind)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Mean Pine Needle Length by Wind Exposure\",\n       x = \"Wind Exposure\",\n       y = \"Mean Length (mm)\") +\n  coord_cartesian(ylim = c(0,25))+\n  scale_color_manual(values = c(\"lee\" = \"forestgreen\", \"wind\" = \"skyblue\"),\n                   labels = c(\"lee\" = \"Leeward\", \"wind\" = \"Windward\"))+\n  theme_classic()\npine_mean_se_plot\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-14-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n# \n\n# **Lecture 5:** Testing Assumptions for Two-Sample T-Test\n\nFor a two-sample t-test, we need to check:\n\n1.  Normality within each group\n2.  Equal variances between groups (for standard t-test)\n3.  Independent observations\n\nIf assumptions are violated:\n\n-   Welch's t-test (unequal variances)\n-   Non-parametric alternatives (Mann-Whitney U test)\n-   \n\n::: callout-tip\n## Practice Exercise 6: Test normality of windward pine needle lengths\n\nqqplots\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming your dataframe is called df\npine_mean_se_plot\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 7: Test normality of windward pine needle lengths\n\nqqplots\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# how do you make separate dataframes to do this on?\n# Separate data by groups\nwindward_data <- pine_df %>% filter(wind == \"wind\")\nleeward_data <- pine_df %>% filter(wind == \"lee\")\nhead(leeward_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  date    group       n_s   wind  tree_no length_mm\n  <chr>   <chr>       <chr> <chr>   <dbl>     <dbl>\n1 3/20/25 cephalopods n     lee         1        20\n2 3/20/25 cephalopods n     lee         1        21\n3 3/20/25 cephalopods n     lee         1        23\n4 3/20/25 cephalopods n     lee         1        25\n5 3/20/25 cephalopods n     lee         1        21\n6 3/20/25 cephalopods n     lee         1        16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 8: Test normality of windward pine needle lengths\n\nqqplots\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# QQ Plot for windward group\nqqPlot(windward_data$length_mm, \n       main = \"QQ Plot for Windward Pine Needles\",\n       ylab = \"Sample Quantiles\")\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-17-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 21 22\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 9: Test normality of windward pine needle lengths\n\nShapiro-Wilk test\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test for windward group\nshapiro_windward <- shapiro.test(windward_data$length_mm)\nprint(\"Shapiro-Wilk test for windward data:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Shapiro-Wilk test for windward data:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(shapiro_windward)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  windward_data$length_mm\nW = 0.96062, p-value = 0.451\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 10: Test normality of windward pine needle lengths\n\nqqplots\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# You can also test the leeward group\n# QQ Plot for leeward group\nqqPlot(leeward_data$length_mm, \n       main = \"QQ Plot for Leeward Pine Needles\",\n       ylab = \"Sample Quantiles\")\n```\n\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-19-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  4 16\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 11: Test normality of windward pine needle lengths\n\nShapiro-Wilk test\n\nNote you need to test each groups separately...\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test for leeward group\nshapiro_lee <- shapiro.test(leeward_data$length_mm)\nprint(\"Shapiro-Wilk test for leeward data:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Shapiro-Wilk test for leeward data:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(shapiro_lee)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  leeward_data$length_mm\nW = 0.95477, p-value = 0.3425\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 12: Test Normality at one time\n\nThere are always a lot of ways to do this in R\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# there are always two ways\n# Test for normality using Shapiro-Wilk test for each wind group\n# All in one pipeline using tidyverse approach\nnormality_results <- pine_df %>%\n  group_by(wind) %>%\n  summarize(\n    shapiro_stat = shapiro.test(length_mm)$statistic,\n    shapiro_p_value = shapiro.test(length_mm)$p.value,\n    normal_distribution = if_else(shapiro_p_value > 0.05, \"Normal\", \"Non-normal\")\n  )\n\n# Print the results\nprint(normality_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  wind  shapiro_stat shapiro_p_value normal_distribution\n  <chr>        <dbl>           <dbl> <chr>              \n1 lee          0.955           0.343 Normal             \n2 wind         0.961           0.451 Normal             \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: callout-tip\n## Practice Exercise 13: Test equal variances\n\nLevenes test can be done on the original dataframe\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Method 1: Using car package's leveneTest\n# This is often preferred as it's more robust to departures from normality\nlevene_result <- leveneTest(length_mm ~ wind, data = pine_df)\nprint(\"Levene's Test for Homogeneity of Variance:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Levene's Test for Homogeneity of Variance:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(levene_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.2004 0.2789\n      46               \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Conducting the Two-Sample T-Test\n\n::::: columns\n::: {.column width=\"60%\"}\nNow we can compare the mean pine needle lengths between windward and leeward sides.\n\nHo: μ₁ = μ₂ (The mean needle lengths are equal)\n\nHa: μ₁ ≠ μ₂ (The mean needle lengths are different)\n\nDeciding between:\n\n-   Standard t-test (equal variances)\n\n-   Welch's t-test (unequal variances)\n\n**Note the Levenes Test should be NOT SIGNIFICANT - What is the null hypothesis**\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlevene_result\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.2004 0.2789\n      46               \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Conducting the Two-Sample T-Test\n\n::::: columns\n::: {.column width=\"60%\"}\nNow we can do a two sample TTEST\n\nCalculate t-statistic manually (optional)\n\nYOUR CODE HERE:\n\nt = (mean1 - mean2) / sqrt((s1\\^2/n1) + (s2\\^2/n2))\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# YOUR TASK: Conduct a two-sample t-test\n# Use var.equal=TRUE for standard t-test or var.equal=FALSE for Welch's t-test\n\n# Standard t-test (if variances are equal)\nt_test_result <- t.test(length_mm ~ wind, data = pine_df, var.equal = TRUE)\nprint(\"Standard two-sample t-test:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Standard two-sample t-test:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  length_mm by wind\nt = 8.6792, df = 46, p-value = 3.01e-11\nalternative hypothesis: true difference in means between group lee and group wind is not equal to 0\n95 percent confidence interval:\n 4.224437 6.775563\nsample estimates:\n mean in group lee mean in group wind \n          20.41667           14.91667 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Welch's t-test (if variances are unequal)\n# YOUR CODE HERE\n```\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Interpreting and Reporting Two-Sample T-Test Results\n\n::::: columns\n::: {.column width=\"60%\"}\n**Interpret the results of the two-sample t-test**\n\nWhat can we conclude about the needle lengths on windward vs. leeward sides?\n\n**How to report this result in a scientific paper:**\n\n\"A two-tailed, two-sample t-test at α=0.05 showed \\[a significant/no significant\\] difference in needle length between windward (M = ..., SD = ...) and leeward (M = ..., SD = ...) sides of pine trees, t(...) = ..., p = ....\"\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-24-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Interpreting and Reporting Two-Sample T-Test Results\n\n::::: columns\n::: {.column width=\"60%\"}\n**Interpret the results of the two-sample t-test**\n\nWhat can we conclude about the needle lengths on windward vs. leeward sides?\n\n**How to report this result in a scientific paper:**\n\n\"A two-tailed, two-sample t-test at α=0.05 showed \\[a significant/no significant\\] difference in needle length between windward (M = ..., SD = ...) and leeward (M = ..., SD = ...) sides of pine trees, t(...) = ..., p = ....\"\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_01_lecture_powerpoint_files/figure-docx/unnamed-chunk-25-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Final Activity:** Assumptions of Parametric Tests\n\n::::: columns\n::: {.column width=\"60%\"}\n**Common assumptions for t-tests:**\n\n1.  Normality: Data comes from normally distributed populations\n2.  Equal variances (for two-sample tests)\n3.  Independence: Observations are independent\n4.  No outliers: Extreme values can influence results\n\nWhat can we do if our data violates these assumptions?\n:::\n\n::: {.column width=\"40%\"}\nAlternatives when assumptions are violated:\n\n-   Data transformation (log, square root, etc.)\n-   Non-parametric tests\n-   \n:::\n:::::\n\n# **Summary and Conclusions**\n\nIn this activity, we've:\n\n1.  Formulated hypotheses about pine needle length\n2.  Tested assumptions for parametric tests\n3.  Conducted one-sample and two-sample t-tests\n4.  Visualized data using appropriate methods\n5.  Learned how to interpret and report t-test results\n\n**Key takeaways:**\n\n-   Always check assumptions before conducting tests\n-   Visualize your data to understand patterns\n-   Report results comprehensively\n-   Consider alternatives when assumptions are violated\n\n# \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}