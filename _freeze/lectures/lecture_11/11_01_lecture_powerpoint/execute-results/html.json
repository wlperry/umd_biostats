{
  "hash": "8ec3e319797f227452d1d5597238a435",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 11 - Multiple Regression\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true\n  message: false\n  warning: false\n  fig-height: 6\n  fig-width: 5\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"11_01_lecture_powerpoint_html.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"11_01_lecture_powerpoint_slides.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: true\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# Lecture 10: Review\n\n::::: columns\n::: {.column width=\"60%\"}\nCovered\n\n-   Regression T-Test Anova\n-   Regression Assumptions\n-   Model II Regression\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# Lecture 11: Overview\n\nMultiple Linear Regression model\n\n-   Regression parameters\n-   Analysis of variance\n-   Null hypotheses\n-   Explained variance\n-   Assumptions and diagnostics\n-   Collinearity\n-   Interactions\n-   Dummy variables\n-   Model selection\n-   Importance of predictors\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\nWhat if more than one predictor (X) variable?\n\n-   If predictors continuous\n-   Mix between categorical and continuous\n-   Can use multiple linear regression\n:::\n\n::: {.column width=\"40%\"}\n|                        | Independent variable |                 |\n|:-----------------------|:---------------------|:----------------|\n| **Dependent variable** | **Continuous**       | **Categorical** |\n| **Continuous**         | Regression           | ANOVA           |\n| **Categorical**        | Logistic regression  | Tabular         |\n:::\n:::::\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\nAbundance of C3 grasses can be modeled as function of\n\n-   latitude\n-   longitude\n-   both\n\nInstead of line, modeled with (hyper)plane\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-716004937.png)\n:::\n:::::\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\nUsed in similar way to simple linear regression:\n\n-   Describe nature of relationship between Y and X's\n-   Determine explained / unexplained variation in Y\n-   Predict new Ys from X\n-   Find the “best” model\n:::\n\n::: {.column width=\"40%\"}\nS![](images/clipboard-716004937.png){width=\"365\"}\n:::\n:::::\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\nCrawley 2012: “Multiple regression models provide some of the most profound challenges faced by the analyst”:\n\n-   Overfitting\n-   Parameter proliferation\n-   Multicollinearity\n-   Model selection\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-1736453741.png)\n:::\n:::::\n\n# **Lecture 11:** Analyses\n\nMultiple Regression:\n\n-   Set of i= 1 to n observations\n-   fixed X-values for p predictor variables (X1, X2…Xp)\n-   random Y-values:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i$$\n\n-   yi: value of Y for ith observation X1 = xi1, X2 = xi2,…, Xp = xip\n\n-   β0: population intercept, the mean value of Y when X1 = 0, X2 = 0,…, Xp = 0\n\n# **Lecture 11:** Multiple linear regression model\n\nMultiple Regression:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i$$\n\n-   β1: partial population slope, change in Y per unit change in X1 holding other X-vars constant\n\n-   β2: partial population slope, change in Y per unit change in X2 holding other X-vars constant\n\n-   βp: partial population slope, change in Y per unit change in Xp holding other X-vars constant\n\n# **Lecture 11:** Regression parameters\n\nMultiple Regression:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i$$\n\n-   εi: unexplained error - difference bw yi and value predicted by model (ŷi)\n\n-   NPP = β0 + β1(lat) + β2 (long) + β3 (soil fertility) + εi\n\n# **Lecture 11:** Regression parameters\n\nMultiple Regression:\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip} + \\epsilon_i$$\n\n-   Estimate multiple regression parameters (intercept, partial slopes) using OLS to fit the regression line\n-   OLS minimize ∑(yi-ŷi)2, the SS (vertical distance) between observed yi and predicted ŷi for each xij\n-   ε estimated as residuals: εi = yi-ŷi\n-   Calculation solves set of simultaneous normal equations with matrix algebra\n\n# **Lecture 11:** Regression parameters\n\nRegression equation can be used for prediction by subbing new values for predictor (X) variables\n\n-   Confidence intervals calculated for parameters\n\n-   Confidence and prediction intervals depend on number of observations and number of predictors\n\n    -   More observations decrease interval width\n    -   More predictors increase interval width\n\n-   Prediction should be restricted to within range of X variables\n\n# **Lecture 11:** Analyses of variance\n\nVariance - SStotal partitioned into SSregression and SSresidual\n\n-   SSregression is variance in Y explained by model\n\n-   SSresidual is variance not explained by model\n\n| Source of variation | SS | df | MS | Interpretation |\n|:--------------|:--------------|:--------------|:--------------|:--------------|\n| Regression | $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ | $p$ | $\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{p}$ | Difference between predicted observation and mean |\n| Residual | $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | $n-p-1$ | $\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-p-1}$ | Difference between each observation and predicted |\n| Total | $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ | $n-1$ |  | Difference between each observation and mean |\n\n# **Lecture 11:** Analyses\n\nSS converted to non-additive MS (SS/df)\n\n-   MSresidual: estimate population variance\n-   MSregression: estimate population variance + variation due to strength of X-Y relationships\n-   MS do not depend on sample size\n\n| Source of variation | SS | df | MS |\n|:-----------------|:-----------------|:-----------------|:-----------------|\n| Regression | $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ | $p$ | $\\frac{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}{p}$ |\n| Residual | $\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$ | $n-p-1$ | $\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{n-p-1}$ |\n| Total | $\\sum_{i=1}^{n} (y_i - \\bar{y})^2$ | $n-1$ |  |\n\n# **Lecture 11:** Hypotheses\n\nTwo Hos usually tested in MLR:\n\n-   “Basic” Ho: all partial regression slopes equal 0; β1 = β2 = … = βp = 0\n-   If “basic” Ho true, MSregression and MSresidual estimate variance and their ratio (F-ratio) = 1\n-   If “basic” Ho false (at least one β ≠ 0) MSregression estimates variance + partial regression slope and their ratio (F-ratio)\n-   will be \\> 1 - F-ratio compared to F-distribution for p-value\n\n# **Lecture 11:** Hypotheses\n\nAlso: is any specific β = 0 (explanatory role)?\n\n-   E.g., does LAT have effect on NPP?\n-   These Hs tested through model comparison\n-   Model w 3 predictors X1, X2,X3 (model 1):\n-   yi= β0 +β1xi1+β2xi2+β3xi3+ εi\n-   To test Ho that β1 = 0 compare fit of model 1 to model 2:\n-   yi= β0 +β2xi2+β3xi3+ εi\n\n# **Lecture 11:** Hypotheses\n\n-   If SSregression of mod1=mod2, cannot reject Ho β1 = 0\n-   If SSregression of mod1 \\> mod2, evidence to reject Ho β1 = 0\n-   SS for β1 is SSextraβ1 = Full SSregression - Reduced SSregression\n-   Use partial F-test to test Ho β1 = 0 :\n\n$$F_{w,n-p} = \\frac{MS_{Extra}}{FULL\\ MS_{Residual}}  $$ Can also use t-test (R provides this value)\n\n# **Lecture 11:** Explained variance\n\nExplained variance (r2) is calculated the same way as for simple regression:\n\n$$r^2 = \\frac{SS_{Regression}}{SS_{Total}} = 1 - \\frac{SS_{Residual}}{SS_{Total}}  $$\n\n-   r2 values can not be used to directly compare models\n-   r2 values will always increase as predictors added\n-   r2 values with different transformation will differ\n\n# **Lecture 11:** Assumptions and diagnostics\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Assume fixed Xs; unrealistic in most biological settings\n-   No major (influential) outliers\n-   Check leverage, influence- Cook’s Di\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2650668640.png){width=\"391\"}\n:::\n:::::\n\n# **Lecture 11:** Assumptions and diagnostics\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Normality, equal variance, independence\n-   Residual QQ-plots, residuals vs. predicted values plot\n-   Distribution/variance often corrected by transforming Y\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3093948430.png){width=\"296\"}\n:::\n:::::\n\n# **Lecture 11:** Assumptions and diagnostics\n\nMore observations than predictor variables\n\n-   Ideally at least 10x observations than predictors to avoid “overfitting”\n-   Uncorrelated predictor variables (assessed using scatterplot matrix; VIFs)\n-   Linear relationship between Y and each X, holding others constant (non-linearity assessed by AV plots)\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\nRegression of Y vs. each X does not consider effect of other predictors:\n\nwant to know shape of relationship while holding other predictors constant\n:::\n\n::: {.column width=\"40%\"}\n:::\n\n![](images/clipboard-2592296129.png){width=\"340\"}\n:::::\n\n# **Lecture 11:** Collinearity\n\n-   Potential predictor variables are often correlated (e.g., morphometrics, nutrients, climatic parameters)\n-   Multicollinearity (strong correlation between predictors) causes problems for parameter estimates\n-   Severe collinearity causes unstable parameter estimates: small change in a single value can result in large changes in βp - estimates\n-   Inflates partial slope error estimates, loss of power\n\n![](images/clipboard-2854056083.png)\n\n# **Lecture 11:** Collinearity\n\nCollinearity can be detected by:\n\n-   Variance inflation Factors:\n\n    -   VIF for Xj=1/ (1-r2 )\n    -   VIF \\> 10 = bad\n\n-   Best/simplest solution:\n\n    -   exclude variables that are highly correlated with other variables\n    -   they are probably measuring similar\n    -   thing and are redundant\n\n# **Lecture 11:** Interactions\n\nPredictors can be modeled as:\n\n-   additive (effect of temp, plus precip, plus fertility) or\n-   multiplicative (interactive)\n-   Interaction: effect of Xi depends on levels of Xj\n-   The partial slope of Y vs. X1 is different for different levels of X2 (and vice versa); measured by β3\n\n$$y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + \\epsilon_i \\quad \\text{vs.} \\quad y_i = \\beta_0 + \\beta_1X_{i1} + \\beta_2X_{i2} + + \\beta_3X_{i3} \\epsilon_i$$\n\n“Curvature” of the regression (hyper)plane\n\n# **Lecture 11:** Analyses\n\n![](images/clipboard-4241772680.png){width=\"348\"}\n\n# **Lecture 11:** Analyses\n\nAdding interactions:\n\n-   many more predictors (“parameter proliferation”):\n-   2n; 6 params= 64 terms; 7 params= 128\n-   interpretation more complex\n-   When to include interactions? When they make biological sense\n\n# **Lecture 11:** Dummy variables\n\nMultiple Linear Regression accommodates continuous and categorical variables (gender, vegetation type, etc.) Categorical vars as “dummy vars”, n of dummy variables = n-1 categories\n\n**Sex M/F:**\n\n-   Need 1 dummy var with two values (0, 1)\n\n**Fertility L/M/H:**\n\n-   Need 2 dummy var, each with two values (0, 1): fert1 (0 if L or H, 1 if M), fert2 (1 if H, 0 if L or M)\n\n| Fertility | fert1 | fert2 |\n|:----------|:------|:------|\n| Low       | 0     | 0     |\n| Med       | 1     | 0     |\n| High      | 0     | 1     |\n\n# **Lecture 11:** Analyses\n\nCoefficients interpreted relative to reference condition\n\n-   R codes dummy variables automatically\n-   picks “reference” level alphabetically\n-   Dummy variables with more than 2 levels add extra predictor variables to model\n\n| Fertility | fert1 | fert2 |\n|-----------|-------|-------|\n| Low       | 0     | 0     |\n| Med       | 1     | 0     |\n| High      | 0     | 1     |\n\n# **Lecture 11:** Analyses\n\n::::: columns\n::: {.column width=\"60%\"}\n![](images/clipboard-3847962697.png){width=\"540\"}\n:::\n\n::: {.column width=\"40%\"}\nS![](images/clipboard-3285444079.png){width=\"250\"}\n:::\n:::::\n\n# **Lecture 11:** Comparing models\n\nWhen have multiple predictors (and interactions!)\n\n-   how to choose “best” model?\n-   Which predictors to include?\n-   Occam’s razor: “best” model balances complexity with fit to data\n\nTo chose:\n\n-   compare “nested” models\n\nOverfitting\n\n-   getting high r2 just by having more (useless predictors)\n-   so r2 is not a good way of choosing between nested models\n\n# **Lecture 11:** Comparing models\n\nNeed to account for increase in fit with added predictors:\n\n-   Adjusted r2\n-   Akaike’s information criterion (AIC)\n-   Both “penalize” models for extra predictors\n-   Higher adjusted r2 and lower AIC are better when comparing models\n\n$$\\text{Adjusted } r^2 = 1 - \\frac{SS_{\\text{Residual}}/(n - (p + 1))}{SS_{\\text{Total}}/(n - 1)}$$ $$\\text{Akaike Information Criterion (AIC)} = n[\\ln(SS_{\\text{Residual}})] + 2(p + 1) - n\\ln(n)$$\n\n# **Lecture 11:** Comparing models\n\nBut how to compare models?\n\n-   Can fit all possible models\n\n    -   compare AICs or adj- r2,\n    -   tedious w lots of predictors\n\n-   Automated forward (and backward) stepwise procedures: start w no terms (all terms), add (remove) terms w largest (smallest)\n\n    -   partial F statistic\n\nWe will use manual form of backward selection\n\n# **Lecture 11:** Analyses\n\n# ![](images/clipboard-1017836861.png)\n\n# **Lecture 11:** Predictors\n\nUsually want to know relative importance of predictors to explaining Y\n\n-   Three general approaches:\n-   Using F-tests (or t-tests) on partial regression slopes\n-   Using coefficient of partial determination\n-   Using standardized partial regression slopes\n\n# **Lecture 11:** Predictors\n\nUsing F-tests (or t-tests) on partial regression slopes:\n\n-   Conduct F tests of Ho that each partial regression slope = 0\n-   If cannot reject Ho, discard predictor\n-   Can get additional clues from relative size of F-values\n-   Does not tell us absolute importance of predictor (usually can not directly compare slope parameters)\n\n# **Lecture 11:** Predictors\n\nUsing coefficient of partial determination:\n\n-   the reduction in variation of Y due to addition of predictor (Xj)\n\n$$r_{X_j}^2 = \\frac{SS_{\\text{Extra}}}{\\text{Reduced }SS_{\\text{Residual}}}$$\n\nSSextra\n\n-   Increased in SSregression when Xj is added to model\n\n-   Reduced SSresidual is the unexplained SS from model without Xj\n\n# **Lecture 11:** Predictors\n\nUsing standardized partial regression slopes:\n\n-   predictors of predictor variables can not be directly compared\n-   Why?\n-   Standardize all vars (mean = 0, sd= 1)\n-   Scales are identical and larger PRS mean more important variable\n\n# **Lecture 11:** Predictors\n\n::::: columns\n::: {.column width=\"60%\"}\nUsing partial r2 values:\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2521916586.png)\n:::\n:::::\n\n# **Lecture 11:** Reporting results\n\n::::: columns\n::: {.column width=\"60%\"}\nResults are easiest to report in tabular format\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-829166687.png)\n:::\n:::::\n\n# **Lecture 11:** Reporting results\n\n::::: columns\n::: {.column width=\"60%\"}\nResults are easiest to report in tabular format\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-2807428674.png)\n:::\n:::::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}