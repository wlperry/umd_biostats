{
  "hash": "0c3cd825b1471052bd2cca128b0245db",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Lecture 06\"\nauthor: \"Bill Perry\"\nexecute:\n  freeze: auto\n  cache: true\n  echo: true\n  keep-md: true # retains the images when you start again\n  message: false\n  warning: false\n  fig-height: 3\n  fig-width: 3\n  paged-print: false\nformat:\n  html:\n    toc: false\n    output-file: \"06_01_lecture_powerpoint_html_old.html\"\n    embed-resources: true\n    self-contained: true\n    max-width: 80ch  # Limits line length to approximately 80 characters\n    css: ../../css/lecture.css\n  revealjs:\n    output-file: \"06_01_lecture_powerpoint_slides_old.html\"\n    self-contained: true\n    css: ../../css/lecture.css\n    slide-number: true\n    transition: fade\n  docx:\n    default: true\n    toc: false\n    toc-depth: 3\n    number-sections: false\n    highlight-style: github\n    reference-doc: ../../ms_templates/custom-reference.docx\n    css: msword.css\n    embed-resources: true\n  pptx:\n    reference-doc: ../../ms_templates/lecture_template.pptx \n    embed-resources: true\neditor: visual\n---\n\n\n\n\n\n\n\n\n\n\n# Lecture 6: Review\n\n::::: columns\n::: {.column width=\"60%\"}\nCovered\n\n-   Introduction to hypothesis testing\n\n-   The standard normal distribution\n\n-   Standard error\n\n-   Confidence intervals\n\n-   Student's t-distribution\n\n-   H testing\n\n-   **One and Two Sample T Test**\n\n-   **p-values**\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-536528302.png){width=\"214\"}\n:::\n:::::\n\n# **Lecture 6:** Overview\n\n::::: columns\n::: {.column width=\"60%\"}\n## The objectives:\n\n-   p-values\n-   Brief review\n-   H test for a single population\n-   1- and 2-sided tests\n-   Hypothesis tests for two populations\n-   Assumptions of parametric tests\n:::\n\n::: {.column width=\"40%\"}\n![](images/pine_needles.jpg){width=\"338\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   Major goal of statistics:\n    -   inferences about populations from samples...\n        -   assign degree of confidence to inferences\n    -   Statistical hypothesis testing:\n        -   formalized approach to inference\n    -   Hypotheses ask whether samples come from populations with certain properties\n    -   Often interested in questions about population means\n        -   but other questions are of interest\n:::\n\n::: {.column width=\"40%\"}\n![](images/pine_tree.png){width=\"372\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nUseful hypotheses: - Rely on specifying - null hypothesis (Ho) - alternate hypothesis (Ha)\n\n-   Ho is the hypothesis of “no effect”\n    -   two samples from population with same mean\n    -   sample is from population of mean = 0\\\n-   Ha (research hypothesis)\n    -   is the opposite of the Ho\n    -   or predicts that there is an effect of x on y\n    -   *but does NOT suggest a direction*\n:::\n\n::: {.column width=\"40%\"}\n\\-![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nTogether Ho and Ha encompass all possible outcomes:\n\n-   For Example:\\\n\n-   Ho: µ=0, Ha: µ ≠ 0\n\n    -   mean equals 0 or mean does not equal 0\n\n-   Ho: µ=35, Ha: µ ≠ 35\n\n    -   mean equals 35 or mean does not equal 35\n    -   Ho: µ1 = µ2, Ha: µ1 ≠ µ2\n    -   mean of population 1 equals mean of population 2 or it does not\n    -   Ho: µ \\> 0, Ha: µ ≤ 0\n    -   can be directional mean is greater than 0 or mean is not equal or less than 0\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nTests assess likelihood of the null hypothesis being true\n\n-   If the Ho is likely false, then Ha assumed to be correct\n-   More precisely:\n    -   the long run probability of obtaining sample value (or more extreme one) if the null hypothesis is true\n        -   p(data\\|Ho) - the probability of observing the data given that the null hypothesis Ho is true\n:::\n\n::: {.column width=\"40%\"}\n\\-![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nHypothesis tests\n\n-   Expressed as p-value (0 to 1)\n-   Interpret p-value as:\n    -   probability of obtaining sample value of statistic (or more extreme one) if Ho is true\n-   High p-value:\n    -   high probability of obtaining sample statistic under Ho\n        -   if the null hypothesis (Ho) were true, you would frequently observe data similar to or more extreme than your sample statistic\n        -   your observed results are quite compatible with what the null hypothesis predicts\n    -   low p-value: low probability of obtaining sample statistic under Ho\n        -   if the null hypothesis (Ho) were true, you would rarely observe data similar to or more extreme than your sample statistic\n        -   Your results are unusual under the null hypothesis, suggesting that either you've witnessed a rare event or the null hypothesis may be incorrect\n:::\n\n::: {.column width=\"40%\"}\n\\-![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nStatistical test results:\n\np = 0.3 means that if I repeated the study 100 times, I would get this (or more extreme) result due to chance 30 times\n\np = 0.03 means that if I repeated the study 100 times, I would get this (or more extreme) result due to chance 3 times\n\n*Which p-value suggests Ho likely false?*\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nStatistical test results:\n\nAt what point reject Ho?\n\np \\< 0.05 conventional “significance threshold” (α = alpha or p value)\n\np \\< 0.05 means: if Ho is true and we repeated the study 100 times - we would get this (or more extreme) result less than 5 times due to chance\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nStatistical test results:\n\nα is the rate at which we will reject a true null hypothesis (Type I error rate)\n\nLowering α will lower likelihood of incorrectly rejecting a true null hypothesis (e.g., 0.01, 0.001)\n\n*Both Hs and α are specified* *BEFORE collection of data and analysis*\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\nTraditionally α=0.05 is used as a cut off for rejecting null hypothesis\n\nThere is nothing magical about 0.05 - actual p-values need to be reported - also need to decide prior to study\n\n| p-value range | Interpretation |\n|----|----|\n| P \\> 0.10 | No evidence against Ho - data appear consistent with Ho |\n| 0.05 \\< P \\< 0.10 | Weak evidence against the Ho in favor of Ha |\n| 0.01 \\< P \\< 0.05 | Moderate evidence against Ho in favor of Ha |\n| 0.001 \\< P \\< 0.01 | Strong evidence against Ho in favor of Ha |\n| P \\< 0.001 | Very strong evidence against Ho in favor of Ha |\n\n# **Lecture 6:** Statistical hypothesis testing\n\n![](images/clipboard-3329723408.png){width=\"493\"}\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nFisher:\n\np-value as informal measure of discrepancy between data and Ho\n\n“If p is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it is below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 …”\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-694363384.png){width=\"547\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nGeneral procedure for H testing:\n\n-   Specify Null (Ho) and alternate (Ha)\n-   Determine test (and test statistic) to be used\n-   Test statistic is used to compare your data to expectation under Ho (null hypothesis)\n-   Specify significance (α or p value) level below which Ho will be rejected\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   General procedure for H testing:\n-   Collect data\n-   Perform test\n    -   If p-value \\< α, conclude Ho is likely false and reject it\n\n    -   If p-value \\> α, conclude no evidence Ho is false and retain it\n:::\n\n::: {.column width=\"40%\"}\n![](images/two_branches.jpg){width=\"450\"}\n:::\n:::::\n\n# **Lecture 6:** Brief review\n\nRecall…\n\n-   Major goal of statistics: inferences about populations from samples... and assign degree of confidence to inferences\n-   Statistical H-testing: formalized approach to inference\n-   Relies on specifying null hypothesis (Ho) and alternate hypothesis (Ha\n-   Tests assess likelihood of the null hypothesis being true\n-   Expressed as p-value: probability of obtaining sample value of statistic (or more extreme one) if Ho is true\n\n# **Lecture 6:** Brief review\n\nRecall pine needle example\n\n-   Probability of getting sample\n\n-   with ȳ at least as far away from 21 as 35)? - p(ȳ ≤ 3500 or ȳ ≥ 3900)\n\n    -   What about - 1-tailed or 2-tailed test?\n\n    -   Can solve using SND and z-scores\n\n# **Lecture 6:** Brief review\n\n::::: columns\n::: {.column width=\"40%\"}\n-   z= (21-35)/40 = -0.48\n\n    -   From z table: p= 0.6368 X 2\n    -   p of getting sample as far away from µ as A is = 0.6368 (63.6%)\n\n-   But - usually can’t use z!\n\n-   Can use t-distribution instead…\n:::\n\n::: {.column width=\"60%\"}\n![](images/clipboard-990096460.png){width=\"555\"}\n:::\n:::::\n\n# **Pine Needle Length: Hypothesis Testing Activity**\n\n::::: columns\n::: {.column width=\"60%\"}\nThis activity will guide you through the process of conducting single-sample and two-sample t-tests on pine needle data. We'll explore how environmental factors like wind exposure might affect pine needle length.\n\nYou'll learn to:\n\n-   Formulate hypotheses\n-   Test assumptions\n-   Perform t-tests\n-   Visualize data\n-   Report results accurately\n:::\n\n::: {.column width=\"40%\"}\n![Pine needles from trees](images/pine_needles.jpg){width=\"316\"}\n:::\n:::::\n\n# **Part 1:** Single Sample T-test\n\n::::: columns\n::: {.column width=\"60%\"}\nA single sample t-test asks whether a population parameter (like $\\bar{x}$) differs from some expected value.\n\nThe question: **Is the average pine needle length from our windward sample different from 55mm?**\n\n# One-sample t-test\n\nUsed when we want to compare a sample mean to a known or hypothesized population value.\n\n### $t = \\frac{\\bar{x} - \\mu}{s/\\sqrt{n}}$\n\nwhere:\n\n-   $\\bar{x}$ is the sample mean\n-   $\\mu$ is the hypothesized population mean\n-   $s$ is the sample standard deviation\n-   $n$ is the sample size\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-386829805.png){width=\"400\"}\n:::\n:::::\n\n# How to do this in R\n\n::::: columns\n::: {.column width=\"60%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install packages if needed (uncomment if necessary)\n# install.packages(\"readr\")\n# install.packages(\"tidyverse\")\n# install.packages(\"car\")\n# install.packages(\"here\")\n\n# Load libraries\nlibrary(car)          # For diagnostic tests\nlibrary(tidyverse)    # For data manipulation and visualization\n```\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the pine needle data\n# Use here() function to specify the path\npine_data <- read_csv(\"data/pine_needles.csv\")\n\n# Examine the first few rows\nhead(pine_data)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  date    group       n_s   wind  tree_no len_mm\n  <chr>   <chr>       <chr> <chr>   <dbl>  <dbl>\n1 3/20/25 cephalopods n     lee         1     20\n2 3/20/25 cephalopods n     lee         1     21\n3 3/20/25 cephalopods n     lee         1     23\n4 3/20/25 cephalopods n     lee         1     25\n5 3/20/25 cephalopods n     lee         1     21\n6 3/20/25 cephalopods n     lee         1     16\n```\n\n\n:::\n\n```{.r .cell-code}\ngrayling_df <- read_csv(\"data/gray_I3_I8.csv\")\n```\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Part 1:** Exploratory Data Analysis\n\nBefore conducting hypothesis tests, we should always explore our data to understand its characteristics.\n\nLet's calculate summary statistics and create visualizations.\n\n**Activity: Calculate basic summary statistics for pine needle length**\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# YOUR TASK: Calculate summary statistics for pine needle length\n# Hint: Use summarize() function to calculate mean, sd, n, etc.\n\n# Create a summary table for all pine needles\npine_summary <- pine_data %>%\n  summarize(\n    mean_length = mean(len_mm),\n    sd_length = sd(len_mm),\n    n = n(),\n    se_length = sd_length / sqrt(n)\n  )\n\nprint(pine_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        <dbl>     <dbl> <int>     <dbl>\n1        17.7      3.53    48     0.509\n```\n\n\n:::\n\n```{.r .cell-code}\n# Now calculate summary statistics by wind exposure\n# YOUR CODE HERE\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 1:** Visualizing the Data\n\n::::: columns\n::: {.column width=\"60%\"}\n**Activity: Create visualizations of pine needle length**\n\nCreate a histogram and a boxplot to visualize the distribution of pine needle length values.\n:::\n\n::: {.column width=\"40%\"}\nEffective data visualization helps us understand:\n\n-   The central tendency\n-   The spread of the data\n-   Potential outliers\n-   Shape of distribution\n:::\n:::::\n\n# Your Task\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# YOUR TASK: Create a histogram of pine needle length\n# Hint: Use ggplot() and geom_histogram()\n\n# Histogram of all pine needle lengths\nggplot(pine_data, aes(x = len_mm)) +\n  geom_histogram(binwidth = 2, fill = \"steelblue\", color = \"black\") +\n  labs(title = \"Distribution of Pine Needle Length\",\n       x = \"Length (mm)\",\n       y = \"Frequency\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/visualize-1.png)\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Boxplot of pine needle length by wind exposure\n# YOUR CODE HERE\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 1:** Single Sample T-Test\n\n::::: columns\n::: {.column width=\"60%\"}\nWe want to test if the mean pine needle length on the windward side differs from 55mm.\n\n**Activity: Define hypotheses and identify assumptions**\n\nH₀: μ = 55 (The mean pine needle length on windward side is 55mm) H₁: μ ≠ 55 (The mean pine needle length on windward side is not 55mm)\n:::\n\n::: {.column width=\"40%\"}\n## Assumptions for t-test:\n\n1.  Data is normally distributed\n2.  Observations are independent\n3.  No significant outliers\n:::\n:::::\n\n# **Part 1:** Testing Assumptions\n\n::::: columns\n::: {.column width=\"60%\"}\nBefore conducting our t-test, we need to verify that our data meets the necessary assumptions.\n\n**Activity: Test the normality assumption**\n:::\n\n::: {.column width=\"40%\"}\nMethods to test normality:\n\n-   Visual methods:\n\n    -   QQ plots or histograms\n\n    -   Statistical tests: Shapiro\n\n    -   Wilk test\n:::\n:::::\n\n# Assumptions in R - qqplots\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# Filter for just windward side needles\nwindward_data <- pine_data %>% \n  filter(wind == \"wind\")\n\n# YOUR TASK: Test normality of windward pine needle lengths\n# QQ Plot\nqqPlot(windward_data$len_mm, \n       main = \"QQ Plot for Windward Pine Needles\",\n       ylab = \"Sample Quantiles\")\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/test_assumptions-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 21 22\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n## Shapiro Wilk\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Shapiro-Wilk test\nshapiro_test <- shapiro.test(windward_data$len_mm)\nprint(shapiro_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  windward_data$len_mm\nW = 0.96062, p-value = 0.451\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Check for outliers using boxplot\n# YOUR CODE HERE\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 1:** Conducting the Single Sample T-Test\n\nNow that we've checked our assumptions, we can perform the single sample t-test.\n\n**Activity: Conduct a single sample t-test to compare windward needle length to 55mm** **What is probability of getting sample at least as far from 55mm as our sample mean?**\n\nThis is our p-value, which helps us decide whether to reject the null hypothesis.\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# Calculate summary statistics for windward needles\nwindward_summary <- windward_data %>%\n  summarize(\n    mean_length = mean(len_mm),\n    sd_length = sd(len_mm),\n    n = n(),\n    se_length = sd_length / sqrt(n)\n  )\n\nprint(windward_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 4\n  mean_length sd_length     n se_length\n        <dbl>     <dbl> <int>     <dbl>\n1        14.9      1.91    24     0.390\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Your Task\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# YOUR TASK: Conduct a single sample t-test\nt_test_result <- t.test(windward_data$len_mm, mu = 55, var.equal = TRUE )\nprint(t_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  windward_data$len_mm\nt = -102.85, df = 23, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 55\n95 percent confidence interval:\n 14.11050 15.72284\nsample estimates:\nmean of x \n 14.91667 \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate t-statistic manually \n# YOUR CODE HERE: t = (sample_mean - hypothesized_mean) / (sample_sd / sqrt(n))\n\n# can you do this manually or manually with R?\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 1:** Interpreting and Reporting Results\n\n**Activity: Interpret the t-test results**\n\n-   What does the p-value tell us?\n-   Should we reject or fail to reject the null hypothesis?\n\n**How to report this result in a scientific paper:**\n\n\"A two-tailed, one-sample t-test at α=0.05 showed that the mean pine needle length on the windward side (... mm, SD = ...) \\[was/was not\\] significantly different from the expected 55 mm, t(...) = ..., p = ...\"\n\n# **Part 2:** Two Sample T-Test\n\nNow, let's compare pine needle lengths between windward and leeward sides of trees.\n\nQuestion: **Is there a significant difference in needle length between the windward and leeward sides?**\n\nThis requires a two-sample t-test.\n\nTwo-sample t-test compares means from two independent groups.\n\n## $t = \\frac{\\bar{x}_1 - \\bar{x}_2}{S_p\\sqrt{\\frac{1}{n_1} + \\frac{1}{n_2}}}$\n\nwhere:\n\n-   x̄₁ and x̄₂: These represent the sample means of the two groups you're comparing \n-   s²ₚ: This is the pooled variance, calculated as: s²ₚ = \\[(n₁ - 1)s₁² + (n₂ - 1)s₂²\\] / (n₁ + n₂ - 2), where s₁² and s₂² are the sample variances of the two groups.\n-   **n₁ and n₂:** These are the sample sizes of the two groups.\n-   **√(1/n₁ + 1/n₂):** This represents the pooled standard error.\n\n# **Part 2:** Exploratory Data Analysis by Group\n\n**Activity: Calculate summary statistics grouped by wind exposure** Before conducting the test, we need to understand the data for each group.\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# YOUR TASK: Calculate summary statistics by wind exposure\n# Hint: Use group_by() and summarize()\n\ngroup_summary <- pine_data %>%\n  group_by(wind) %>%\n  summarize(\n    mean_length = mean(len_mm),\n    sd_length = sd(len_mm),\n    n = n(),\n    se_length = sd_length / sqrt(n)\n  )\n\nprint(group_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  wind  mean_length sd_length     n se_length\n  <chr>       <dbl>     <dbl> <int>     <dbl>\n1 lee          20.4      2.45    24     0.500\n2 wind         14.9      1.91    24     0.390\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Alternative 1\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the difference in means\n# YOUR CODE HERE\n\n# Assuming your dataframe is called df\ngroup_summary %>%\n  summarize(difference = mean_length[wind == \"wind\"] - mean_length[wind == \"lee\"])\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  difference\n       <dbl>\n1       -5.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Alternative 2\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Or alternatively using filter and pull:\nlee_mean <- group_summary %>% filter(wind == \"lee\") %>% pull(mean_length)\nwind_mean <- group_summary %>% filter(wind == \"wind\") %>% pull(mean_length)\ndifference <- wind_mean - lee_mean\ndifference\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] -5.5\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 2:** Visualizing Group Differences\n\n**Activity: Create visualizations to compare the groups** Effective visualizations for group comparisons:\n\n-   Side-by-side boxplots\n-   Violin plots\n-   Error bar plots\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# YOUR TASK: Create boxplots to compare groups\nggplot(pine_data, aes(x = wind, y = len_mm, fill = wind)) +\n  geom_boxplot() +\n  labs(title = \"Pine Needle Length by Wind Exposure\",\n       x = \"Wind Exposure\",\n       y = \"Length (mm)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/group_visualization-1.png)\n:::\n\n```{.r .cell-code}\n# how can you do this by wind to see both plots\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# your task\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# YOUR TASK: Create a plot using stat_summary to show means and standard errors\nggplot(pine_data, aes(x = wind, y = len_mm, color = wind)) +\n  stat_summary(fun = mean, geom = \"point\") +\n  stat_summary(fun.data = mean_se, geom = \"errorbar\", width = 0.2) +\n  labs(title = \"Mean Pine Needle Length by Wind Exposure\",\n       x = \"Wind Exposure\",\n       y = \"Mean Length (mm)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-8-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 2:** Testing Assumptions for Two-Sample T-Test\n\n**Activity: Test assumptions for two-sample t-test**\n\nFor a two-sample t-test, we need to check:\n\n1.  Normality within each group\n2.  Equal variances between groups (for standard t-test)\n3.  Independent observations\n\nIf assumptions are violated:\n\n-   Welch's t-test (unequal variances)\n-   Non-parametric alternatives (Mann-Whitney U test)\n\n# your task\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# YOUR TASK: Test normality of windward pine needle lengths\n# QQ Plot\nqqPlot(pine_data$len_mm, \n       main = \"QQ Plot for Windward Pine Needles\",\n       ylab = \"Sample Quantiles\")\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-9-1.png)\n:::\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1]  4 28\n```\n\n\n:::\n:::\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# Testing normality for each group\n# Leeward group\nlee_data <- pine_data %>% filter(wind == \"lee\")\nshapiro_lee <- shapiro.test(lee_data$len_mm)\nprint(\"Shapiro-Wilk test for leeward data:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Shapiro-Wilk test for leeward data:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(shapiro_lee)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tShapiro-Wilk normality test\n\ndata:  lee_data$len_mm\nW = 0.95477, p-value = 0.3425\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# windward group\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Windward group\n# YOUR CODE HERE for windward group normality test\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# Remember you can always do it in one go\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# there are always two ways\n# Test for normality using Shapiro-Wilk test for each wind group\n# All in one pipeline using tidyverse approach\nnormality_results <- pine_data %>%\n  group_by(wind) %>%\n  summarize(\n    shapiro_stat = shapiro.test(len_mm)$statistic,\n    shapiro_p_value = shapiro.test(len_mm)$p.value,\n    normal_distribution = if_else(shapiro_p_value > 0.05, \"Normal\", \"Non-normal\")\n  )\n\n# Print the results\nprint(normality_results)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 4\n  wind  shapiro_stat shapiro_p_value normal_distribution\n  <chr>        <dbl>           <dbl> <chr>              \n1 lee          0.955           0.343 Normal             \n2 wind         0.961           0.451 Normal             \n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n\n# Conduct a Levenes Test\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Test for equal variances\n# YOUR TASK: Conduct Levene's test for equality of variances\nlevene_test <- leveneTest(len_mm ~ wind, data = pine_data)\nprint(levene_test)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  1.2004 0.2789\n      46               \n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Visual check for normality with QQ plots\n# YOUR CODE HERE\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 2:** Conducting the Two-Sample T-Test\n\n**Activity: Conduct a two-sample t-test**\n\nNow we can compare the mean pine needle lengths between windward and leeward sides.\n\nH₀: μ₁ = μ₂ (The mean needle lengths are equal) H₁: μ₁ ≠ μ₂ (The mean needle lengths are different)\n\nDeciding between:\n\n-   Standard t-test (equal variances)\n-   Welch's t-test (unequal variances)\n\n# Based on our Levene's test result.\n\n\n\n\n\n\n\n\n\n\n::: {.cell exercise='true'}\n\n```{.r .cell-code}\n# YOUR TASK: Conduct a two-sample t-test\n# Use var.equal=TRUE for standard t-test or var.equal=FALSE for Welch's t-test\n\n# Standard t-test (if variances are equal)\nt_test_result <- t.test(len_mm ~ wind, data = pine_data, var.equal = TRUE)\nprint(\"Standard two-sample t-test:\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Standard two-sample t-test:\"\n```\n\n\n:::\n\n```{.r .cell-code}\nprint(t_test_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tTwo Sample t-test\n\ndata:  len_mm by wind\nt = 8.6792, df = 46, p-value = 3.01e-11\nalternative hypothesis: true difference in means between group lee and group wind is not equal to 0\n95 percent confidence interval:\n 4.224437 6.775563\nsample estimates:\n mean in group lee mean in group wind \n          20.41667           14.91667 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Welch's t-test (if variances are unequal)\n# YOUR CODE HERE\n\n# Calculate t-statistic manually (optional)\n# YOUR CODE HERE: t = (mean1 - mean2) / sqrt((s1^2/n1) + (s2^2/n2))\n```\n:::\n\n\n\n\n\n\n\n\n\n\n# **Part 2:** Interpreting and Reporting Two-Sample T-Test Results\n\n::::: columns\n::: {.column width=\"60%\"}\n**Activity: Interpret the results of the two-sample t-test**\n\nWhat can we conclude about the needle lengths on windward vs. leeward sides?\n\n**How to report this result in a scientific paper:**\n\n\"A two-tailed, two-sample t-test at α=0.05 showed \\[a significant/no significant\\] difference in needle length between windward (M = ..., SD = ...) and leeward (M = ..., SD = ...) sides of pine trees, t(...) = ..., p = ....\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-3575593369.png){width=\"350\"}\n:::\n:::::\n\n# **Part 3:** Paired T-Test (Extended Activity)\n\nIf we collected data in pairs (same tree, different sides), we would use a paired t-test. **How would the analysis differ?**\n\n1.  We'd calculate the difference for each pair\n2.  Test if the mean difference equals zero\n3.  The paired approach often has more statistical power\n\nPaired t-test formula:\n\n## $t = \\frac{\\bar{d}}{s_d/\\sqrt{n}}$\n\nwhere:\n\n-   $\\bar{d}$ is the mean difference\n-   $s_d$ is the standard deviation of differences\n-   $n$ is the number of pairs\n\n# **Final Activity:** Assumptions of Parametric Tests\n\n::::: columns\n::: {.column width=\"60%\"}\n**Common assumptions for t-tests:**\n\n1.  Normality: Data comes from normally distributed populations\n2.  Equal variances (for two-sample tests)\n3.  Independence: Observations are independent\n4.  No outliers: Extreme values can influence results\n\nWhat can we do if our data violates these assumptions?\n:::\n\n::: {.column width=\"40%\"}\nAlternatives when assumptions are violated:\n\n-   Data transformation (log, square root, etc.)\n-   Non-parametric tests\n-   Bootstrapping approaches\n:::\n:::::\n\n# **Summary and Conclusions**\n\nIn this activity, we've:\n\n1.  Formulated hypotheses about pine needle length\n2.  Tested assumptions for parametric tests\n3.  Conducted one-sample and two-sample t-tests\n4.  Visualized data using appropriate methods\n5.  Learned how to interpret and report t-test results\n\n**Key takeaways:**\n\n-   Always check assumptions before conducting tests\n-   Visualize your data to understand patterns\n-   Report results comprehensively\n-   Consider alternatives when assumptions are violated\n\n# **Lecture 5:** Understanding P-values\n\n::::: columns\n::: {.column width=\"60%\"}\nA **p-value** is the probability of observing the sample result (or something more extreme) if the null hypothesis is true.\n\n**Common interpretations:** - p \\< 0.05: Strong evidence against H₀ - 0.05 ≤ p \\< 0.10: Moderate evidence against H₀ - p ≥ 0.10: Insufficient evidence against H₀\n\n**Common misinterpretations:** - p-value is NOT the probability that H₀ is true - p-value is NOT the probability that results occurred by chance - Statistical significance ≠ practical significance\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-14-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Type I and Type II Errors\n\n::::: columns\n::: {.column width=\"60%\"}\nWhen making decisions based on hypothesis tests, two types of errors can occur:\n\n**Type I Error (False Positive)** - Rejecting H₀ when it's actually true - Probability = α (significance level) - \"Finding an effect that isn't real\"\n\n**Type II Error (False Negative)** - Failing to reject H₀ when it's actually false - Probability = β - \"Missing an effect that is real\"\n\n**Statistical Power = 1 - β** - Probability of correctly rejecting a false H₀ - Increases with: - Larger sample size - Larger effect size - Lower variability - Higher α level\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-15-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 6: Interpreting P-values and Errors\n\nGiven the following scenarios, identify whether a Type I or Type II error might have occurred:\n\n1.  A researcher concludes that a new fishing regulation increased grayling size, when in fact it had no effect.\n\n2.  A study fails to detect a real decline in grayling population due to warming water, concluding there was no effect.\n\n3.  Let's calculate the power of our t-test to detect a 30 mm difference in length between lakes:\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate power for detecting a 30 mm difference\n# First determine parameters\nlake_I3 <- grayling_df %>% filter(lake == \"I3\")\nlake_I8 <- grayling_df %>% filter(lake == \"I8\") \n\nn1 <- nrow(lake_I3)\nn2 <- nrow(lake_I8)\nsd_pooled <- sqrt((var(lake_I3$total_length_mm) * (n1-1) + \n                  var(lake_I8$total_length_mm) * (n2-1)) / \n                  (n1 + n2 - 2))\n\n# Calculate power\neffect_size <- 30 / sd_pooled  # Cohen's d\ndf <- n1 + n2 - 2\nalpha <- 0.05\npower <- power.t.test(n = min(n1, n2), \n                     delta = effect_size,\n                     sd = 1,  # Using standardized effect size\n                     sig.level = alpha,\n                     type = \"two.sample\",\n                     alternative = \"two.sided\")\n\n# Display results\npower\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n     Two-sample t test power calculation \n\n              n = 66\n          delta = 0.6741298\n             sd = 1\n      sig.level = 0.05\n          power = 0.9702076\n    alternative = two.sided\n\nNOTE: n is number in *each* group\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n\n# **Lecture 5:** Summary\n\n::::: columns\n::: {.column width=\"60%\"}\n**Key concepts covered:**\n\n1.  **Probability distributions** model random phenomena\n    -   Normal distribution is especially important\n    -   Z-scores standardize measurements\n2.  **Standard error** measures precision of estimates\n    -   Decreases with larger sample sizes\n    -   Used to construct confidence intervals\n3.  **Confidence intervals** express uncertainty\n    -   Provide plausible range for parameters\n    -   95% CI: `mean ± 1.96 × SE`\n4.  **Hypothesis testing** evaluates claims\n    -   Null vs. alternative hypotheses\n    -   P-values quantify evidence against H₀\n    -   Consider both statistical and practical significance\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-17-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Final Exercise: Comprehensive Analysis\n\nNow that we've covered the key concepts, let's perform a complete analysis of the Arctic grayling data:\n\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Comprehensive analysis of Arctic grayling data\n# 1. Data visualization\nlength_boxplot <- grayling_df %>%\n  ggplot(aes(x = lake, y = total_length_mm, fill = lake)) +\n  geom_boxplot() +\n  labs(title = \"Fish Length by Lake\",\n       x = \"Lake\",\n       y = \"Length (mm)\") +\n  theme_minimal()\n\n# 2. Compare means with t-test\nlength_ttest <- t.test(total_length_mm ~ lake, data = grayling_df)\n\n# 3. Length-mass relationship\nlength_mass_model <- lm(mass_g ~ total_length_mm * lake, data = grayling_df)\nmodel_summary <- summary(length_mass_model)\n\n# 4. Display results\nlength_boxplot\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-18-1.png)\n:::\n\n```{.r .cell-code}\nlength_ttest\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  total_length_mm by lake\nt = -15.532, df = 161.63, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group I3 and group I8 is not equal to 0\n95 percent confidence interval:\n -109.32342  -84.66053\nsample estimates:\nmean in group I3 mean in group I8 \n        265.6061         362.5980 \n```\n\n\n:::\n\n```{.r .cell-code}\nmodel_summary\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = mass_g ~ total_length_mm * lake, data = grayling_df)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-151.223  -14.839   -0.764   10.670  153.130 \n\nCoefficients:\n                        Estimate Std. Error t value Pr(>|t|)    \n(Intercept)            -219.3313    47.9087  -4.578 9.30e-06 ***\ntotal_length_mm           1.3924     0.1794   7.763 8.88e-13 ***\nlakeI8                 -522.5506    56.5882  -9.234  < 2e-16 ***\ntotal_length_mm:lakeI8    1.9738     0.1972  10.009  < 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 40.93 on 162 degrees of freedom\n  (2 observations deleted due to missingness)\nMultiple R-squared:  0.9644,\tAdjusted R-squared:  0.9637 \nF-statistic:  1461 on 3 and 162 DF,  p-value: < 2.2e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# 5. Calculate 95% confidence intervals for each lake\nci_results <- grayling_df %>%\n  group_by(lake) %>%\n  summarize(\n    mean_length = mean(total_length_mm, na.rm = TRUE),\n    sd_length = sd(total_length_mm, na.rm = TRUE),\n    n = sum(!is.na(total_length_mm)),\n    se_length = sd_length / sqrt(n),\n    t_crit = qt(0.975, df = n - 1),\n    margin_error = t_crit * se_length,\n    ci_lower = mean_length - margin_error,\n    ci_upper = mean_length + margin_error,\n    .groups = \"drop\"\n  )\n\n# Display confidence intervals\nci_results\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 9\n  lake  mean_length sd_length     n se_length t_crit margin_error ci_lower\n  <chr>       <dbl>     <dbl> <int>     <dbl>  <dbl>        <dbl>    <dbl>\n1 I3           266.      28.3    66      3.48   2.00         6.96     259.\n2 I8           363.      52.3   102      5.18   1.98        10.3      352.\n# ℹ 1 more variable: ci_upper <dbl>\n```\n\n\n:::\n\n```{.r .cell-code}\n# 6. Visualize regression with confidence intervals\nregression_plot <- grayling_df %>%\n  ggplot(aes(x = total_length_mm, y = mass_g, color = lake)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(title = \"Length-Mass Relationship by Lake\",\n       x = \"Length (mm)\",\n       y = \"Mass (g)\") +\n  theme_minimal()\n\nregression_plot\n```\n\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-18-2.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n\nBased on this analysis: 1. Are there significant differences in fish length between the two lakes? 2. How does the length-mass relationship differ between lakes? 3. What conclusions can you draw about Arctic grayling in these two lakes?\n:::\n\n# **Lecture 5:** Error Bars and Their Interpretation\n\n::::: columns\n::: {.column width=\"60%\"}\nError bars are graphical representations of the variability of data that show:\n\n-   The **precision** of a measurement\n-   The **uncertainty** around an estimate\n-   A **confidence interval** for a parameter\n\nCommon types of error bars: 1. **Standard Error (SE)**: Shows precision of the mean 2. **Standard Deviation (SD)**: Shows variability in the data 3. **Confidence Interval (CI)**: Shows plausible range for parameter\n\nWhen interpreting graphs: - Always check what the error bars represent - Non-overlapping 95% CI bars suggest statistically significant differences - Error bars help assess both statistical and practical significance\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-19-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Sampling and Pseudoreplication\n\n::::: columns\n::: {.column width=\"60%\"}\n**Pseudoreplication** occurs when measurements that are not independent are analyzed as if they were independent.\n\n-   A critical consideration in experimental design\n-   Results in underestimated standard errors and confidence intervals\n-   Leads to inflated Type I error rates (false positives)\n\n**Examples of pseudoreplication:** - Measuring the same individual multiple times - Treating multiple fish from the same tank as independent - Using multiple data points from a single site\n\n**How to avoid pseudoreplication:** - Identify the true experimental unit - Use appropriate statistical techniques (e.g., mixed models) - Be clear about the level of replication\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-20-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Practical Applications in Fish Biology\n\n::::: columns\n::: {.column width=\"60%\"}\nThe statistical concepts we've covered today are essential for fisheries biologists and ecologists:\n\n-   **Z-scores** help identify unusual fish sizes in a population\n-   **Standard error** quantifies uncertainty in growth rate estimates\n-   **Confidence intervals** provide plausible ranges for population parameters\n-   **Hypothesis testing** evaluates effects of management practices\n-   **P-values** determine significance of environmental impacts\n\n**Real-world applications:** - Assessing population health and structure - Evaluating effectiveness of fishing regulations - Quantifying relationships between fish size and habitat variables - Predicting impacts of climate change on fish populations - Designing effective conservation strategies\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](06_01_lecture_powerpoint_old_files/figure-docx/unnamed-chunk-21-1.png)\n:::\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nMajor goal of statistics:\n\ninferences about populations from samples assign degree of confidence to inferences\n\nStatistical H-testing:\n\nformalized approach to inference\n\n-   hypotheses ask whether samples come from populations with certain properties\n-   often interested in questions about population means (but not only)\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nRelies on specifying null hypothesis (Ho) and alternate hypothesis (Ha)\n\n-   Ho is the hypothesis of \"no effect\"\n    -   (two samples from population with same mean, sample is from population of mean=0)\n-   Ha (research hypothesis) the opposite of the Ho\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## Practice Exercise 10: Formulating Hypotheses\n\nFor the following scenarios, write out the null and alternative hypotheses:\n\n1.  Testing if the mean fish length in Lake S 06 is greater than 50 mm.\n\n2.  Testing if there is a difference in mean fish lengths between lakes Toolik and S 06.\n\n3.  Testing if lake E 01 has a higher variance in fish lengths compared to Lake Toolik.\n\nFor each scenario, remember that the null hypothesis typically represents \"no effect\" or \"no difference\", while the alternative hypothesis represents what you are trying to demonstrate.\n:::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   p = 0.3 means that if study repeated 100 times\n    -   would get this (or more extreme) result due to chance 30 times\n-   p = 0.03 means that if study repeated 100 times\n    -   would get this (or more extreme) result due to chance 3 times\n\nWhich p-value suggests Ho likely false?\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nAt what point reject Ho?\n\n-   p \\< 0.05 conventional \"significance threshold\" (α)\n\n-   p \\< 0.05 means:\n\n    -   if Ho is true - if study repeated 100 times\n        -   would get this (or more extreme) result less than 5 times due to chance\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\n-   α is the rate at which we will reject a true null hypothesis (Type I error rate)\n\n-   Lowering α will lower likelihood of incorrectly rejecting a true null hypothesis (e.g., 0.01, 0.001)\n\n-   Both hypotheses and α are specified *BEFORE* collection of data and analysis\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nTraditionally α=0.05 is used as a cut off for rejecting null hypothesis\n\nNothing magical about 0.0 - actual p-values need to be reported.\n\n| p-value range | Interpretation |\n|----|----|\n| P \\> 0.10 | No evidence against Ho - data appear consistent with Ho |\n| 0.05 \\< P \\< 0.10 | Weak evidence against the Ho in favor of Ha |\n| 0.01 \\< P \\< 0.05 | Moderate evidence against Ho in favor of Ha |\n| 0.001 \\< P \\< 0.01 | Strong evidence against Ho in favor of Ha |\n| P \\< 0.001 | Very strong evidence against Ho in favor of Ha |\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nFisher:\n\np-value as informal measure of discrepancy betwen data and Ho\n\n\"If p is between 0.1 and 0.9 there is certainly no reason to suspect the hypothesis tested. If it is below 0.02 it is strongly indicated that the hypothesis fails to account for the whole of the facts. We shall not often be astray if we draw a conventional line at .05 …\"\n:::\n\n::: {.column width=\"40%\"}\n![](images/clipboard-694363384.png){width=\"372\"}s\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nGeneral procedure for H testing:\n\n-   Specify Null (Ho) and alternate (Ha)\n-   Determine test (and test statistic) to be used\n-   Test statistic is used to compare your data to expectation under Ho (null hypothesis)\n-   Specify significance (α or p value) level below which Ho will be rejected\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n# **Lecture 5:** Statistical hypothesis testing\n\n::::: columns\n::: {.column width=\"60%\"}\nGeneral procedure for H testing:\n\n-   Collect data - Perform test\n-   If p-value \\< α, conclude Ho is likely false and reject it\n-   If p-value \\> α, conclude no evidence Ho is false and retain it\n:::\n\n::: {.column width=\"40%\"}\n\n\n\n\n\n\n\n\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\n\n\n\n:::\n:::::\n\n::: callout-tip\n## \n:::\n\n# **Lecture 5:** Next Steps in Statistical Analysis\n\nIn future lectures, we'll explore:\n\n-   One-sample and two-sample t-tests\n-   Analysis of variance (ANOVA)\n-   Linear regression and correlation\n-   Chi-square tests\n-   Non-parametric methods\n-   Multiple regression and model selection\n-   Mixed effects models\n\nEach method builds on the statistical foundation we've established today, applying probability concepts to make inferences from data.\n\n::: callout-tip\n## Learning Resources\n\n-   Practice problems in the textbook (Chapter 4 & 5)\n-   Online resources:\n    -   Khan Academy: Probability and Statistics\n    -   StatQuest with Josh Starmer (YouTube channel)\n    -   R for Data Science (r4ds.had.co.nz)\n-   Office hours: Wednesdays 2-4pm\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}